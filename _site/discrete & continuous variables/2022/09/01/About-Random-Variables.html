<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Comprehensive Guide to Random Variables | Tingaldama</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Comprehensive Guide to Random Variables" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Random Variables Suppose X represents some unknown quantity of interest, such as which way a dice will land when we roll it, or the temperature outside your house at the current time. If the value of X is unknown and/or could change, we call it a random variable or rv. The set of possible values, denoted X, is known as the sample space or state space. An event is a set of outcomes from a given sample space. For example, if X represents the face of a dice that is rolled, so X = {1, 2, . . . , 6}, the event of “seeing a 1” is denoted X = 1, the event of “seeing an odd number” is denoted X ∈ {1, 3, 5}, the event of “seeing a number between 1 and 3” is denoted 1 ≤ X ≤ 3 Discrete and Continuous Random Variables If the sample space X is finite or countably infinite, then X is called a discrete random variable. In this case, we denote the probability of the event that X has value x by Pr(X = x). Probability mass function or pmf as a function which computes the probability of events which correspond to setting the rv to each possible value. If X ∈ R is a real-valued quantity, it is called a continuous random variable. In this case, we can no longer create a finite (or countable) set of distinct possible values it can take on. However, there are a countable number of intervals which we can partition the real line into. Joint Distribution The joint distribution of two random variables ( X ) and ( Y ), denoted as ( p(x, y) = p(X = x, Y = y) ), describes the probability that ( X ) takes on value ( x ) and ( Y ) takes on value ( y ) simultaneously. This joint distribution accounts for the relationship between the two random variables. Marginal Distribution The marginal distribution of a random variable is obtained by summing (or integrating, in the case of continuous variables) the joint distribution over all possible values of the other variable. For example, the marginal distribution of ( X ) is obtained by summing the joint distribution over all possible values of ( Y ): [p(x) = \sum_y p(x, y)] which is also called sum rule The product rule: [p(X = x, Y = y) = p(X = x) \cdot p(Y = y X = x)] By extending the product rule to ( D ) variables, we get the chain rule of probability: [p(x_{1:D}) = p(x_1) p(x_2 \mid x_1) p(x_3 \mid x_1, x_2) p(x_4 \mid x_1, x_2, x_3) \ldots p(x_D \mid x_{1:D-1})] Independence and Conditional Independence Independence: [P(X₁, …, Xₙ) = ∏ᵢ₌₁ⁿ P(Xᵢ)] Indendence is rare, the conditional independence: [X ⊥ Y Z ⇐⇒ p(X, Y Z) = p(X Z)p(Y Z)] Pairwise independence doesn’t imply mutual independence Pairwise Independence vs. Mutual Independence Pairwise independence doesn’t imply mutual independence Pairwise Independence: A set of random variables $(X_1, X_2, \ldots, X_n)$ is pairwise independent if every pair of random variables is independent. That is, for all (i \neq j), \[P(X_i \cap X_j) = P(X_i) P(X_j).\] Mutual Independence: A set of random variables (X_1, X_2, \ldots, X_n) is mutually independent if every subset of the random variables is independent. That is, for any subset ({i_1, i_2, \ldots, i_k}) of ({1, 2, \ldots, n}), \(P(X_{i_1} \cap X_{i_2} \cap \cdots \cap X_{i_k}) = \prod_{j=1}^{k} P(X_{i_j})\)" />
<meta property="og:description" content="Random Variables Suppose X represents some unknown quantity of interest, such as which way a dice will land when we roll it, or the temperature outside your house at the current time. If the value of X is unknown and/or could change, we call it a random variable or rv. The set of possible values, denoted X, is known as the sample space or state space. An event is a set of outcomes from a given sample space. For example, if X represents the face of a dice that is rolled, so X = {1, 2, . . . , 6}, the event of “seeing a 1” is denoted X = 1, the event of “seeing an odd number” is denoted X ∈ {1, 3, 5}, the event of “seeing a number between 1 and 3” is denoted 1 ≤ X ≤ 3 Discrete and Continuous Random Variables If the sample space X is finite or countably infinite, then X is called a discrete random variable. In this case, we denote the probability of the event that X has value x by Pr(X = x). Probability mass function or pmf as a function which computes the probability of events which correspond to setting the rv to each possible value. If X ∈ R is a real-valued quantity, it is called a continuous random variable. In this case, we can no longer create a finite (or countable) set of distinct possible values it can take on. However, there are a countable number of intervals which we can partition the real line into. Joint Distribution The joint distribution of two random variables ( X ) and ( Y ), denoted as ( p(x, y) = p(X = x, Y = y) ), describes the probability that ( X ) takes on value ( x ) and ( Y ) takes on value ( y ) simultaneously. This joint distribution accounts for the relationship between the two random variables. Marginal Distribution The marginal distribution of a random variable is obtained by summing (or integrating, in the case of continuous variables) the joint distribution over all possible values of the other variable. For example, the marginal distribution of ( X ) is obtained by summing the joint distribution over all possible values of ( Y ): [p(x) = \sum_y p(x, y)] which is also called sum rule The product rule: [p(X = x, Y = y) = p(X = x) \cdot p(Y = y X = x)] By extending the product rule to ( D ) variables, we get the chain rule of probability: [p(x_{1:D}) = p(x_1) p(x_2 \mid x_1) p(x_3 \mid x_1, x_2) p(x_4 \mid x_1, x_2, x_3) \ldots p(x_D \mid x_{1:D-1})] Independence and Conditional Independence Independence: [P(X₁, …, Xₙ) = ∏ᵢ₌₁ⁿ P(Xᵢ)] Indendence is rare, the conditional independence: [X ⊥ Y Z ⇐⇒ p(X, Y Z) = p(X Z)p(Y Z)] Pairwise independence doesn’t imply mutual independence Pairwise Independence vs. Mutual Independence Pairwise independence doesn’t imply mutual independence Pairwise Independence: A set of random variables $(X_1, X_2, \ldots, X_n)$ is pairwise independent if every pair of random variables is independent. That is, for all (i \neq j), \[P(X_i \cap X_j) = P(X_i) P(X_j).\] Mutual Independence: A set of random variables (X_1, X_2, \ldots, X_n) is mutually independent if every subset of the random variables is independent. That is, for any subset ({i_1, i_2, \ldots, i_k}) of ({1, 2, \ldots, n}), \(P(X_{i_1} \cap X_{i_2} \cap \cdots \cap X_{i_k}) = \prod_{j=1}^{k} P(X_{i_j})\)" />
<link rel="canonical" href="http://localhost:4000/tingaldama1/discrete%20&%20continuous%20variables/2022/09/01/About-Random-Variables.html" />
<meta property="og:url" content="http://localhost:4000/tingaldama1/discrete%20&%20continuous%20variables/2022/09/01/About-Random-Variables.html" />
<meta property="og:site_name" content="Tingaldama" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-09-01T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Comprehensive Guide to Random Variables" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-09-01T00:00:00+01:00","datePublished":"2022-09-01T00:00:00+01:00","description":"Random Variables Suppose X represents some unknown quantity of interest, such as which way a dice will land when we roll it, or the temperature outside your house at the current time. If the value of X is unknown and/or could change, we call it a random variable or rv. The set of possible values, denoted X, is known as the sample space or state space. An event is a set of outcomes from a given sample space. For example, if X represents the face of a dice that is rolled, so X = {1, 2, . . . , 6}, the event of “seeing a 1” is denoted X = 1, the event of “seeing an odd number” is denoted X ∈ {1, 3, 5}, the event of “seeing a number between 1 and 3” is denoted 1 ≤ X ≤ 3 Discrete and Continuous Random Variables If the sample space X is finite or countably infinite, then X is called a discrete random variable. In this case, we denote the probability of the event that X has value x by Pr(X = x). Probability mass function or pmf as a function which computes the probability of events which correspond to setting the rv to each possible value. If X ∈ R is a real-valued quantity, it is called a continuous random variable. In this case, we can no longer create a finite (or countable) set of distinct possible values it can take on. However, there are a countable number of intervals which we can partition the real line into. Joint Distribution The joint distribution of two random variables ( X ) and ( Y ), denoted as ( p(x, y) = p(X = x, Y = y) ), describes the probability that ( X ) takes on value ( x ) and ( Y ) takes on value ( y ) simultaneously. This joint distribution accounts for the relationship between the two random variables. Marginal Distribution The marginal distribution of a random variable is obtained by summing (or integrating, in the case of continuous variables) the joint distribution over all possible values of the other variable. For example, the marginal distribution of ( X ) is obtained by summing the joint distribution over all possible values of ( Y ): [p(x) = \\sum_y p(x, y)] which is also called sum rule The product rule: [p(X = x, Y = y) = p(X = x) \\cdot p(Y = y X = x)] By extending the product rule to ( D ) variables, we get the chain rule of probability: [p(x_{1:D}) = p(x_1) p(x_2 \\mid x_1) p(x_3 \\mid x_1, x_2) p(x_4 \\mid x_1, x_2, x_3) \\ldots p(x_D \\mid x_{1:D-1})] Independence and Conditional Independence Independence: [P(X₁, …, Xₙ) = ∏ᵢ₌₁ⁿ P(Xᵢ)] Indendence is rare, the conditional independence: [X ⊥ Y Z ⇐⇒ p(X, Y Z) = p(X Z)p(Y Z)] Pairwise independence doesn’t imply mutual independence Pairwise Independence vs. Mutual Independence Pairwise independence doesn’t imply mutual independence Pairwise Independence: A set of random variables $(X_1, X_2, \\ldots, X_n)$ is pairwise independent if every pair of random variables is independent. That is, for all (i \\neq j), \\[P(X_i \\cap X_j) = P(X_i) P(X_j).\\] Mutual Independence: A set of random variables (X_1, X_2, \\ldots, X_n) is mutually independent if every subset of the random variables is independent. That is, for any subset ({i_1, i_2, \\ldots, i_k}) of ({1, 2, \\ldots, n}), \\(P(X_{i_1} \\cap X_{i_2} \\cap \\cdots \\cap X_{i_k}) = \\prod_{j=1}^{k} P(X_{i_j})\\)","headline":"Comprehensive Guide to Random Variables","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/tingaldama1/discrete%20&%20continuous%20variables/2022/09/01/About-Random-Variables.html"},"url":"http://localhost:4000/tingaldama1/discrete%20&%20continuous%20variables/2022/09/01/About-Random-Variables.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/tingaldama1/assets/css/style.css?v=">
    <script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
    <script src="/tingaldama1/assets/js/respond.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="/tingaldama1/assets/css/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup theme-color -->
<!-- start theme color meta headers -->
<meta name="theme-color" content="#353535">
<meta name="msapplication-navbutton-color" content="#353535">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- end theme color meta headers -->


<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/tingaldama1/favicon.ico" -->

<!-- end custom head snippets -->
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<link rel="shortcut icon" href="/tingaldama1//assets/images/favicon.jpg" type="image/x-icon">


  </head>
  <style>
    .categories{
  font-weight: bold;
  color: #949494;
}
 .date{
  font-weight: bold !important;
  color: #949494 !important;
}

  </style>
  <body>
    
<div id="header">
    <nav>
      <ul>
        <li class="fork"><a href="/tingaldama1/">Home</a></li>
        <li class="downloads"><a href="/tingaldama1/contact">Contacts</a></li>
        <li class="downloads"><a href="/tingaldama1/about">About</a></li>
        <li class="downloads"><a href="/tingaldama1/blog">Blogs</a></li>
        <li class="downloads"><a href="/tingaldama1/mlps-resources">MLOps Resources</a></li>
        <li class="downloads"><a href="/tingaldama1/personal-growth">Personal Growth</a></li>
          
      </ul>
    </nav>
  </div><!-- end header -->

    <div class="wrapper">

      <section>
        

        <main>
          <article>
            <h1>Comprehensive Guide to Random Variables</h1>
            <div class="post-meta">
             <span class="date">Date: September 01, 2022</span>
      <br>
      <span class="categories">
        Categories: Discrete & Continuous variables
      </span>
            </div>
            <br>
            <div class="post-content">
              
              <h2 id="random-variables">Random Variables</h2>

<p>Suppose X represents some unknown quantity of interest, such as which way a dice will land when we roll it, or the temperature outside your house at the current time. If the value of X is unknown and/or could change, we call it a random variable or rv. The set of possible values, denoted X, is known as the sample space or state space. An event is a set of outcomes from a given sample space. For example, if X represents the face of a dice that is rolled, so X = {1, 2, . . . , 6}, the event of “seeing a 1” is denoted X = 1, the event of “seeing an odd number” is denoted X ∈ {1, 3, 5}, the event of “seeing a number between 1 and 3” is denoted 1 ≤ X ≤ 3</p>

<h3 id="discrete-and-continuous-random-variables">Discrete and Continuous Random Variables</h3>

<p>If the sample space X is finite or countably infinite, then X is called a discrete random variable.
In this case, we denote the probability of the event that X has value x by Pr(X = x). Probability mass function or pmf as a function which computes the probability of events which correspond to setting the rv to each possible value. If X ∈ R is a real-valued quantity, it is called a continuous random variable. In this case, we
can no longer create a finite (or countable) set of distinct possible values it can take on. However,
there are a countable number of intervals which we can partition the real line into.</p>

<h3 id="joint-distribution">Joint Distribution</h3>

<p>The joint distribution of two random variables ( X ) and ( Y ), denoted as ( p(x, y) = p(X = x, Y = y) ), describes the probability that ( X ) takes on value ( x ) and ( Y ) takes on value ( y ) simultaneously. This joint distribution accounts for the relationship between the two random variables.</p>

<p>Marginal Distribution</p>

<p>The marginal distribution of a random variable is obtained by summing (or integrating, in the case of continuous variables) the joint distribution over all possible values of the other variable. For example, the marginal distribution of ( X ) is obtained by summing the joint distribution over all possible values of ( Y ):</p>

\[p(x) = \sum_y p(x, y)\]

<p>which is also called sum rule</p>

<p>The product rule:</p>

\[p(X = x, Y = y) = p(X = x) \cdot p(Y = y | X = x)\]

<p>By extending the product rule to ( D ) variables, we get the chain rule of probability:</p>

\[p(x_{1:D}) = p(x_1) p(x_2 \mid x_1) p(x_3 \mid x_1, x_2) p(x_4 \mid x_1, x_2, x_3) \ldots p(x_D \mid x_{1:D-1})\]

<hr />

<h3 id="independence-and-conditional-independence">Independence and Conditional Independence</h3>

<p>Independence:</p>

\[P(X₁, ..., Xₙ) = ∏ᵢ₌₁ⁿ P(Xᵢ)\]

<p>Indendence is rare, the conditional independence:</p>

\[X ⊥ Y | Z ⇐⇒ p(X, Y | Z) = p(X | Z)p(Y | Z)\]

<p>Pairwise independence doesn’t imply mutual independence</p>

<h3 id="pairwise-independence-vs-mutual-independence">Pairwise Independence vs. Mutual Independence</h3>

<p>Pairwise independence doesn’t imply mutual independence</p>

<ul>
  <li><strong>Pairwise Independence</strong>: A set of random variables $(X_1, X_2, \ldots, X_n)$ is pairwise independent if every pair of random variables is independent. That is, for all (i \neq j),</li>
  <li>
\[P(X_i \cap X_j) = P(X_i) P(X_j).\]
  </li>
  <li><strong>Mutual Independence</strong>: A set of random variables (X_1, X_2, \ldots, X_n) is mutually independent if every subset of the random variables is independent. That is, for any subset ({i_1, i_2, \ldots, i_k}) of ({1, 2, \ldots, n}),
\(P(X_{i_1} \cap X_{i_2} \cap \cdots \cap X_{i_k}) = \prod_{j=1}^{k} P(X_{i_j})\)</li>
</ul>

            </div>
          </article>
        </main>

      </section>
      <div id="title" style="text-align: center;">
    
    <hr>
    <span class="credits left">Project maintained by <a href=""></a></span>
    <br>
    <br>
    <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href=""></a></span>

    <p>&copy; 2024 Tingaldama. All rights reserved.</p>
  </div>
    </div>
  </body>
</html>
