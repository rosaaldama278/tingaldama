<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Step by Step to Build a Multi-Lingual Translation Language Model with Transformer | Tingaldama</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Step by Step to Build a Multi-Lingual Translation Language Model with Transformer" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In the “Attention is All You Need” paper, the authors introduced the self-attention mechanism within the encoder-decoder architecture for building translation models. This guide provides a step-by-step tutorial on constructing a translation model using the Transformer architecture. We will code the encoder and decoder, train the model, save checkpoints, and perform inference. This post offers a comprehensive, hands-on approach to building a translation model with the Transformer. The transformer model architecture was proposed in the paper: Encoder The encoder’s purpose is to obtain the best contextual representation for the source language. It consists of multiple layers, and the input goes through these layers multiple times to yield optimal results. This iterative process allows the encoder to capture the nuances and dependencies within the source language. On the left side in the red circle is the encoder layer: Positional Encoding Before we dive into the encoder layers. Lets take a look at embedding and positional encoding. In simple terms, we need to mark the position of each token to understand the context. After all, ‘I love you’ and ‘You love me’ are very different statements - just ask anyone who’s ever mixed them up in a text message! Why do we use sinusoidal positional encoding you may ask? please check this article for reference: https://arxiv.org/pdf/2106.02795 class PositionalEncoding(nn.Module): def __init__(self, d_model, dropout=0.1, max_len = 5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer(&#39;pe&#39;, pe) def forward(self, input): input = input + self.pe[:, :input.size(1)].detach() return self.dropout(input Token embedding  Consider token embedding as a way to represent each token in a high-dimensional space. Each token is assigned a unique vector representation, which encodes its semantic and syntactic properties. By learning these embeddings during the training process, the model can capture the relationships and similarities between different tokens. The token embeddings are typically initialized randomly and then learned and updated during the training process. Heres the code for token embedding: class TokenEmbedding(nn.Module): def __init__(self, vocab_size, d_model, max_len=5000): super(TokenEmbedding, self).__init__() self.embedding = nn.Embedding(vocab_size, d_model) self.max_len = max_len def forward(self, input): # Truncate or pad the input sequences to max_len input = input[:, :self.max_len] return self.embedding(input Self-Attention and Multi-head Attention  As the model processes each word in the input sequence, self-attention focuses on all the words in the entire input sequence, helping the model encode the current word better. During this process, the self-attention mechanism integrates the understanding of all relevant words into the word being processed. More specifically, its functions include: Self-attention: Sequence Modeling: Self-attention can be used for modeling sequence data (such as text, time series, audio, etc.). It captures dependencies at different positions within the sequence, thus better understanding the context. This is highly useful for tasks like machine translation, text generation, and sentiment analysis. Parallel Computation: Self-attention allows for parallel computation, which means it can be effectively accelerated on modern hardware. Compared to sequential models like RNNs and CNNs, it is easier to train and infer efficiently on GPUs and TPUs (because scores can be computed in parallel in self-attention). Long-Distance Dependency Capture: Traditional recurrent neural networks (RNNs) may face issues like vanishing or exploding gradients when processing long sequences. Self-attention handles long-distance dependencies better because it doesn’t require sequential processing of the input sequence. Multi-head attention: Enhanced Ability to Focus on Different Positions: Multi-head attention extends the model’s ability to focus on different positions within the input sequence. Multiple Sets of Query/Key/Value Weight Matrices: There are multiple sets of query, key, and value weight matrices (Transformers use eight attention heads), each randomly initialized and the weights will be learned in the training process. class ScaledProductAttn(nn.Module): def __init__(self, dropout = 0.1): super(ScaledProductAttn, self).__init__() self.dropout = nn.Dropout(p=dropout) self.softmax = nn.Softmax(dim=-1) def forward(self, query, key, value, attn_mask = None): _, _, _, d_k= query.shape assert d_k != 0 attn = torch.matmul(query, key.transpose(-1, -2)) / np.sqrt(d_k) if attn_mask is not None: attn = attn.masked_fill(attn_mask == False, float(&#39;-inf&#39;)) attn = self.dropout(self.softmax(attn)) context = torch.matmul(attn, value) return contex class MultiHeadAttn(nn.Module): def __init__(self, n_head, d_model,dropout = 0.1): super(MultiHeadAttn, self).__init__() self.Q = nn.Linear(d_model, d_model) self.K = nn.Linear(d_model, d_model) self.V = nn.Linear(d_model, d_model) self.n_head = n_head self.scaled_dot_attn = ScaledProductAttn(dropout) self.dropout = nn.Dropout(p = dropout) self.norm = nn.LayerNorm(d_model) def forward(self, x, attn_mask=None): batch_size, seq_len, d_model = x.shape h_dim = d_model // self.n_head assert h_dim * self.n_head == d_model Q = self.Q(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) K = self.K(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) V = self.V(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) # print(f&quot;the shape of Q: {Q}&quot;) # print(f&quot;the shape of K: {K}&quot;) # print(f&quot;the shape of V: {V}&quot;) # print(f&quot;attn_mask shape: {attn_mask.shape}&quot;) if attn_mask is not None: attn_mask = attn_mask.expand(batch_size, self.n_head, seq_len, seq_len) # Expanding to [batch_size, n_head, seq_len, seq_len] # print(f&quot;attn_mask shape after expansion: {attn_mask.shape}&quot;) attn_score = self.scaled_dot_attn(Q, K, V, attn_mask) attn_score = attn_score.permute(0,2,1,3).reshape(batch_size, seq_len, -1) attn_score = self.dropout(attn_score) attn_score = self.norm(attn_score + x) return attn_score We put everything togther, this is the code for the encoder layer class EncoderLayer(nn.Module): &quot;&quot;&quot; multi-head attention feedforward network normalization layers regularization &quot;&quot;&quot; def __init__(self, n_head, d_model, d_ff, dropout=0.1): super(EncoderLayer, self).__init__() self.multiheadattn = MultiHeadAttn(n_head, d_model, dropout) self.fnn = PoswiseFeedForwardNet(d_model, d_ff, dropout) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) def forward(self, x, attn_mask): x = self.norm1(x) attn_output = self.multiheadattn(x, attn_mask) x = x + self.dropout(attn_output) x = self.norm2(x) ff_output = self.fnn(x) x = x + self.dropout(ff_output) return x Decoder The decoder is a crucial component in the Transformer architecture, responsible for generating the translated tokens in the target language. Its main objective is to capture the dependencies and relationships among the translated tokens while utilizing the representations from the encoder. The decoder operates by first performing self-attention on each of the translated tokens in the source language. This self-attention mechanism allows the decoder to consider the context and dependencies within the translated sequence itself. By attending to its own previous outputs, the decoder can generate more coherent and contextually relevant translations. The red circlec is the decoder part of the architecture: The decoder architecture consists of several key components, including multi-head self-attention, cross-attention, normalization, and regularization techniques. These components work together to enable the decoder to effectively capture the nuances and dependencies in the target language. Masked Multi-Head attention We shared the multi-head attention class with the encoder: class ScaledProductAttn(nn.Module): def __init__(self, dropout = 0.1): super(ScaledProductAttn, self).__init__() self.dropout = nn.Dropout(p=dropout) self.softmax = nn.Softmax(dim=-1) def forward(self, query, key, value, attn_mask = None): _, _, _, d_k= query.shape assert d_k != 0 attn = torch.matmul(query, key.transpose(-1, -2)) / np.sqrt(d_k) if attn_mask is not None: attn = attn.masked_fill(attn_mask == False, float(&#39;-inf&#39;)) attn = self.dropout(self.softmax(attn)) context = torch.matmul(attn, value) return context class MultiHeadAttn(nn.Module): def __init__(self, n_head, d_model,dropout = 0.1): super(MultiHeadAttn, self).__init__() self.Q = nn.Linear(d_model, d_model) self.K = nn.Linear(d_model, d_model) self.V = nn.Linear(d_model, d_model) self.n_head = n_head self.scaled_dot_attn = ScaledProductAttn(dropout) self.dropout = nn.Dropout(p = dropout) self.norm = nn.LayerNorm(d_model) def forward(self, x, attn_mask=None): batch_size, seq_len, d_model = x.shape h_dim = d_model // self.n_head assert h_dim * self.n_head == d_model Q = self.Q(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) K = self.K(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) V = self.V(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) if attn_mask is not None: attn_mask = attn_mask.expand(batch_size, self.n_head, seq_len, seq_len) # Expanding to [batch_size, n_head, seq_len, seq_len] # print(f&quot;attn_mask shape after expansion: {attn_mask.shape}&quot;) attn_score = self.scaled_dot_attn(Q, K, V, attn_mask) attn_score = attn_score.permute(0,2,1,3).reshape(batch_size, seq_len, -1) attn_score = self.dropout(attn_score) attn_score = self.norm(attn_score + x) return attn_score Cross Attention Cross Attention only modifies the input of Self Attention. The decoder of the Transformer is shown in the right module of the figure below: With three inputs labeled as input1~3. The decoder recursively inputs input1: the output of the decoder from the previous time step (the first input is &lt;bos&gt;, indicating the beginning of the sentence) Adds it to input2: position encoding representing positional information, and performs cross attention with input3. Intuitively, what Cross Attention does is to use the information of key/value to represent the information of query, or to condition query on the condition of key/value. It can also be said to introduce the information of key/value into the information of query (because there is a residual layer that adds to the original query information), and what is obtained is the relevance of query to key (query attending to key, e.g., vehicle attending to lanes, vice versa). In the decoder, the output is generated by first projecting the hidden state into a vector with the same size as the target vocabulary. This vector is then passed through a Softmax function to obtain a probability distribution over all possible tokens in the vocabulary. The token with the highest probability is typically selected as the output at each timestep. However, there are alternative strategies for token selection, such as beam search or sampling methods, which consider multiple high-probability candidates instead of always choosing the most probable token. Heres the code for Corss-Head Attention, it could use the Multi-Head Attention code, but for clarity, i rewrote the class for Cross-Head Attention class MultiHeadCrossAttn(nn.Module): def __init__(self, n_head, d_model, dropout=0.1): super(MultiHeadCrossAttn, self).__init__() self.n_head = n_head self.d_model = d_model self.head_dim = d_model // n_head assert self.head_dim * n_head == d_model, &quot;d_model must be divisible by n_head&quot; # Initialize linear layers for Q, K, V transformations self.query = nn.Linear(d_model, d_model) self.key = nn.Linear(d_model, d_model) self.value = nn.Linear(d_model, d_model) # Attention mechanism self.attention = ScaledProductAttn(dropout) # Layer norm and dropout self.dropout = nn.Dropout(p=dropout) self.norm = nn.LayerNorm(d_model) def forward(self, target, memory, attn_mask=None): # target is from decoder, memory is the encoder&#39;s output batch_size, target_len, _ = target.shape _, memory_len, _ = memory.shape # Project target and memory to query, key, value spaces Q = self.query(target).view(batch_size, target_len, self.n_head, self.head_dim).permute(0, 2, 1, 3) K = self.key(memory).view(batch_size, memory_len, self.n_head, self.head_dim).permute(0, 2, 1, 3) V = self.value(memory).view(batch_size, memory_len, self.n_head, self.head_dim).permute(0, 2, 1, 3) # Expand the attention mask if present if attn_mask is not None: attn_mask = attn_mask.expand(batch_size, self.n_head, target_len, memory_len) # Apply scaled dot product attention attn_output = self.attention(Q, K, V, attn_mask) attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(batch_size, target_len, self.d_model) # Apply dropout, add residual and norm output = self.dropout(attn_output) output = self.norm(output + target) return output Encoder-decoder Now we have encoder and decoder and we put them together import torch import torch.nn as nn class EncoderDecoder(nn.Module): def __init__(self, encoder, decoder, device): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.device = device def padding_mask(self, input): input = input.to(self.device) input_mask = (input != 0).unsqueeze(1).unsqueeze(2).to(self.device) return input_mask def target_mask(self, target): target = target.to(self.device) target_pad_mask = (target != 0).unsqueeze(1).unsqueeze(2).to(self.device) # shape(batch_size, 1, 1, seq_length) target_sub_mask = torch.tril(torch.ones((target.shape[1], target.shape[1]), device=self.device)).bool() # shape(seq_len, seq_len) target_mask = target_pad_mask &amp; target_sub_mask # shape(batch_size, 1, seq_length, seq_length) return target_mask def forward(self, input, target): input = input.to(self.device) target = target.to(self.device) input_mask = self.padding_mask(input) target_mask = self.target_mask(target) # encoder feed through encoded_input = self.encoder(input, input_mask) # decoder feed through output = self.decoder(target, encoded_input, input_mask, target_mask) return output Padding Mask Padding mask was used to mask the padded tokens in attention calculation import torch def create_padding_mask(input_seq, pad_token): &quot;&quot;&quot; Create a mask tensor to indicate the padding tokens in the input sequence. &quot;&quot;&quot; mask = (input_seq != pad_token).float() return mask # Example usage batch_size = 2 seq_len = 5 pad_token = 0 input_seq = torch.tensor([[1, 2, 3, 0, 0], [4, 5, 0, 0, 0]]) mask = create_padding_mask(input_seq, pad_token)python &#39;&#39;&#39; tensor([[1., 1., 1., 0., 0.], [1., 1., 0., 0., 0.]]) &#39;&#39;&#39; Subsequent Mask/ Look-ahead Mask The subsequent mask is used in the decoder self-attention mechanism to prevent the model from attending to future positions during the decoding process. It ensures that the predictions for a given position can only depend on the known outputs at positions less than or equal to the current position. Subsequent Mask: tensor([[ True, False, False, False, False, False], [ True, True, False, False, False, False], [ True, True, True, False, False, False], [ True, True, True, True, False, False], [ True, True, True, True, True, False], [ True, True, True, True, True, True]]) Masked Tokens at Each Time Step: Time Step 1: I __ __ __ __ __ Time Step 2: I love __ __ __ __ Time Step 3: I love to __ __ __ Time Step 4: I love to code __ __ Time Step 5: I love to code in __ Time Step 6: I love to code in PyTorch tensor([[ True, False, False, False, False, False], [ True, True, False, False, False, False], [ True, True, True, False, False, False], [ True, True, True, True, False, False], [ True, True, True, True, True, False], [ True, True, True, True, True, True]])TrainingzLzpython Training Pytorch dataset &amp; dataloader class MyDataset(Dataset): def __init__(self, data): self.src_lang = data[&#39;src&#39;] self.tgt_lang = data[&#39;tgt&#39;] def __len__(self): return len(self.src_lang) def __getitem__(self, index): return { &#39;src&#39;: self.src_lang[index], &#39;tgt&#39;: self.tgt_lang[index] } def collate_fn(batch): src_batch = [item[&#39;src&#39;] for item in batch] tgt_batch = [item[&#39;tgt&#39;] for item in batch] #padding src_batch_padded = pad_sequence([torch.tensor(x, dtype=torch.long) for x in src_batch], batch_first=True, padding_value=0) tgt_batch_padded = pad_sequence([torch.tensor(x, dtype=torch.long) for x in tgt_batch], batch_first=True, padding_value=0) return { &#39;src&#39;: src_batch_padded, &#39;tgt&#39;: tgt_batch_padded, } # Create instances of MyDataset train_dataset = MyDataset(train_data) validation_dataset = MyDataset(validation_data) # Create data loaders train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn) validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn) Training Save the checkpoints # Setup TensorBoard writer and device writer = SummaryWriter() device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # Initialize model components num_layers = 6 num_heads = 8 d_model = 512 d_ff = 1024 encoder = Encoder(vocab_size=10000, n_layer=num_layers, n_head=num_heads, d_model=d_model, d_ff=d_ff) decoder = Decoder(vocab_size=10000, n_layer=num_layers, n_head=num_heads, d_model=d_model, d_ff=d_ff) model = EncoderDecoder(encoder, decoder, device).to(device) # Setup training essentials criterion = nn.CrossEntropyLoss(ignore_index=0) optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5) scheduler = StepLR(optimizer, step_size=5, gamma=0.5) scaler = GradScaler() # Training loop EPOCHS = 30 checkpoints_dir = &#39;../checkpoints&#39; os.makedirs(checkpoints_dir, exist_ok=True) best_vloss = float(&#39;inf&#39;) for epoch in range(EPOCHS): print(f&#39;EPOCH {epoch + 1}:&#39;) epoch_avg_loss = train_one_epoch(train_loader, epoch, writer, model, optimizer, criterion, scaler, device) print(f&#39;Average Training Loss: {epoch_avg_loss:.2f}&#39;) avg_vloss = validate(validation_loader, model, criterion, device) print(f&#39;Validation Loss: {avg_vloss:.2f}&#39;) writer.add_scalars(&#39;Training vs. Validation Loss&#39;, {&#39;Training&#39;: epoch_avg_loss, &#39;Validation&#39;: avg_vloss}, epoch + 1) writer.flush() if avg_vloss &lt; best_vloss: best_vloss = avg_vloss model_filename = os.path.join(checkpoints_dir, f&#39;model_epoch_{epoch + 1}.pt&#39;) torch.save(model.state_dict(), model_filename) print(f&#39;Model saved: {model_filename}&#39;) scheduler.step() print(f&quot;Learning Rate: {scheduler.get_last_lr()}&quot;) torch.cuda.empty_cache() writer.close() Inference def translate(src_sentence, model, sp, max_length=10): model.eval() # Tokenize the source sentence src_tokens = sp.encode_as_ids(src_sentence) src_tensor = torch.LongTensor(src_tokens).unsqueeze(0).to(device) with torch.no_grad(): src_mask = model.padding_mask(src_tensor) memory = model.encoder(src_tensor, src_mask) trg_indexes = [sp.bos_id()] for _ in range(max_length): trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device) trg_mask = model.target_mask(trg_tensor) with torch.no_grad(): output = model.decoder(trg_tensor, memory, src_mask, trg_mask) pred_token = output.argmax(2)[:, -1].item() trg_indexes.append(pred_token) if pred_token == sp.eos_id(): break trg_tokens = sp.decode_ids(trg_indexes) return trg_tokens #Initialize the model and import the tokenizer and checkpoint with the best performance num_layers = 6 num_heads = 8 d_model = 512 d_ff = 1024 encoder = Encoder(vocab_size=10000, n_layer=num_layers, n_head=num_heads, d_model=d_model, d_ff=d_ff) decoder = Decoder(vocab_size=10000, n_layer=num_layers, n_head=num_heads, d_model=d_model, d_ff=d_ff) model = EncoderDecoder(encoder, decoder, device).to(device) model_path = os.path.join(parent_dir, &#39;checkpoints&#39;, src_lang + &#39;-&#39; + tgt_lang, &#39;model.pt&#39;) model.load_state_dict(torch.load(model_path, map_location=torch.device(&#39;cpu&#39;))) model.eval() spm_model_path = os.path.join(parent_dir, &#39;data&#39;, src_lang + &#39;-&#39; + tgt_lang, &#39;bpe&#39;, &#39;bpe.model&#39;) sp = spm.SentencePieceProcessor(model_file=spm_model_path) translated_sentence = translate(src_sentence, model, sp) Translate Examples Heres the examples of translating english into Spanish, German and Chinese" />
<meta property="og:description" content="In the “Attention is All You Need” paper, the authors introduced the self-attention mechanism within the encoder-decoder architecture for building translation models. This guide provides a step-by-step tutorial on constructing a translation model using the Transformer architecture. We will code the encoder and decoder, train the model, save checkpoints, and perform inference. This post offers a comprehensive, hands-on approach to building a translation model with the Transformer. The transformer model architecture was proposed in the paper: Encoder The encoder’s purpose is to obtain the best contextual representation for the source language. It consists of multiple layers, and the input goes through these layers multiple times to yield optimal results. This iterative process allows the encoder to capture the nuances and dependencies within the source language. On the left side in the red circle is the encoder layer: Positional Encoding Before we dive into the encoder layers. Lets take a look at embedding and positional encoding. In simple terms, we need to mark the position of each token to understand the context. After all, ‘I love you’ and ‘You love me’ are very different statements - just ask anyone who’s ever mixed them up in a text message! Why do we use sinusoidal positional encoding you may ask? please check this article for reference: https://arxiv.org/pdf/2106.02795 class PositionalEncoding(nn.Module): def __init__(self, d_model, dropout=0.1, max_len = 5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer(&#39;pe&#39;, pe) def forward(self, input): input = input + self.pe[:, :input.size(1)].detach() return self.dropout(input Token embedding  Consider token embedding as a way to represent each token in a high-dimensional space. Each token is assigned a unique vector representation, which encodes its semantic and syntactic properties. By learning these embeddings during the training process, the model can capture the relationships and similarities between different tokens. The token embeddings are typically initialized randomly and then learned and updated during the training process. Heres the code for token embedding: class TokenEmbedding(nn.Module): def __init__(self, vocab_size, d_model, max_len=5000): super(TokenEmbedding, self).__init__() self.embedding = nn.Embedding(vocab_size, d_model) self.max_len = max_len def forward(self, input): # Truncate or pad the input sequences to max_len input = input[:, :self.max_len] return self.embedding(input Self-Attention and Multi-head Attention  As the model processes each word in the input sequence, self-attention focuses on all the words in the entire input sequence, helping the model encode the current word better. During this process, the self-attention mechanism integrates the understanding of all relevant words into the word being processed. More specifically, its functions include: Self-attention: Sequence Modeling: Self-attention can be used for modeling sequence data (such as text, time series, audio, etc.). It captures dependencies at different positions within the sequence, thus better understanding the context. This is highly useful for tasks like machine translation, text generation, and sentiment analysis. Parallel Computation: Self-attention allows for parallel computation, which means it can be effectively accelerated on modern hardware. Compared to sequential models like RNNs and CNNs, it is easier to train and infer efficiently on GPUs and TPUs (because scores can be computed in parallel in self-attention). Long-Distance Dependency Capture: Traditional recurrent neural networks (RNNs) may face issues like vanishing or exploding gradients when processing long sequences. Self-attention handles long-distance dependencies better because it doesn’t require sequential processing of the input sequence. Multi-head attention: Enhanced Ability to Focus on Different Positions: Multi-head attention extends the model’s ability to focus on different positions within the input sequence. Multiple Sets of Query/Key/Value Weight Matrices: There are multiple sets of query, key, and value weight matrices (Transformers use eight attention heads), each randomly initialized and the weights will be learned in the training process. class ScaledProductAttn(nn.Module): def __init__(self, dropout = 0.1): super(ScaledProductAttn, self).__init__() self.dropout = nn.Dropout(p=dropout) self.softmax = nn.Softmax(dim=-1) def forward(self, query, key, value, attn_mask = None): _, _, _, d_k= query.shape assert d_k != 0 attn = torch.matmul(query, key.transpose(-1, -2)) / np.sqrt(d_k) if attn_mask is not None: attn = attn.masked_fill(attn_mask == False, float(&#39;-inf&#39;)) attn = self.dropout(self.softmax(attn)) context = torch.matmul(attn, value) return contex class MultiHeadAttn(nn.Module): def __init__(self, n_head, d_model,dropout = 0.1): super(MultiHeadAttn, self).__init__() self.Q = nn.Linear(d_model, d_model) self.K = nn.Linear(d_model, d_model) self.V = nn.Linear(d_model, d_model) self.n_head = n_head self.scaled_dot_attn = ScaledProductAttn(dropout) self.dropout = nn.Dropout(p = dropout) self.norm = nn.LayerNorm(d_model) def forward(self, x, attn_mask=None): batch_size, seq_len, d_model = x.shape h_dim = d_model // self.n_head assert h_dim * self.n_head == d_model Q = self.Q(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) K = self.K(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) V = self.V(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) # print(f&quot;the shape of Q: {Q}&quot;) # print(f&quot;the shape of K: {K}&quot;) # print(f&quot;the shape of V: {V}&quot;) # print(f&quot;attn_mask shape: {attn_mask.shape}&quot;) if attn_mask is not None: attn_mask = attn_mask.expand(batch_size, self.n_head, seq_len, seq_len) # Expanding to [batch_size, n_head, seq_len, seq_len] # print(f&quot;attn_mask shape after expansion: {attn_mask.shape}&quot;) attn_score = self.scaled_dot_attn(Q, K, V, attn_mask) attn_score = attn_score.permute(0,2,1,3).reshape(batch_size, seq_len, -1) attn_score = self.dropout(attn_score) attn_score = self.norm(attn_score + x) return attn_score We put everything togther, this is the code for the encoder layer class EncoderLayer(nn.Module): &quot;&quot;&quot; multi-head attention feedforward network normalization layers regularization &quot;&quot;&quot; def __init__(self, n_head, d_model, d_ff, dropout=0.1): super(EncoderLayer, self).__init__() self.multiheadattn = MultiHeadAttn(n_head, d_model, dropout) self.fnn = PoswiseFeedForwardNet(d_model, d_ff, dropout) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) def forward(self, x, attn_mask): x = self.norm1(x) attn_output = self.multiheadattn(x, attn_mask) x = x + self.dropout(attn_output) x = self.norm2(x) ff_output = self.fnn(x) x = x + self.dropout(ff_output) return x Decoder The decoder is a crucial component in the Transformer architecture, responsible for generating the translated tokens in the target language. Its main objective is to capture the dependencies and relationships among the translated tokens while utilizing the representations from the encoder. The decoder operates by first performing self-attention on each of the translated tokens in the source language. This self-attention mechanism allows the decoder to consider the context and dependencies within the translated sequence itself. By attending to its own previous outputs, the decoder can generate more coherent and contextually relevant translations. The red circlec is the decoder part of the architecture: The decoder architecture consists of several key components, including multi-head self-attention, cross-attention, normalization, and regularization techniques. These components work together to enable the decoder to effectively capture the nuances and dependencies in the target language. Masked Multi-Head attention We shared the multi-head attention class with the encoder: class ScaledProductAttn(nn.Module): def __init__(self, dropout = 0.1): super(ScaledProductAttn, self).__init__() self.dropout = nn.Dropout(p=dropout) self.softmax = nn.Softmax(dim=-1) def forward(self, query, key, value, attn_mask = None): _, _, _, d_k= query.shape assert d_k != 0 attn = torch.matmul(query, key.transpose(-1, -2)) / np.sqrt(d_k) if attn_mask is not None: attn = attn.masked_fill(attn_mask == False, float(&#39;-inf&#39;)) attn = self.dropout(self.softmax(attn)) context = torch.matmul(attn, value) return context class MultiHeadAttn(nn.Module): def __init__(self, n_head, d_model,dropout = 0.1): super(MultiHeadAttn, self).__init__() self.Q = nn.Linear(d_model, d_model) self.K = nn.Linear(d_model, d_model) self.V = nn.Linear(d_model, d_model) self.n_head = n_head self.scaled_dot_attn = ScaledProductAttn(dropout) self.dropout = nn.Dropout(p = dropout) self.norm = nn.LayerNorm(d_model) def forward(self, x, attn_mask=None): batch_size, seq_len, d_model = x.shape h_dim = d_model // self.n_head assert h_dim * self.n_head == d_model Q = self.Q(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) K = self.K(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) V = self.V(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) if attn_mask is not None: attn_mask = attn_mask.expand(batch_size, self.n_head, seq_len, seq_len) # Expanding to [batch_size, n_head, seq_len, seq_len] # print(f&quot;attn_mask shape after expansion: {attn_mask.shape}&quot;) attn_score = self.scaled_dot_attn(Q, K, V, attn_mask) attn_score = attn_score.permute(0,2,1,3).reshape(batch_size, seq_len, -1) attn_score = self.dropout(attn_score) attn_score = self.norm(attn_score + x) return attn_score Cross Attention Cross Attention only modifies the input of Self Attention. The decoder of the Transformer is shown in the right module of the figure below: With three inputs labeled as input1~3. The decoder recursively inputs input1: the output of the decoder from the previous time step (the first input is &lt;bos&gt;, indicating the beginning of the sentence) Adds it to input2: position encoding representing positional information, and performs cross attention with input3. Intuitively, what Cross Attention does is to use the information of key/value to represent the information of query, or to condition query on the condition of key/value. It can also be said to introduce the information of key/value into the information of query (because there is a residual layer that adds to the original query information), and what is obtained is the relevance of query to key (query attending to key, e.g., vehicle attending to lanes, vice versa). In the decoder, the output is generated by first projecting the hidden state into a vector with the same size as the target vocabulary. This vector is then passed through a Softmax function to obtain a probability distribution over all possible tokens in the vocabulary. The token with the highest probability is typically selected as the output at each timestep. However, there are alternative strategies for token selection, such as beam search or sampling methods, which consider multiple high-probability candidates instead of always choosing the most probable token. Heres the code for Corss-Head Attention, it could use the Multi-Head Attention code, but for clarity, i rewrote the class for Cross-Head Attention class MultiHeadCrossAttn(nn.Module): def __init__(self, n_head, d_model, dropout=0.1): super(MultiHeadCrossAttn, self).__init__() self.n_head = n_head self.d_model = d_model self.head_dim = d_model // n_head assert self.head_dim * n_head == d_model, &quot;d_model must be divisible by n_head&quot; # Initialize linear layers for Q, K, V transformations self.query = nn.Linear(d_model, d_model) self.key = nn.Linear(d_model, d_model) self.value = nn.Linear(d_model, d_model) # Attention mechanism self.attention = ScaledProductAttn(dropout) # Layer norm and dropout self.dropout = nn.Dropout(p=dropout) self.norm = nn.LayerNorm(d_model) def forward(self, target, memory, attn_mask=None): # target is from decoder, memory is the encoder&#39;s output batch_size, target_len, _ = target.shape _, memory_len, _ = memory.shape # Project target and memory to query, key, value spaces Q = self.query(target).view(batch_size, target_len, self.n_head, self.head_dim).permute(0, 2, 1, 3) K = self.key(memory).view(batch_size, memory_len, self.n_head, self.head_dim).permute(0, 2, 1, 3) V = self.value(memory).view(batch_size, memory_len, self.n_head, self.head_dim).permute(0, 2, 1, 3) # Expand the attention mask if present if attn_mask is not None: attn_mask = attn_mask.expand(batch_size, self.n_head, target_len, memory_len) # Apply scaled dot product attention attn_output = self.attention(Q, K, V, attn_mask) attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(batch_size, target_len, self.d_model) # Apply dropout, add residual and norm output = self.dropout(attn_output) output = self.norm(output + target) return output Encoder-decoder Now we have encoder and decoder and we put them together import torch import torch.nn as nn class EncoderDecoder(nn.Module): def __init__(self, encoder, decoder, device): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.device = device def padding_mask(self, input): input = input.to(self.device) input_mask = (input != 0).unsqueeze(1).unsqueeze(2).to(self.device) return input_mask def target_mask(self, target): target = target.to(self.device) target_pad_mask = (target != 0).unsqueeze(1).unsqueeze(2).to(self.device) # shape(batch_size, 1, 1, seq_length) target_sub_mask = torch.tril(torch.ones((target.shape[1], target.shape[1]), device=self.device)).bool() # shape(seq_len, seq_len) target_mask = target_pad_mask &amp; target_sub_mask # shape(batch_size, 1, seq_length, seq_length) return target_mask def forward(self, input, target): input = input.to(self.device) target = target.to(self.device) input_mask = self.padding_mask(input) target_mask = self.target_mask(target) # encoder feed through encoded_input = self.encoder(input, input_mask) # decoder feed through output = self.decoder(target, encoded_input, input_mask, target_mask) return output Padding Mask Padding mask was used to mask the padded tokens in attention calculation import torch def create_padding_mask(input_seq, pad_token): &quot;&quot;&quot; Create a mask tensor to indicate the padding tokens in the input sequence. &quot;&quot;&quot; mask = (input_seq != pad_token).float() return mask # Example usage batch_size = 2 seq_len = 5 pad_token = 0 input_seq = torch.tensor([[1, 2, 3, 0, 0], [4, 5, 0, 0, 0]]) mask = create_padding_mask(input_seq, pad_token)python &#39;&#39;&#39; tensor([[1., 1., 1., 0., 0.], [1., 1., 0., 0., 0.]]) &#39;&#39;&#39; Subsequent Mask/ Look-ahead Mask The subsequent mask is used in the decoder self-attention mechanism to prevent the model from attending to future positions during the decoding process. It ensures that the predictions for a given position can only depend on the known outputs at positions less than or equal to the current position. Subsequent Mask: tensor([[ True, False, False, False, False, False], [ True, True, False, False, False, False], [ True, True, True, False, False, False], [ True, True, True, True, False, False], [ True, True, True, True, True, False], [ True, True, True, True, True, True]]) Masked Tokens at Each Time Step: Time Step 1: I __ __ __ __ __ Time Step 2: I love __ __ __ __ Time Step 3: I love to __ __ __ Time Step 4: I love to code __ __ Time Step 5: I love to code in __ Time Step 6: I love to code in PyTorch tensor([[ True, False, False, False, False, False], [ True, True, False, False, False, False], [ True, True, True, False, False, False], [ True, True, True, True, False, False], [ True, True, True, True, True, False], [ True, True, True, True, True, True]])TrainingzLzpython Training Pytorch dataset &amp; dataloader class MyDataset(Dataset): def __init__(self, data): self.src_lang = data[&#39;src&#39;] self.tgt_lang = data[&#39;tgt&#39;] def __len__(self): return len(self.src_lang) def __getitem__(self, index): return { &#39;src&#39;: self.src_lang[index], &#39;tgt&#39;: self.tgt_lang[index] } def collate_fn(batch): src_batch = [item[&#39;src&#39;] for item in batch] tgt_batch = [item[&#39;tgt&#39;] for item in batch] #padding src_batch_padded = pad_sequence([torch.tensor(x, dtype=torch.long) for x in src_batch], batch_first=True, padding_value=0) tgt_batch_padded = pad_sequence([torch.tensor(x, dtype=torch.long) for x in tgt_batch], batch_first=True, padding_value=0) return { &#39;src&#39;: src_batch_padded, &#39;tgt&#39;: tgt_batch_padded, } # Create instances of MyDataset train_dataset = MyDataset(train_data) validation_dataset = MyDataset(validation_data) # Create data loaders train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn) validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn) Training Save the checkpoints # Setup TensorBoard writer and device writer = SummaryWriter() device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # Initialize model components num_layers = 6 num_heads = 8 d_model = 512 d_ff = 1024 encoder = Encoder(vocab_size=10000, n_layer=num_layers, n_head=num_heads, d_model=d_model, d_ff=d_ff) decoder = Decoder(vocab_size=10000, n_layer=num_layers, n_head=num_heads, d_model=d_model, d_ff=d_ff) model = EncoderDecoder(encoder, decoder, device).to(device) # Setup training essentials criterion = nn.CrossEntropyLoss(ignore_index=0) optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5) scheduler = StepLR(optimizer, step_size=5, gamma=0.5) scaler = GradScaler() # Training loop EPOCHS = 30 checkpoints_dir = &#39;../checkpoints&#39; os.makedirs(checkpoints_dir, exist_ok=True) best_vloss = float(&#39;inf&#39;) for epoch in range(EPOCHS): print(f&#39;EPOCH {epoch + 1}:&#39;) epoch_avg_loss = train_one_epoch(train_loader, epoch, writer, model, optimizer, criterion, scaler, device) print(f&#39;Average Training Loss: {epoch_avg_loss:.2f}&#39;) avg_vloss = validate(validation_loader, model, criterion, device) print(f&#39;Validation Loss: {avg_vloss:.2f}&#39;) writer.add_scalars(&#39;Training vs. Validation Loss&#39;, {&#39;Training&#39;: epoch_avg_loss, &#39;Validation&#39;: avg_vloss}, epoch + 1) writer.flush() if avg_vloss &lt; best_vloss: best_vloss = avg_vloss model_filename = os.path.join(checkpoints_dir, f&#39;model_epoch_{epoch + 1}.pt&#39;) torch.save(model.state_dict(), model_filename) print(f&#39;Model saved: {model_filename}&#39;) scheduler.step() print(f&quot;Learning Rate: {scheduler.get_last_lr()}&quot;) torch.cuda.empty_cache() writer.close() Inference def translate(src_sentence, model, sp, max_length=10): model.eval() # Tokenize the source sentence src_tokens = sp.encode_as_ids(src_sentence) src_tensor = torch.LongTensor(src_tokens).unsqueeze(0).to(device) with torch.no_grad(): src_mask = model.padding_mask(src_tensor) memory = model.encoder(src_tensor, src_mask) trg_indexes = [sp.bos_id()] for _ in range(max_length): trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device) trg_mask = model.target_mask(trg_tensor) with torch.no_grad(): output = model.decoder(trg_tensor, memory, src_mask, trg_mask) pred_token = output.argmax(2)[:, -1].item() trg_indexes.append(pred_token) if pred_token == sp.eos_id(): break trg_tokens = sp.decode_ids(trg_indexes) return trg_tokens #Initialize the model and import the tokenizer and checkpoint with the best performance num_layers = 6 num_heads = 8 d_model = 512 d_ff = 1024 encoder = Encoder(vocab_size=10000, n_layer=num_layers, n_head=num_heads, d_model=d_model, d_ff=d_ff) decoder = Decoder(vocab_size=10000, n_layer=num_layers, n_head=num_heads, d_model=d_model, d_ff=d_ff) model = EncoderDecoder(encoder, decoder, device).to(device) model_path = os.path.join(parent_dir, &#39;checkpoints&#39;, src_lang + &#39;-&#39; + tgt_lang, &#39;model.pt&#39;) model.load_state_dict(torch.load(model_path, map_location=torch.device(&#39;cpu&#39;))) model.eval() spm_model_path = os.path.join(parent_dir, &#39;data&#39;, src_lang + &#39;-&#39; + tgt_lang, &#39;bpe&#39;, &#39;bpe.model&#39;) sp = spm.SentencePieceProcessor(model_file=spm_model_path) translated_sentence = translate(src_sentence, model, sp) Translate Examples Heres the examples of translating english into Spanish, German and Chinese" />
<link rel="canonical" href="http://localhost:4000/tingaldama/multi-lingual/transformer/2023/12/05/Built-Translation-Model-with-Transformer.html" />
<meta property="og:url" content="http://localhost:4000/tingaldama/multi-lingual/transformer/2023/12/05/Built-Translation-Model-with-Transformer.html" />
<meta property="og:site_name" content="Tingaldama" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-12-05T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Step by Step to Build a Multi-Lingual Translation Language Model with Transformer" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-12-05T00:00:00+01:00","datePublished":"2023-12-05T00:00:00+01:00","description":"In the “Attention is All You Need” paper, the authors introduced the self-attention mechanism within the encoder-decoder architecture for building translation models. This guide provides a step-by-step tutorial on constructing a translation model using the Transformer architecture. We will code the encoder and decoder, train the model, save checkpoints, and perform inference. This post offers a comprehensive, hands-on approach to building a translation model with the Transformer. The transformer model architecture was proposed in the paper: Encoder The encoder’s purpose is to obtain the best contextual representation for the source language. It consists of multiple layers, and the input goes through these layers multiple times to yield optimal results. This iterative process allows the encoder to capture the nuances and dependencies within the source language. On the left side in the red circle is the encoder layer: Positional Encoding Before we dive into the encoder layers. Lets take a look at embedding and positional encoding. In simple terms, we need to mark the position of each token to understand the context. After all, ‘I love you’ and ‘You love me’ are very different statements - just ask anyone who’s ever mixed them up in a text message! Why do we use sinusoidal positional encoding you may ask? please check this article for reference: https://arxiv.org/pdf/2106.02795 class PositionalEncoding(nn.Module): def __init__(self, d_model, dropout=0.1, max_len = 5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer(&#39;pe&#39;, pe) def forward(self, input): input = input + self.pe[:, :input.size(1)].detach() return self.dropout(input Token embedding  Consider token embedding as a way to represent each token in a high-dimensional space. Each token is assigned a unique vector representation, which encodes its semantic and syntactic properties. By learning these embeddings during the training process, the model can capture the relationships and similarities between different tokens. The token embeddings are typically initialized randomly and then learned and updated during the training process. Heres the code for token embedding: class TokenEmbedding(nn.Module): def __init__(self, vocab_size, d_model, max_len=5000): super(TokenEmbedding, self).__init__() self.embedding = nn.Embedding(vocab_size, d_model) self.max_len = max_len def forward(self, input): # Truncate or pad the input sequences to max_len input = input[:, :self.max_len] return self.embedding(input Self-Attention and Multi-head Attention  As the model processes each word in the input sequence, self-attention focuses on all the words in the entire input sequence, helping the model encode the current word better. During this process, the self-attention mechanism integrates the understanding of all relevant words into the word being processed. More specifically, its functions include: Self-attention: Sequence Modeling: Self-attention can be used for modeling sequence data (such as text, time series, audio, etc.). It captures dependencies at different positions within the sequence, thus better understanding the context. This is highly useful for tasks like machine translation, text generation, and sentiment analysis. Parallel Computation: Self-attention allows for parallel computation, which means it can be effectively accelerated on modern hardware. Compared to sequential models like RNNs and CNNs, it is easier to train and infer efficiently on GPUs and TPUs (because scores can be computed in parallel in self-attention). Long-Distance Dependency Capture: Traditional recurrent neural networks (RNNs) may face issues like vanishing or exploding gradients when processing long sequences. Self-attention handles long-distance dependencies better because it doesn’t require sequential processing of the input sequence. Multi-head attention: Enhanced Ability to Focus on Different Positions: Multi-head attention extends the model’s ability to focus on different positions within the input sequence. Multiple Sets of Query/Key/Value Weight Matrices: There are multiple sets of query, key, and value weight matrices (Transformers use eight attention heads), each randomly initialized and the weights will be learned in the training process. class ScaledProductAttn(nn.Module): def __init__(self, dropout = 0.1): super(ScaledProductAttn, self).__init__() self.dropout = nn.Dropout(p=dropout) self.softmax = nn.Softmax(dim=-1) def forward(self, query, key, value, attn_mask = None): _, _, _, d_k= query.shape assert d_k != 0 attn = torch.matmul(query, key.transpose(-1, -2)) / np.sqrt(d_k) if attn_mask is not None: attn = attn.masked_fill(attn_mask == False, float(&#39;-inf&#39;)) attn = self.dropout(self.softmax(attn)) context = torch.matmul(attn, value) return contex class MultiHeadAttn(nn.Module): def __init__(self, n_head, d_model,dropout = 0.1): super(MultiHeadAttn, self).__init__() self.Q = nn.Linear(d_model, d_model) self.K = nn.Linear(d_model, d_model) self.V = nn.Linear(d_model, d_model) self.n_head = n_head self.scaled_dot_attn = ScaledProductAttn(dropout) self.dropout = nn.Dropout(p = dropout) self.norm = nn.LayerNorm(d_model) def forward(self, x, attn_mask=None): batch_size, seq_len, d_model = x.shape h_dim = d_model // self.n_head assert h_dim * self.n_head == d_model Q = self.Q(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) K = self.K(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) V = self.V(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) # print(f&quot;the shape of Q: {Q}&quot;) # print(f&quot;the shape of K: {K}&quot;) # print(f&quot;the shape of V: {V}&quot;) # print(f&quot;attn_mask shape: {attn_mask.shape}&quot;) if attn_mask is not None: attn_mask = attn_mask.expand(batch_size, self.n_head, seq_len, seq_len) # Expanding to [batch_size, n_head, seq_len, seq_len] # print(f&quot;attn_mask shape after expansion: {attn_mask.shape}&quot;) attn_score = self.scaled_dot_attn(Q, K, V, attn_mask) attn_score = attn_score.permute(0,2,1,3).reshape(batch_size, seq_len, -1) attn_score = self.dropout(attn_score) attn_score = self.norm(attn_score + x) return attn_score We put everything togther, this is the code for the encoder layer class EncoderLayer(nn.Module): &quot;&quot;&quot; multi-head attention feedforward network normalization layers regularization &quot;&quot;&quot; def __init__(self, n_head, d_model, d_ff, dropout=0.1): super(EncoderLayer, self).__init__() self.multiheadattn = MultiHeadAttn(n_head, d_model, dropout) self.fnn = PoswiseFeedForwardNet(d_model, d_ff, dropout) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) def forward(self, x, attn_mask): x = self.norm1(x) attn_output = self.multiheadattn(x, attn_mask) x = x + self.dropout(attn_output) x = self.norm2(x) ff_output = self.fnn(x) x = x + self.dropout(ff_output) return x Decoder The decoder is a crucial component in the Transformer architecture, responsible for generating the translated tokens in the target language. Its main objective is to capture the dependencies and relationships among the translated tokens while utilizing the representations from the encoder. The decoder operates by first performing self-attention on each of the translated tokens in the source language. This self-attention mechanism allows the decoder to consider the context and dependencies within the translated sequence itself. By attending to its own previous outputs, the decoder can generate more coherent and contextually relevant translations. The red circlec is the decoder part of the architecture: The decoder architecture consists of several key components, including multi-head self-attention, cross-attention, normalization, and regularization techniques. These components work together to enable the decoder to effectively capture the nuances and dependencies in the target language. Masked Multi-Head attention We shared the multi-head attention class with the encoder: class ScaledProductAttn(nn.Module): def __init__(self, dropout = 0.1): super(ScaledProductAttn, self).__init__() self.dropout = nn.Dropout(p=dropout) self.softmax = nn.Softmax(dim=-1) def forward(self, query, key, value, attn_mask = None): _, _, _, d_k= query.shape assert d_k != 0 attn = torch.matmul(query, key.transpose(-1, -2)) / np.sqrt(d_k) if attn_mask is not None: attn = attn.masked_fill(attn_mask == False, float(&#39;-inf&#39;)) attn = self.dropout(self.softmax(attn)) context = torch.matmul(attn, value) return context class MultiHeadAttn(nn.Module): def __init__(self, n_head, d_model,dropout = 0.1): super(MultiHeadAttn, self).__init__() self.Q = nn.Linear(d_model, d_model) self.K = nn.Linear(d_model, d_model) self.V = nn.Linear(d_model, d_model) self.n_head = n_head self.scaled_dot_attn = ScaledProductAttn(dropout) self.dropout = nn.Dropout(p = dropout) self.norm = nn.LayerNorm(d_model) def forward(self, x, attn_mask=None): batch_size, seq_len, d_model = x.shape h_dim = d_model // self.n_head assert h_dim * self.n_head == d_model Q = self.Q(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) K = self.K(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) V = self.V(x).view(batch_size, seq_len, self.n_head, h_dim).permute(0,2,1,3) if attn_mask is not None: attn_mask = attn_mask.expand(batch_size, self.n_head, seq_len, seq_len) # Expanding to [batch_size, n_head, seq_len, seq_len] # print(f&quot;attn_mask shape after expansion: {attn_mask.shape}&quot;) attn_score = self.scaled_dot_attn(Q, K, V, attn_mask) attn_score = attn_score.permute(0,2,1,3).reshape(batch_size, seq_len, -1) attn_score = self.dropout(attn_score) attn_score = self.norm(attn_score + x) return attn_score Cross Attention Cross Attention only modifies the input of Self Attention. The decoder of the Transformer is shown in the right module of the figure below: With three inputs labeled as input1~3. The decoder recursively inputs input1: the output of the decoder from the previous time step (the first input is &lt;bos&gt;, indicating the beginning of the sentence) Adds it to input2: position encoding representing positional information, and performs cross attention with input3. Intuitively, what Cross Attention does is to use the information of key/value to represent the information of query, or to condition query on the condition of key/value. It can also be said to introduce the information of key/value into the information of query (because there is a residual layer that adds to the original query information), and what is obtained is the relevance of query to key (query attending to key, e.g., vehicle attending to lanes, vice versa). In the decoder, the output is generated by first projecting the hidden state into a vector with the same size as the target vocabulary. This vector is then passed through a Softmax function to obtain a probability distribution over all possible tokens in the vocabulary. The token with the highest probability is typically selected as the output at each timestep. However, there are alternative strategies for token selection, such as beam search or sampling methods, which consider multiple high-probability candidates instead of always choosing the most probable token. Heres the code for Corss-Head Attention, it could use the Multi-Head Attention code, but for clarity, i rewrote the class for Cross-Head Attention class MultiHeadCrossAttn(nn.Module): def __init__(self, n_head, d_model, dropout=0.1): super(MultiHeadCrossAttn, self).__init__() self.n_head = n_head self.d_model = d_model self.head_dim = d_model // n_head assert self.head_dim * n_head == d_model, &quot;d_model must be divisible by n_head&quot; # Initialize linear layers for Q, K, V transformations self.query = nn.Linear(d_model, d_model) self.key = nn.Linear(d_model, d_model) self.value = nn.Linear(d_model, d_model) # Attention mechanism self.attention = ScaledProductAttn(dropout) # Layer norm and dropout self.dropout = nn.Dropout(p=dropout) self.norm = nn.LayerNorm(d_model) def forward(self, target, memory, attn_mask=None): # target is from decoder, memory is the encoder&#39;s output batch_size, target_len, _ = target.shape _, memory_len, _ = memory.shape # Project target and memory to query, key, value spaces Q = self.query(target).view(batch_size, target_len, self.n_head, self.head_dim).permute(0, 2, 1, 3) K = self.key(memory).view(batch_size, memory_len, self.n_head, self.head_dim).permute(0, 2, 1, 3) V = self.value(memory).view(batch_size, memory_len, self.n_head, self.head_dim).permute(0, 2, 1, 3) # Expand the attention mask if present if attn_mask is not None: attn_mask = attn_mask.expand(batch_size, self.n_head, target_len, memory_len) # Apply scaled dot product attention attn_output = self.attention(Q, K, V, attn_mask) attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(batch_size, target_len, self.d_model) # Apply dropout, add residual and norm output = self.dropout(attn_output) output = self.norm(output + target) return output Encoder-decoder Now we have encoder and decoder and we put them together import torch import torch.nn as nn class EncoderDecoder(nn.Module): def __init__(self, encoder, decoder, device): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.device = device def padding_mask(self, input): input = input.to(self.device) input_mask = (input != 0).unsqueeze(1).unsqueeze(2).to(self.device) return input_mask def target_mask(self, target): target = target.to(self.device) target_pad_mask = (target != 0).unsqueeze(1).unsqueeze(2).to(self.device) # shape(batch_size, 1, 1, seq_length) target_sub_mask = torch.tril(torch.ones((target.shape[1], target.shape[1]), device=self.device)).bool() # shape(seq_len, seq_len) target_mask = target_pad_mask &amp; target_sub_mask # shape(batch_size, 1, seq_length, seq_length) return target_mask def forward(self, input, target): input = input.to(self.device) target = target.to(self.device) input_mask = self.padding_mask(input) target_mask = self.target_mask(target) # encoder feed through encoded_input = self.encoder(input, input_mask) # decoder feed through output = self.decoder(target, encoded_input, input_mask, target_mask) return output Padding Mask Padding mask was used to mask the padded tokens in attention calculation import torch def create_padding_mask(input_seq, pad_token): &quot;&quot;&quot; Create a mask tensor to indicate the padding tokens in the input sequence. &quot;&quot;&quot; mask = (input_seq != pad_token).float() return mask # Example usage batch_size = 2 seq_len = 5 pad_token = 0 input_seq = torch.tensor([[1, 2, 3, 0, 0], [4, 5, 0, 0, 0]]) mask = create_padding_mask(input_seq, pad_token)python &#39;&#39;&#39; tensor([[1., 1., 1., 0., 0.], [1., 1., 0., 0., 0.]]) &#39;&#39;&#39; Subsequent Mask/ Look-ahead Mask The subsequent mask is used in the decoder self-attention mechanism to prevent the model from attending to future positions during the decoding process. It ensures that the predictions for a given position can only depend on the known outputs at positions less than or equal to the current position. Subsequent Mask: tensor([[ True, False, False, False, False, False], [ True, True, False, False, False, False], [ True, True, True, False, False, False], [ True, True, True, True, False, False], [ True, True, True, True, True, False], [ True, True, True, True, True, True]]) Masked Tokens at Each Time Step: Time Step 1: I __ __ __ __ __ Time Step 2: I love __ __ __ __ Time Step 3: I love to __ __ __ Time Step 4: I love to code __ __ Time Step 5: I love to code in __ Time Step 6: I love to code in PyTorch tensor([[ True, False, False, False, False, False], [ True, True, False, False, False, False], [ True, True, True, False, False, False], [ True, True, True, True, False, False], [ True, True, True, True, True, False], [ True, True, True, True, True, True]])TrainingzLzpython Training Pytorch dataset &amp; dataloader class MyDataset(Dataset): def __init__(self, data): self.src_lang = data[&#39;src&#39;] self.tgt_lang = data[&#39;tgt&#39;] def __len__(self): return len(self.src_lang) def __getitem__(self, index): return { &#39;src&#39;: self.src_lang[index], &#39;tgt&#39;: self.tgt_lang[index] } def collate_fn(batch): src_batch = [item[&#39;src&#39;] for item in batch] tgt_batch = [item[&#39;tgt&#39;] for item in batch] #padding src_batch_padded = pad_sequence([torch.tensor(x, dtype=torch.long) for x in src_batch], batch_first=True, padding_value=0) tgt_batch_padded = pad_sequence([torch.tensor(x, dtype=torch.long) for x in tgt_batch], batch_first=True, padding_value=0) return { &#39;src&#39;: src_batch_padded, &#39;tgt&#39;: tgt_batch_padded, } # Create instances of MyDataset train_dataset = MyDataset(train_data) validation_dataset = MyDataset(validation_data) # Create data loaders train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn) validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn) Training Save the checkpoints # Setup TensorBoard writer and device writer = SummaryWriter() device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # Initialize model components num_layers = 6 num_heads = 8 d_model = 512 d_ff = 1024 encoder = Encoder(vocab_size=10000, n_layer=num_layers, n_head=num_heads, d_model=d_model, d_ff=d_ff) decoder = Decoder(vocab_size=10000, n_layer=num_layers, n_head=num_heads, d_model=d_model, d_ff=d_ff) model = EncoderDecoder(encoder, decoder, device).to(device) # Setup training essentials criterion = nn.CrossEntropyLoss(ignore_index=0) optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5) scheduler = StepLR(optimizer, step_size=5, gamma=0.5) scaler = GradScaler() # Training loop EPOCHS = 30 checkpoints_dir = &#39;../checkpoints&#39; os.makedirs(checkpoints_dir, exist_ok=True) best_vloss = float(&#39;inf&#39;) for epoch in range(EPOCHS): print(f&#39;EPOCH {epoch + 1}:&#39;) epoch_avg_loss = train_one_epoch(train_loader, epoch, writer, model, optimizer, criterion, scaler, device) print(f&#39;Average Training Loss: {epoch_avg_loss:.2f}&#39;) avg_vloss = validate(validation_loader, model, criterion, device) print(f&#39;Validation Loss: {avg_vloss:.2f}&#39;) writer.add_scalars(&#39;Training vs. Validation Loss&#39;, {&#39;Training&#39;: epoch_avg_loss, &#39;Validation&#39;: avg_vloss}, epoch + 1) writer.flush() if avg_vloss &lt; best_vloss: best_vloss = avg_vloss model_filename = os.path.join(checkpoints_dir, f&#39;model_epoch_{epoch + 1}.pt&#39;) torch.save(model.state_dict(), model_filename) print(f&#39;Model saved: {model_filename}&#39;) scheduler.step() print(f&quot;Learning Rate: {scheduler.get_last_lr()}&quot;) torch.cuda.empty_cache() writer.close() Inference def translate(src_sentence, model, sp, max_length=10): model.eval() # Tokenize the source sentence src_tokens = sp.encode_as_ids(src_sentence) src_tensor = torch.LongTensor(src_tokens).unsqueeze(0).to(device) with torch.no_grad(): src_mask = model.padding_mask(src_tensor) memory = model.encoder(src_tensor, src_mask) trg_indexes = [sp.bos_id()] for _ in range(max_length): trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device) trg_mask = model.target_mask(trg_tensor) with torch.no_grad(): output = model.decoder(trg_tensor, memory, src_mask, trg_mask) pred_token = output.argmax(2)[:, -1].item() trg_indexes.append(pred_token) if pred_token == sp.eos_id(): break trg_tokens = sp.decode_ids(trg_indexes) return trg_tokens #Initialize the model and import the tokenizer and checkpoint with the best performance num_layers = 6 num_heads = 8 d_model = 512 d_ff = 1024 encoder = Encoder(vocab_size=10000, n_layer=num_layers, n_head=num_heads, d_model=d_model, d_ff=d_ff) decoder = Decoder(vocab_size=10000, n_layer=num_layers, n_head=num_heads, d_model=d_model, d_ff=d_ff) model = EncoderDecoder(encoder, decoder, device).to(device) model_path = os.path.join(parent_dir, &#39;checkpoints&#39;, src_lang + &#39;-&#39; + tgt_lang, &#39;model.pt&#39;) model.load_state_dict(torch.load(model_path, map_location=torch.device(&#39;cpu&#39;))) model.eval() spm_model_path = os.path.join(parent_dir, &#39;data&#39;, src_lang + &#39;-&#39; + tgt_lang, &#39;bpe&#39;, &#39;bpe.model&#39;) sp = spm.SentencePieceProcessor(model_file=spm_model_path) translated_sentence = translate(src_sentence, model, sp) Translate Examples Heres the examples of translating english into Spanish, German and Chinese","headline":"Step by Step to Build a Multi-Lingual Translation Language Model with Transformer","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/tingaldama/multi-lingual/transformer/2023/12/05/Built-Translation-Model-with-Transformer.html"},"url":"http://localhost:4000/tingaldama/multi-lingual/transformer/2023/12/05/Built-Translation-Model-with-Transformer.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/tingaldama/assets/css/style.css?v=">
    <script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
    <script src="/tingaldama/assets/js/respond.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="/tingaldama/assets/css/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup theme-color -->
<!-- start theme color meta headers -->
<meta name="theme-color" content="#353535">
<meta name="msapplication-navbutton-color" content="#353535">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- end theme color meta headers -->


<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/tingaldama/favicon.ico" -->

<!-- end custom head snippets -->
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<link rel="shortcut icon" href="/tingaldama//assets/images/favicon.jpg" type="image/x-icon">


  </head>
  <style>
    .categories{
  font-weight: bold;
  color: #949494;
}
 .date{
  font-weight: bold !important;
  color: #949494 !important;
}

  </style>
  <body>
    
<div id="header">
    <nav>
      <ul>
        <li class="fork"><a href="/tingaldama/">Home</a></li>
        <li class="downloads"><a href="/tingaldama/contact">Contacts</a></li>
        <li class="downloads"><a href="/tingaldama/about">About</a></li>
        <li class="downloads"><a href="/tingaldama/blog">Blogs</a></li>
        <li class="downloads"><a href="/tingaldama/mlps-resources">MLOps Resources</a></li>
        <li class="downloads"><a href="/tingaldama/personal-growth">Personal Growth</a></li>
          
      </ul>
    </nav>
  </div><!-- end header -->

    <div class="wrapper">

      <section>
        

        <main>
          <article>
            <h1>Step by Step to Build a Multi-Lingual Translation Language Model with Transformer</h1>
            <div class="post-meta">
             <span class="date">Date: December 05, 2023</span>
      <br>
      <span class="categories">
        Categories: multi-lingual | transformer
      </span>
            </div>
            <br>
            <div class="post-content">
              
              <p>In the “Attention is All You Need” paper, the authors introduced the self-attention mechanism within the encoder-decoder architecture for building translation models. This guide provides a step-by-step tutorial on constructing a translation model using the Transformer architecture. We will code the encoder and decoder, train the model, save checkpoints, and perform inference. This post offers a comprehensive, hands-on approach to building a translation model with the Transformer.</p>

<p>The transformer model architecture was proposed in the paper:</p>

<p><img src="/tingaldama/assets/images/translation_model/transformer.png" alt="Transformer Model" /></p>

<h2 id="encoder">Encoder</h2>

<p>The encoder’s purpose is to obtain the <strong>best contextual representation for the source language</strong>. It consists of multiple layers, and the input goes through these layers multiple times to yield optimal results. This iterative process allows the encoder to capture the nuances and dependencies within the source language.
On the left side in the red circle is the encoder layer:</p>

<p><img src="/tingaldama/assets/images/translation_model//encoder.png" alt="Encoder" /></p>

<hr />

<h3 id="positional-encoding">Positional Encoding</h3>

<p>Before we dive into the encoder layers. Lets take a look at embedding and positional encoding.
In simple terms, we need to mark the position of each token to understand the context. After all, ‘I love you’ and ‘You love me’ are very different statements - just ask anyone who’s ever mixed them up in a text message! Why do we use sinusoidal positional encoding you may ask? please check this article for reference:<a href=""> https://arxiv.org/pdf/2106.02795</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_len</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">pe</span><span class="sh">'</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="nb">input</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)].</span><span class="nf">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="nb">input</span>
</code></pre></div></div>

<h3 id="token-embedding">Token embedding </h3>

<p>Consider token embedding as a way to represent each token in a high-dimensional space. Each token is assigned a unique vector representation, which encodes its semantic and syntactic properties. By learning these embeddings during the training process, the model can capture the relationships and similarities between different tokens. The token embeddings are typically initialized randomly and then learned and updated during the training process. Heres the code for token embedding:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TokenEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TokenEmbedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">max_len</span> <span class="o">=</span> <span class="n">max_len</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># Truncate or pad the input sequences to max_len
</span>        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="p">:</span><span class="n">self</span><span class="p">.</span><span class="n">max_len</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span>
</code></pre></div></div>

<h3 id="self-attention-and-multi-head-attention">Self-Attention and Multi-head Attention </h3>

<p>As the model processes each word in the input sequence, self-attention focuses on all the words in the entire input sequence, helping the model encode the current word better. During this process, the self-attention mechanism integrates the understanding of all relevant words into the word being processed. More specifically, its functions include:
    <em>Self-attention:</em></p>

<ul>
  <li>Sequence Modeling: Self-attention can be used for modeling sequence data (such as text, time series, audio, etc.). It captures dependencies at different positions within the sequence, thus better understanding the context. This is highly useful for tasks like machine translation, text generation, and sentiment analysis.</li>
  <li>Parallel Computation: Self-attention allows for parallel computation, which means it can be effectively accelerated on modern hardware. Compared to sequential models like RNNs and CNNs, it is easier to train and infer efficiently on GPUs and TPUs (because scores can be computed in parallel in self-attention).</li>
  <li>Long-Distance Dependency Capture: Traditional recurrent neural networks (RNNs) may face issues like vanishing or exploding gradients when processing long sequences. Self-attention handles long-distance dependencies better because it doesn’t require sequential processing of the input sequence.</li>
  <li>Multi-head attention:</li>
  <li>Enhanced Ability to Focus on Different Positions: Multi-head attention extends the model’s ability to focus on different positions within the input sequence.</li>
  <li>Multiple Sets of Query/Key/Value Weight Matrices: There are multiple sets of query, key, and value weight matrices (Transformers use eight attention heads), each randomly initialized and the weights will be learned in the training process.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="k">class</span> <span class="nc">ScaledProductAttn</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
         <span class="nf">super</span><span class="p">(</span><span class="n">ScaledProductAttn</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
         <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
         <span class="n">self</span><span class="p">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
          <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">d_k</span><span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="n">shape</span>
          <span class="k">assert</span> <span class="n">d_k</span> <span class="o">!=</span> <span class="mi">0</span>
          <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
              <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">attn_mask</span> <span class="o">==</span> <span class="bp">False</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>
          <span class="n">attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">attn</span><span class="p">))</span>
          <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">contex</span>

  <span class="k">class</span> <span class="nc">MultiHeadAttn</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span><span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
          <span class="nf">super</span><span class="p">(</span><span class="n">MultiHeadAttn</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
          <span class="n">self</span><span class="p">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
          <span class="n">self</span><span class="p">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
          <span class="n">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
          <span class="n">self</span><span class="p">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>
          <span class="n">self</span><span class="p">.</span><span class="n">scaled_dot_attn</span> <span class="o">=</span> <span class="nc">ScaledProductAttn</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
          <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">)</span>
          <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span>  <span class="n">attn_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
          <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
          <span class="n">h_dim</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span>
          <span class="k">assert</span> <span class="n">h_dim</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span> <span class="o">==</span> <span class="n">d_model</span>
          <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">Q</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
          <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">K</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
          <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">V</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
          <span class="c1"># print(f"the shape of Q: {Q}")
</span>          <span class="c1"># print(f"the shape of K: {K}")
</span>          <span class="c1"># print(f"the shape of V: {V}")
</span>          <span class="c1"># print(f"attn_mask shape: {attn_mask.shape}")
</span>          <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
              <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>  <span class="c1"># Expanding to [batch_size, n_head, seq_len, seq_len]
</span>          <span class="c1"># print(f"attn_mask shape after expansion: {attn_mask.shape}")
</span>          <span class="n">attn_score</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">scaled_dot_attn</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">)</span>
          <span class="n">attn_score</span> <span class="o">=</span> <span class="n">attn_score</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

          <span class="n">attn_score</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_score</span><span class="p">)</span>
          <span class="n">attn_score</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">attn_score</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">attn_score</span>

</code></pre></div></div>

<p>We put everything togther, this is the code for the encoder layer</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
<span class="sh">"""</span><span class="s">
  multi-head attention
  feedforward network
  normalization layers
  regularization

</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">multiheadattn</span> <span class="o">=</span> <span class="nc">MultiHeadAttn</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fnn</span> <span class="o">=</span> <span class="nc">PoswiseFeedForwardNet</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">multiheadattn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<h2 id="decoder">Decoder</h2>

<p>The decoder is a crucial component in the Transformer architecture, responsible for generating the translated tokens in the target language. Its main objective is to capture the dependencies and relationships among the translated tokens while utilizing the representations from the encoder.
The decoder operates by first performing self-attention on each of the translated tokens in the source language. This self-attention mechanism allows the decoder to consider the context and dependencies within the translated sequence itself. By attending to its own previous outputs, the decoder can generate more coherent and contextually relevant translations. The red circlec is the decoder part of the architecture:</p>

<p><img src="/tingaldama/assets/images/translation_model/decoder.png" alt="decoder" /></p>

<p>The decoder architecture consists of several key components, including multi-head self-attention, cross-attention, normalization, and regularization techniques. These components work together to enable the decoder to effectively capture the nuances and dependencies in the target language.</p>

<h3 id="masked-multi-head-attention">Masked Multi-Head attention</h3>

<p>We shared the multi-head attention class with the encoder:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

<span class="k">class</span> <span class="nc">ScaledProductAttn</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ScaledProductAttn</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">d_k</span><span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="n">shape</span>
        <span class="k">assert</span> <span class="n">d_k</span> <span class="o">!=</span> <span class="mi">0</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">attn_mask</span> <span class="o">==</span> <span class="bp">False</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">attn</span><span class="p">))</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">context</span>

<span class="k">class</span> <span class="nc">MultiHeadAttn</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span><span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">MultiHeadAttn</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>
        <span class="n">self</span><span class="p">.</span><span class="n">scaled_dot_attn</span> <span class="o">=</span> <span class="nc">ScaledProductAttn</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span>  <span class="n">attn_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">h_dim</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span>
        <span class="k">assert</span> <span class="n">h_dim</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span> <span class="o">==</span> <span class="n">d_model</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">Q</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">K</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">V</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
  
        <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>  <span class="c1"># Expanding to [batch_size, n_head, seq_len, seq_len]
</span>        <span class="c1"># print(f"attn_mask shape after expansion: {attn_mask.shape}")
</span>        <span class="n">attn_score</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">scaled_dot_attn</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">)</span>
        <span class="n">attn_score</span> <span class="o">=</span> <span class="n">attn_score</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">attn_score</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_score</span><span class="p">)</span>
        <span class="n">attn_score</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">attn_score</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">attn_score</span>

</code></pre></div></div>

<h3 id="cross-attention">Cross Attention</h3>

<p>Cross Attention only modifies the input of Self Attention. The decoder of the Transformer is shown in the right module of the figure below:</p>

<p><img src="/tingaldama/assets/images/translation_model/inputs.png" alt="crossattn" /></p>

<ol>
  <li>With three inputs labeled as input1~3. The decoder recursively inputs input1: the output of the decoder from the previous time step (the first input is <code class="language-plaintext highlighter-rouge">&lt;bos&gt;</code>, indicating the beginning of the sentence)</li>
  <li>Adds it to input2: position encoding representing positional information, and performs cross attention with input3. Intuitively, what Cross Attention does is to use the information of key/value to represent the information of query, or to condition query on the condition of key/value. It can also be said to introduce the information of key/value into the information of query (because there is a residual layer that adds to the original query information), and what is obtained is the relevance of query to key (query attending to key, e.g., vehicle attending to lanes, vice versa).</li>
  <li>
    <p>In the decoder, the output is generated by first projecting the hidden state into a vector with the same size as the target vocabulary. This vector is then passed through a Softmax function to obtain a probability distribution over all possible tokens in the vocabulary. The token with the highest probability is typically selected as the output at each timestep. However, there are alternative strategies for token selection, such as beam search or sampling methods, which consider multiple high-probability candidates instead of always choosing the most probable token.</p>

    <p>Heres the code for Corss-Head Attention, it could use the Multi-Head Attention code, but for clarity, i rewrote the class for Cross-Head Attention</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadCrossAttn</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">MultiHeadCrossAttn</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_head</span>

        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="n">n_head</span> <span class="o">==</span> <span class="n">d_model</span><span class="p">,</span> <span class="sh">"</span><span class="s">d_model must be divisible by n_head</span><span class="sh">"</span>

        <span class="c1"># Initialize linear layers for Q, K, V transformations
</span>        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

        <span class="c1"># Attention mechanism
</span>        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="nc">ScaledProductAttn</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># Layer norm and dropout
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># target is from decoder, memory is the encoder's output
</span>        <span class="n">batch_size</span><span class="p">,</span> <span class="n">target_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">memory_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">memory</span><span class="p">.</span><span class="n">shape</span>

        <span class="c1"># Project target and memory to query, key, value spaces
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">target</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">target_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">memory</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">memory_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">memory</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">memory_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="c1"># Expand the attention mask if present
</span>        <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">target_len</span><span class="p">,</span> <span class="n">memory_len</span><span class="p">)</span>

        <span class="c1"># Apply scaled dot product attention
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">target_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>

        <span class="c1"># Apply dropout, add residual and norm
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">output</span> <span class="o">+</span> <span class="n">target</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>


</code></pre></div>    </div>
  </li>
</ol>

<h2 id="encoder-decoder">Encoder-decoder</h2>

<p>Now we have encoder and decoder and we put them together</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">EncoderDecoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>
        <span class="n">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>

    <span class="k">def</span> <span class="nf">padding_mask</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">input_mask</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_mask</span>

    <span class="k">def</span> <span class="nf">target_mask</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">target_pad_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">target</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># shape(batch_size, 1, 1, seq_length)
</span>        <span class="n">target_sub_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">target</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">target</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">device</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)).</span><span class="nf">bool</span><span class="p">()</span>  <span class="c1"># shape(seq_len, seq_len)
</span>        <span class="n">target_mask</span> <span class="o">=</span> <span class="n">target_pad_mask</span> <span class="o">&amp;</span> <span class="n">target_sub_mask</span>  <span class="c1"># shape(batch_size, 1, seq_length, seq_length)
</span>        <span class="k">return</span> <span class="n">target_mask</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">input_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">padding_mask</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">target_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">target_mask</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>

        <span class="c1"># encoder feed through
</span>        <span class="n">encoded_input</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">)</span>

        <span class="c1"># decoder feed through
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">encoded_input</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>

</code></pre></div></div>

<h3 id="padding-mask">Padding Mask</h3>

<p>Padding mask was used to mask the padded tokens in attention calculation</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">create_padding_mask</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">pad_token</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Create a mask tensor to indicate the padding tokens in the input sequence.
  
    </span><span class="sh">"""</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_seq</span> <span class="o">!=</span> <span class="n">pad_token</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">mask</span>

<span class="c1"># Example usage
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">pad_token</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">input_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                          <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="n">mask</span> <span class="o">=</span> <span class="nf">create_padding_mask</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">pad_token</span><span class="p">)</span><span class="n">python</span>

<span class="sh">'''</span><span class="s">
tensor([[1., 1., 1., 0., 0.],
        [1., 1., 0., 0., 0.]])
</span><span class="sh">'''</span>

</code></pre></div></div>

<h3 id="subsequent-mask-look-ahead-mask">Subsequent Mask/ Look-ahead Mask</h3>

<p>The subsequent mask is used in the decoder self-attention mechanism to prevent the model from attending to future positions during the decoding process. It ensures that the predictions for a given position can only depend on the known outputs at positions less than or equal to the current position.</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Subsequent Mask:
tensor([[ True, False, False, False, False, False],
        [ True,  True, False, False, False, False],
        [ True,  True,  True, False, False, False],
        [ True,  True,  True,  True, False, False],
        [ True,  True,  True,  True,  True, False],
        [ True,  True,  True,  True,  True,  True]])

Masked Tokens at Each Time Step:
Time Step 1: I __ __ __ __ __
Time Step 2: I love __ __ __ __
Time Step 3: I love to __ __ __
Time Step 4: I love to code __ __
Time Step 5: I love to code in __
Time Step 6: I love to code in PyTorch
tensor([[ True, False, False, False, False, False],
        [ True,  True, False, False, False, False],
        [ True,  True,  True, False, False, False],
        [ True,  True,  True,  True, False, False],
        [ True,  True,  True,  True,  True, False],
        [ True,  True,  True,  True,  True,  True]])TrainingzLzpython
</code></pre></div></div>

<h2 id="training">Training</h2>

<h3 id="pytorch-dataset--dataloader">Pytorch dataset &amp; dataloader</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MyDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">src_lang</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">src</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tgt_lang</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">tgt</span><span class="sh">'</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">src_lang</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">return</span>  <span class="p">{</span>
            <span class="sh">'</span><span class="s">src</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">src_lang</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">tgt</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">tgt_lang</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="p">}</span>

<span class="k">def</span> <span class="nf">collate_fn</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="n">src_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">src</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">tgt_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">tgt</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>

    <span class="c1">#padding
</span>    <span class="n">src_batch_padded</span> <span class="o">=</span> <span class="nf">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">src_batch</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">tgt_batch_padded</span> <span class="o">=</span> <span class="nf">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tgt_batch</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  
    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">src</span><span class="sh">'</span><span class="p">:</span> <span class="n">src_batch_padded</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">tgt</span><span class="sh">'</span><span class="p">:</span> <span class="n">tgt_batch_padded</span><span class="p">,</span>
    <span class="p">}</span>


  <span class="c1"># Create instances of MyDataset
</span>    <span class="n">train_dataset</span> <span class="o">=</span> <span class="nc">MyDataset</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
    <span class="n">validation_dataset</span> <span class="o">=</span> <span class="nc">MyDataset</span><span class="p">(</span><span class="n">validation_data</span><span class="p">)</span>

    <span class="c1"># Create data loaders
</span>    <span class="n">train_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">)</span>
    <span class="n">validation_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">validation_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="training-1">Training</h3>

<p>Save the checkpoints</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Setup TensorBoard writer and device
</span>    <span class="n">writer</span> <span class="o">=</span> <span class="nc">SummaryWriter</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Initialize model components
</span>    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">6</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
    <span class="n">d_ff</span> <span class="o">=</span> <span class="mi">1024</span>
    <span class="n">encoder</span> <span class="o">=</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_layer</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="n">d_ff</span><span class="p">)</span>
    <span class="n">decoder</span> <span class="o">=</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_layer</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="n">d_ff</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">device</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Setup training essentials
</span>    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="nc">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="nc">GradScaler</span><span class="p">()</span>

    <span class="c1"># Training loop
</span>    <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">30</span>
    <span class="n">checkpoints_dir</span> <span class="o">=</span> <span class="sh">'</span><span class="s">../checkpoints</span><span class="sh">'</span>
    <span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">checkpoints_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">best_vloss</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">inf</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">EPOCH </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">:</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">epoch_avg_loss</span> <span class="o">=</span> <span class="nf">train_one_epoch</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">writer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">scaler</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Average Training Loss: </span><span class="si">{</span><span class="n">epoch_avg_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">avg_vloss</span> <span class="o">=</span> <span class="nf">validate</span><span class="p">(</span><span class="n">validation_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Validation Loss: </span><span class="si">{</span><span class="n">avg_vloss</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">writer</span><span class="p">.</span><span class="nf">add_scalars</span><span class="p">(</span><span class="sh">'</span><span class="s">Training vs. Validation Loss</span><span class="sh">'</span><span class="p">,</span> <span class="p">{</span><span class="sh">'</span><span class="s">Training</span><span class="sh">'</span><span class="p">:</span> <span class="n">epoch_avg_loss</span><span class="p">,</span> <span class="sh">'</span><span class="s">Validation</span><span class="sh">'</span><span class="p">:</span> <span class="n">avg_vloss</span><span class="p">},</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">writer</span><span class="p">.</span><span class="nf">flush</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">avg_vloss</span> <span class="o">&lt;</span> <span class="n">best_vloss</span><span class="p">:</span>
            <span class="n">best_vloss</span> <span class="o">=</span> <span class="n">avg_vloss</span>
            <span class="n">model_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">checkpoints_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="s">model_epoch_</span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">.pt</span><span class="sh">'</span><span class="p">)</span>
            <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="n">model_filename</span><span class="p">)</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Model saved: </span><span class="si">{</span><span class="n">model_filename</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">scheduler</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Learning Rate: </span><span class="si">{</span><span class="n">scheduler</span><span class="p">.</span><span class="nf">get_last_lr</span><span class="p">()</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>

    <span class="n">writer</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="inference">Inference</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">translate</span><span class="p">(</span><span class="n">src_sentence</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">sp</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

    <span class="c1"># Tokenize the source sentence
</span>    <span class="n">src_tokens</span> <span class="o">=</span> <span class="n">sp</span><span class="p">.</span><span class="nf">encode_as_ids</span><span class="p">(</span><span class="n">src_sentence</span><span class="p">)</span>
    <span class="n">src_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">(</span><span class="n">src_tokens</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">src_mask</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">padding_mask</span><span class="p">(</span><span class="n">src_tensor</span><span class="p">)</span>
        <span class="n">memory</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">src_tensor</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>

    <span class="n">trg_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">sp</span><span class="p">.</span><span class="nf">bos_id</span><span class="p">()]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>
        <span class="n">trg_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">(</span><span class="n">trg_indexes</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">target_mask</span><span class="p">(</span><span class="n">trg_tensor</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">trg_tensor</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">)</span>
            <span class="n">pred_token</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>
            <span class="n">trg_indexes</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">pred_token</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pred_token</span> <span class="o">==</span> <span class="n">sp</span><span class="p">.</span><span class="nf">eos_id</span><span class="p">():</span>
            <span class="k">break</span>
	<span class="n">trg_tokens</span> <span class="o">=</span> <span class="n">sp</span><span class="p">.</span><span class="nf">decode_ids</span><span class="p">(</span><span class="n">trg_indexes</span><span class="p">)</span>
 	<span class="k">return</span> <span class="n">trg_tokens</span>

<span class="c1">#Initialize the model and import the tokenizer and checkpoint with the best performance
</span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">d_ff</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_layer</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="n">d_ff</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_layer</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="n">d_ff</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">device</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">parent_dir</span><span class="p">,</span> <span class="sh">'</span><span class="s">checkpoints</span><span class="sh">'</span><span class="p">,</span> <span class="n">src_lang</span> <span class="o">+</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span>  <span class="o">+</span> <span class="n">tgt_lang</span><span class="p">,</span> <span class="sh">'</span><span class="s">model.pt</span><span class="sh">'</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">)))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

<span class="n">spm_model_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">parent_dir</span><span class="p">,</span> <span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">,</span> <span class="n">src_lang</span> <span class="o">+</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span> <span class="o">+</span> <span class="n">tgt_lang</span><span class="p">,</span> <span class="sh">'</span><span class="s">bpe</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">bpe.model</span><span class="sh">'</span><span class="p">)</span>
<span class="n">sp</span> <span class="o">=</span> <span class="n">spm</span><span class="p">.</span><span class="nc">SentencePieceProcessor</span><span class="p">(</span><span class="n">model_file</span><span class="o">=</span><span class="n">spm_model_path</span><span class="p">)</span>
<span class="n">translated_sentence</span> <span class="o">=</span> <span class="nf">translate</span><span class="p">(</span><span class="n">src_sentence</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">sp</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="translate-examples">Translate Examples</h3>

<p>Heres the examples of translating english into Spanish, German and Chinese</p>

<p><img src="/tingaldama/assets/images/translation_model/trans-examples.png" alt="trans-examples" /></p>

            </div>
          </article>
        </main>

      </section>
      <div id="title" style="text-align: center;">
    
    <hr>
    <span class="credits left">Project maintained by <a href=""></a></span>
    <br>
    <br>
    <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href=""></a></span>

    <p>&copy; 2024 Tingaldama. All rights reserved.</p>
  </div>
    </div>
  </body>
</html>
