<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>In-Depth Guide to Cross Entropy Loss | Tingaldama</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="In-Depth Guide to Cross Entropy Loss" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Entropy originated in the field of thermodynamics and was later adapted to information theory by Claude Shannon. Its used to measure uncertainty or unpredictabilityin a system. Before this, there was no formal way to quantify the information needed to describe a system’s state. You might wonder if this ‘uncertainty’ is the same as the uncertainty we experience in daily life, like being unsure if someone will come to dinner. It’s similar but not the same… In everyday use, ‘uncertainty’ means having doubts or a lack of confidence. In information theory, ‘uncertainty’ is a precise measure of how much information or surprise an outcome holds, based on the probability distribution of a random variable . Let’s dive into an example. Lets simulate a fair coin toss for 1000 times: import random # Function to simulate a fair coin toss def coin_toss(): return &quot;Heads&quot; if random.choice([True, False]) else &quot;Tails&quot; # Simulate 1000 coin tosses tosses = [coin_toss() for _ in range(1000)] # Count the number of heads and tails heads_count = tosses.count(&quot;Heads&quot;) tails_count = tosses.count(&quot;Tails&quot;) # Print the results print(f&quot;Heads: {heads_count}&quot;) print(f&quot;Tails: {tails_count}&quot;) -------------- Heads: 488 Tails: 512 The probability: Getting a heads is: P(H) = 488 / 1000 = 0.488 Getting a tail is: P(T) = 512 / 1000 = 0.512 Now lets simulate a unfair coin toss for 100 times: import random # Function to simulate an unfair coin toss def unfair_coin_toss(): return &quot;Tails&quot; if random.random() &lt; 5/6 else &quot;Heads&quot; # Simulate 1000 coin tosses tosses = [unfair_coin_toss() for _ in range(1000)] # Count the number of heads and tails heads_count = tosses.count(&quot;Heads&quot;) tails_count = tosses.count(&quot;Tails&quot;) # Print the results print(f&quot;Heads: {heads_count}&quot;) print(f&quot;Tails: {tails_count}&quot;) ---------- Heads: 177 Tails: 823 The probability: Getting a heads is: P(H) = 177 / 1000 = 0.177 Getting a tail is: P(T) = 823 / 1000 = 0.823 Calculate Entropy in both scenarios using this Entropy formula: [H(X)=−∑iP(xi)logP(xi)] Fair coin: [H(X)=−∑iP(xi)logP(xi) = -(0.488*log(0.488) + 0.512 * log(0.512)) = 0.9996] Unfair coin: [H(X)=−∑iP(xi)logP(xi) = -(0.177*log(0.177) + 0.823 * log(0.823)) = 0.6735] In the scenario with a fair coin, the entropy is higher, reflecting greater uncertainty compared to the biased coin scenario. This aligns with our intuition: when two events are equally likely, uncertainty is at its maximum. Cross-Entropy Loss in Machine Learning In the context of machine learning, entropy is used to measure the uncertainty in predictions. For a perfect model, the predicted probability distribution matches the actual distribution, resulting in low entropy. Conversely, higher entropy indicates a higher degree of uncertainty in the predictions. This Cross-Entropy loss function is widely used in classification problems because it quantifies how well the predicted probabilities match the true class labels. Cross-entropy loss builds upon the concept of entropy. It measures the difference between two probability distributions. Consider a binary classification problem where we predict whether an email is spam (1) or not spam (0). Suppose we have the following true labels and predicted probabilities: True label: 1 (spam) Predicted probability: 0.8 (spam) The cross-entropy loss for this single prediction is: [H(P, Q) = -[1 \cdot \log(0.8) + 0 \cdot \log(0.2)] = -\log(0.8) \approx 0.22] This value indicates the level of uncertainty in our prediction. If our model were perfect, the predicted probability would be 1, and the loss would be zero, indicating no uncertainty. Binary cross-entropy loss penalizes incorrect predictions, encouraging the model to output probabilities closer to the true labels. Cross-Entropy Loss in Categorical Classification In multi-class classification problems, cross-entropy loss is extended to categorical cross-entropy loss. Here, the loss function compares the predicted probability distribution over multiple classes with the true distribution. For a classification problem with K classes, the categorical cross-entropy loss is: For example, in a three-class classification problem (A, B, C) with true label B and predicted probabilities [0.1, 0.7, 0.2], the categorical cross-entropy loss is: [L = -[0 \cdot \log(0.1) + 1 \cdot \log(0.7) + 0 \cdot \log(0.2)] = -\log(0.7) \approx 0.36] Cross-Entropy Loss in language model Let’s take a look at an example of how Cross-Entropy loss is used in an encoder-decoder translation model. We’ll start by using a trained translation model to generate the first token in the target language. Python # encode the src setence and predict the first token in tgt language src_sentence = &#39;Tell me something about yourself&#39; encoded_src = torch.LongTensor(tokenizer.encode_as_ids(src_sentence)).unsqueeze(0 #the output of the encoder memory = model.encoder(src_sentence, padding_mask) #the first token in the tgt sentence (bos token) x = tokenizer.bos_id() tgt_tensor = torch.LongTensor([x]).unsqueeze(0) #feed both memory and x into decoder, the output of decoder are logits with shape(1, 1, vocab_size) with torch.no_grad(): logits = model.decoder(tgt_tenosr, memory, padding_mask, target_mask) The shape of output logits is (batch_size, sequence_length, vocab_size): torch.Size([1, 1, 10000]) Then apply the softmax function to the logits generated by the model. This converts the logits into a probability distribution over the entire vocabulary. Find the index of the token in the vocabulary that has the highest probability. import torch.nn as nn import torch sm = nn.Softmax(dim = -1) prob = sm(logits) print(prob) print(torch.argmax(prob)) ----tensor(900) print(prob[0][0][900]) -----tensor(0.5717) print(tokenizer.decode_ids(900)) ----&#39;告诉&#39; The predicted token is ‘告诉‘, whcih aligns with our the first token of the tgt sentence. tgt_sen = &#39;告诉我关于你的事情&#39; However, since our model only assigns a probability of 0.5717 to the correct token, which indicates a relatively low confidence, we can calculate the loss using the CrossEntropyLoss equation. The loss is given by -log(0.5717). Let’s compute the loss using PyTorch’s CrossEntropyLoss() function. # now we calculate the loss for the &#39;correct&#39; prediction with lower confidence criterion = nn.CrossEntropyLoss() print(torch.LongTensor([tgt_ids[0]]).view(-1)) loss = criterion(logits.view(1,-1), torch.LongTensor([tgt_ids[0]]).view(-1)) print(loss) --- tensor(0.5591) Let’s explore two different scenarios: one where our model makes an incorrect prediction and another where it makes a correct prediction with high confidence. For the incorrect prediction, the loss is calculated as -log(4.6965e-07) ≈ 16.2224. For the correct prediction with high confidence, the loss is -log(1.0) ≈ 0 #change the logits on index 900 to make the predicted token not the token 900 logits[0][0][900] /= 100 sm = nn.Softmax(dim = -1) prob = sm(logits) print(torch.argmax(prob)) ____1114 #notice that the predicted token is 1114 instead of token 900 now #now we calculate the loss for this &quot;wrong&quot; prediction criterion = nn.CrossEntropyLoss() loss = criterion(logits.view(1,-1), torch.LongTensor([tgt_ids[0]]).view(-1)) print(loss) ------16.2224 logits[0][0][900] *= 10 sm = nn.Softmax(dim = -1) prob = sm(logits) print(torch.argmax(prob)) ----900 print(prob[0][0][900] ----tensor(1.) #now we calculate the loss for correct prediction with absolute confidence criterion = nn.CrossEntropyLoss() loss = criterion(logits.view(1,-1), torch.LongTensor([tgt_ids[0]]).view(-1)) print(loss) ----tensor(0.) For the entire source code of this translation model: reference my Github repo: https://github.com/tingaldama278/English-Chinese-Translation-App Conclusion Cross-entropy loss is a powerful tool in machine learning, leveraging the concept of entropy from Shannon’s Information Theory to measure the uncertainty in predictions. By penalizing incorrect predictions and rewarding correct ones, it guides models to improve their accuracy in both binary and categorical classification tasks. Understanding and effectively implementing cross-entropy loss is crucial for developing robust and reliable predictive models" />
<meta property="og:description" content="Entropy originated in the field of thermodynamics and was later adapted to information theory by Claude Shannon. Its used to measure uncertainty or unpredictabilityin a system. Before this, there was no formal way to quantify the information needed to describe a system’s state. You might wonder if this ‘uncertainty’ is the same as the uncertainty we experience in daily life, like being unsure if someone will come to dinner. It’s similar but not the same… In everyday use, ‘uncertainty’ means having doubts or a lack of confidence. In information theory, ‘uncertainty’ is a precise measure of how much information or surprise an outcome holds, based on the probability distribution of a random variable . Let’s dive into an example. Lets simulate a fair coin toss for 1000 times: import random # Function to simulate a fair coin toss def coin_toss(): return &quot;Heads&quot; if random.choice([True, False]) else &quot;Tails&quot; # Simulate 1000 coin tosses tosses = [coin_toss() for _ in range(1000)] # Count the number of heads and tails heads_count = tosses.count(&quot;Heads&quot;) tails_count = tosses.count(&quot;Tails&quot;) # Print the results print(f&quot;Heads: {heads_count}&quot;) print(f&quot;Tails: {tails_count}&quot;) -------------- Heads: 488 Tails: 512 The probability: Getting a heads is: P(H) = 488 / 1000 = 0.488 Getting a tail is: P(T) = 512 / 1000 = 0.512 Now lets simulate a unfair coin toss for 100 times: import random # Function to simulate an unfair coin toss def unfair_coin_toss(): return &quot;Tails&quot; if random.random() &lt; 5/6 else &quot;Heads&quot; # Simulate 1000 coin tosses tosses = [unfair_coin_toss() for _ in range(1000)] # Count the number of heads and tails heads_count = tosses.count(&quot;Heads&quot;) tails_count = tosses.count(&quot;Tails&quot;) # Print the results print(f&quot;Heads: {heads_count}&quot;) print(f&quot;Tails: {tails_count}&quot;) ---------- Heads: 177 Tails: 823 The probability: Getting a heads is: P(H) = 177 / 1000 = 0.177 Getting a tail is: P(T) = 823 / 1000 = 0.823 Calculate Entropy in both scenarios using this Entropy formula: [H(X)=−∑iP(xi)logP(xi)] Fair coin: [H(X)=−∑iP(xi)logP(xi) = -(0.488*log(0.488) + 0.512 * log(0.512)) = 0.9996] Unfair coin: [H(X)=−∑iP(xi)logP(xi) = -(0.177*log(0.177) + 0.823 * log(0.823)) = 0.6735] In the scenario with a fair coin, the entropy is higher, reflecting greater uncertainty compared to the biased coin scenario. This aligns with our intuition: when two events are equally likely, uncertainty is at its maximum. Cross-Entropy Loss in Machine Learning In the context of machine learning, entropy is used to measure the uncertainty in predictions. For a perfect model, the predicted probability distribution matches the actual distribution, resulting in low entropy. Conversely, higher entropy indicates a higher degree of uncertainty in the predictions. This Cross-Entropy loss function is widely used in classification problems because it quantifies how well the predicted probabilities match the true class labels. Cross-entropy loss builds upon the concept of entropy. It measures the difference between two probability distributions. Consider a binary classification problem where we predict whether an email is spam (1) or not spam (0). Suppose we have the following true labels and predicted probabilities: True label: 1 (spam) Predicted probability: 0.8 (spam) The cross-entropy loss for this single prediction is: [H(P, Q) = -[1 \cdot \log(0.8) + 0 \cdot \log(0.2)] = -\log(0.8) \approx 0.22] This value indicates the level of uncertainty in our prediction. If our model were perfect, the predicted probability would be 1, and the loss would be zero, indicating no uncertainty. Binary cross-entropy loss penalizes incorrect predictions, encouraging the model to output probabilities closer to the true labels. Cross-Entropy Loss in Categorical Classification In multi-class classification problems, cross-entropy loss is extended to categorical cross-entropy loss. Here, the loss function compares the predicted probability distribution over multiple classes with the true distribution. For a classification problem with K classes, the categorical cross-entropy loss is: For example, in a three-class classification problem (A, B, C) with true label B and predicted probabilities [0.1, 0.7, 0.2], the categorical cross-entropy loss is: [L = -[0 \cdot \log(0.1) + 1 \cdot \log(0.7) + 0 \cdot \log(0.2)] = -\log(0.7) \approx 0.36] Cross-Entropy Loss in language model Let’s take a look at an example of how Cross-Entropy loss is used in an encoder-decoder translation model. We’ll start by using a trained translation model to generate the first token in the target language. Python # encode the src setence and predict the first token in tgt language src_sentence = &#39;Tell me something about yourself&#39; encoded_src = torch.LongTensor(tokenizer.encode_as_ids(src_sentence)).unsqueeze(0 #the output of the encoder memory = model.encoder(src_sentence, padding_mask) #the first token in the tgt sentence (bos token) x = tokenizer.bos_id() tgt_tensor = torch.LongTensor([x]).unsqueeze(0) #feed both memory and x into decoder, the output of decoder are logits with shape(1, 1, vocab_size) with torch.no_grad(): logits = model.decoder(tgt_tenosr, memory, padding_mask, target_mask) The shape of output logits is (batch_size, sequence_length, vocab_size): torch.Size([1, 1, 10000]) Then apply the softmax function to the logits generated by the model. This converts the logits into a probability distribution over the entire vocabulary. Find the index of the token in the vocabulary that has the highest probability. import torch.nn as nn import torch sm = nn.Softmax(dim = -1) prob = sm(logits) print(prob) print(torch.argmax(prob)) ----tensor(900) print(prob[0][0][900]) -----tensor(0.5717) print(tokenizer.decode_ids(900)) ----&#39;告诉&#39; The predicted token is ‘告诉‘, whcih aligns with our the first token of the tgt sentence. tgt_sen = &#39;告诉我关于你的事情&#39; However, since our model only assigns a probability of 0.5717 to the correct token, which indicates a relatively low confidence, we can calculate the loss using the CrossEntropyLoss equation. The loss is given by -log(0.5717). Let’s compute the loss using PyTorch’s CrossEntropyLoss() function. # now we calculate the loss for the &#39;correct&#39; prediction with lower confidence criterion = nn.CrossEntropyLoss() print(torch.LongTensor([tgt_ids[0]]).view(-1)) loss = criterion(logits.view(1,-1), torch.LongTensor([tgt_ids[0]]).view(-1)) print(loss) --- tensor(0.5591) Let’s explore two different scenarios: one where our model makes an incorrect prediction and another where it makes a correct prediction with high confidence. For the incorrect prediction, the loss is calculated as -log(4.6965e-07) ≈ 16.2224. For the correct prediction with high confidence, the loss is -log(1.0) ≈ 0 #change the logits on index 900 to make the predicted token not the token 900 logits[0][0][900] /= 100 sm = nn.Softmax(dim = -1) prob = sm(logits) print(torch.argmax(prob)) ____1114 #notice that the predicted token is 1114 instead of token 900 now #now we calculate the loss for this &quot;wrong&quot; prediction criterion = nn.CrossEntropyLoss() loss = criterion(logits.view(1,-1), torch.LongTensor([tgt_ids[0]]).view(-1)) print(loss) ------16.2224 logits[0][0][900] *= 10 sm = nn.Softmax(dim = -1) prob = sm(logits) print(torch.argmax(prob)) ----900 print(prob[0][0][900] ----tensor(1.) #now we calculate the loss for correct prediction with absolute confidence criterion = nn.CrossEntropyLoss() loss = criterion(logits.view(1,-1), torch.LongTensor([tgt_ids[0]]).view(-1)) print(loss) ----tensor(0.) For the entire source code of this translation model: reference my Github repo: https://github.com/tingaldama278/English-Chinese-Translation-App Conclusion Cross-entropy loss is a powerful tool in machine learning, leveraging the concept of entropy from Shannon’s Information Theory to measure the uncertainty in predictions. By penalizing incorrect predictions and rewarding correct ones, it guides models to improve their accuracy in both binary and categorical classification tasks. Understanding and effectively implementing cross-entropy loss is crucial for developing robust and reliable predictive models" />
<link rel="canonical" href="http://localhost:4000/tingaldama1/cross-entropy-loss/language-model/2024/02/21/Understanding-Cross-Entropy-Loss.html" />
<meta property="og:url" content="http://localhost:4000/tingaldama1/cross-entropy-loss/language-model/2024/02/21/Understanding-Cross-Entropy-Loss.html" />
<meta property="og:site_name" content="Tingaldama" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-02-21T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="In-Depth Guide to Cross Entropy Loss" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-02-21T00:00:00+01:00","datePublished":"2024-02-21T00:00:00+01:00","description":"Entropy originated in the field of thermodynamics and was later adapted to information theory by Claude Shannon. Its used to measure uncertainty or unpredictabilityin a system. Before this, there was no formal way to quantify the information needed to describe a system’s state. You might wonder if this ‘uncertainty’ is the same as the uncertainty we experience in daily life, like being unsure if someone will come to dinner. It’s similar but not the same… In everyday use, ‘uncertainty’ means having doubts or a lack of confidence. In information theory, ‘uncertainty’ is a precise measure of how much information or surprise an outcome holds, based on the probability distribution of a random variable . Let’s dive into an example. Lets simulate a fair coin toss for 1000 times: import random # Function to simulate a fair coin toss def coin_toss(): return &quot;Heads&quot; if random.choice([True, False]) else &quot;Tails&quot; # Simulate 1000 coin tosses tosses = [coin_toss() for _ in range(1000)] # Count the number of heads and tails heads_count = tosses.count(&quot;Heads&quot;) tails_count = tosses.count(&quot;Tails&quot;) # Print the results print(f&quot;Heads: {heads_count}&quot;) print(f&quot;Tails: {tails_count}&quot;) -------------- Heads: 488 Tails: 512 The probability: Getting a heads is: P(H) = 488 / 1000 = 0.488 Getting a tail is: P(T) = 512 / 1000 = 0.512 Now lets simulate a unfair coin toss for 100 times: import random # Function to simulate an unfair coin toss def unfair_coin_toss(): return &quot;Tails&quot; if random.random() &lt; 5/6 else &quot;Heads&quot; # Simulate 1000 coin tosses tosses = [unfair_coin_toss() for _ in range(1000)] # Count the number of heads and tails heads_count = tosses.count(&quot;Heads&quot;) tails_count = tosses.count(&quot;Tails&quot;) # Print the results print(f&quot;Heads: {heads_count}&quot;) print(f&quot;Tails: {tails_count}&quot;) ---------- Heads: 177 Tails: 823 The probability: Getting a heads is: P(H) = 177 / 1000 = 0.177 Getting a tail is: P(T) = 823 / 1000 = 0.823 Calculate Entropy in both scenarios using this Entropy formula: [H(X)=−∑iP(xi)logP(xi)] Fair coin: [H(X)=−∑iP(xi)logP(xi) = -(0.488*log(0.488) + 0.512 * log(0.512)) = 0.9996] Unfair coin: [H(X)=−∑iP(xi)logP(xi) = -(0.177*log(0.177) + 0.823 * log(0.823)) = 0.6735] In the scenario with a fair coin, the entropy is higher, reflecting greater uncertainty compared to the biased coin scenario. This aligns with our intuition: when two events are equally likely, uncertainty is at its maximum. Cross-Entropy Loss in Machine Learning In the context of machine learning, entropy is used to measure the uncertainty in predictions. For a perfect model, the predicted probability distribution matches the actual distribution, resulting in low entropy. Conversely, higher entropy indicates a higher degree of uncertainty in the predictions. This Cross-Entropy loss function is widely used in classification problems because it quantifies how well the predicted probabilities match the true class labels. Cross-entropy loss builds upon the concept of entropy. It measures the difference between two probability distributions. Consider a binary classification problem where we predict whether an email is spam (1) or not spam (0). Suppose we have the following true labels and predicted probabilities: True label: 1 (spam) Predicted probability: 0.8 (spam) The cross-entropy loss for this single prediction is: [H(P, Q) = -[1 \\cdot \\log(0.8) + 0 \\cdot \\log(0.2)] = -\\log(0.8) \\approx 0.22] This value indicates the level of uncertainty in our prediction. If our model were perfect, the predicted probability would be 1, and the loss would be zero, indicating no uncertainty. Binary cross-entropy loss penalizes incorrect predictions, encouraging the model to output probabilities closer to the true labels. Cross-Entropy Loss in Categorical Classification In multi-class classification problems, cross-entropy loss is extended to categorical cross-entropy loss. Here, the loss function compares the predicted probability distribution over multiple classes with the true distribution. For a classification problem with K classes, the categorical cross-entropy loss is: For example, in a three-class classification problem (A, B, C) with true label B and predicted probabilities [0.1, 0.7, 0.2], the categorical cross-entropy loss is: [L = -[0 \\cdot \\log(0.1) + 1 \\cdot \\log(0.7) + 0 \\cdot \\log(0.2)] = -\\log(0.7) \\approx 0.36] Cross-Entropy Loss in language model Let’s take a look at an example of how Cross-Entropy loss is used in an encoder-decoder translation model. We’ll start by using a trained translation model to generate the first token in the target language. Python # encode the src setence and predict the first token in tgt language src_sentence = &#39;Tell me something about yourself&#39; encoded_src = torch.LongTensor(tokenizer.encode_as_ids(src_sentence)).unsqueeze(0 #the output of the encoder memory = model.encoder(src_sentence, padding_mask) #the first token in the tgt sentence (bos token) x = tokenizer.bos_id() tgt_tensor = torch.LongTensor([x]).unsqueeze(0) #feed both memory and x into decoder, the output of decoder are logits with shape(1, 1, vocab_size) with torch.no_grad(): logits = model.decoder(tgt_tenosr, memory, padding_mask, target_mask) The shape of output logits is (batch_size, sequence_length, vocab_size): torch.Size([1, 1, 10000]) Then apply the softmax function to the logits generated by the model. This converts the logits into a probability distribution over the entire vocabulary. Find the index of the token in the vocabulary that has the highest probability. import torch.nn as nn import torch sm = nn.Softmax(dim = -1) prob = sm(logits) print(prob) print(torch.argmax(prob)) ----tensor(900) print(prob[0][0][900]) -----tensor(0.5717) print(tokenizer.decode_ids(900)) ----&#39;告诉&#39; The predicted token is ‘告诉‘, whcih aligns with our the first token of the tgt sentence. tgt_sen = &#39;告诉我关于你的事情&#39; However, since our model only assigns a probability of 0.5717 to the correct token, which indicates a relatively low confidence, we can calculate the loss using the CrossEntropyLoss equation. The loss is given by -log(0.5717). Let’s compute the loss using PyTorch’s CrossEntropyLoss() function. # now we calculate the loss for the &#39;correct&#39; prediction with lower confidence criterion = nn.CrossEntropyLoss() print(torch.LongTensor([tgt_ids[0]]).view(-1)) loss = criterion(logits.view(1,-1), torch.LongTensor([tgt_ids[0]]).view(-1)) print(loss) --- tensor(0.5591) Let’s explore two different scenarios: one where our model makes an incorrect prediction and another where it makes a correct prediction with high confidence. For the incorrect prediction, the loss is calculated as -log(4.6965e-07) ≈ 16.2224. For the correct prediction with high confidence, the loss is -log(1.0) ≈ 0 #change the logits on index 900 to make the predicted token not the token 900 logits[0][0][900] /= 100 sm = nn.Softmax(dim = -1) prob = sm(logits) print(torch.argmax(prob)) ____1114 #notice that the predicted token is 1114 instead of token 900 now #now we calculate the loss for this &quot;wrong&quot; prediction criterion = nn.CrossEntropyLoss() loss = criterion(logits.view(1,-1), torch.LongTensor([tgt_ids[0]]).view(-1)) print(loss) ------16.2224 logits[0][0][900] *= 10 sm = nn.Softmax(dim = -1) prob = sm(logits) print(torch.argmax(prob)) ----900 print(prob[0][0][900] ----tensor(1.) #now we calculate the loss for correct prediction with absolute confidence criterion = nn.CrossEntropyLoss() loss = criterion(logits.view(1,-1), torch.LongTensor([tgt_ids[0]]).view(-1)) print(loss) ----tensor(0.) For the entire source code of this translation model: reference my Github repo: https://github.com/tingaldama278/English-Chinese-Translation-App Conclusion Cross-entropy loss is a powerful tool in machine learning, leveraging the concept of entropy from Shannon’s Information Theory to measure the uncertainty in predictions. By penalizing incorrect predictions and rewarding correct ones, it guides models to improve their accuracy in both binary and categorical classification tasks. Understanding and effectively implementing cross-entropy loss is crucial for developing robust and reliable predictive models","headline":"In-Depth Guide to Cross Entropy Loss","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/tingaldama1/cross-entropy-loss/language-model/2024/02/21/Understanding-Cross-Entropy-Loss.html"},"url":"http://localhost:4000/tingaldama1/cross-entropy-loss/language-model/2024/02/21/Understanding-Cross-Entropy-Loss.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/tingaldama1/assets/css/style.css?v=">
    <script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
    <script src="/tingaldama1/assets/js/respond.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="/tingaldama1/assets/css/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup theme-color -->
<!-- start theme color meta headers -->
<meta name="theme-color" content="#353535">
<meta name="msapplication-navbutton-color" content="#353535">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- end theme color meta headers -->


<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/tingaldama1/favicon.ico" -->

<!-- end custom head snippets -->
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<link rel="shortcut icon" href="/tingaldama1//assets/images/favicon.jpg" type="image/x-icon">


  </head>
  <style>
    .categories{
  font-weight: bold;
  color: #949494;
}
 .date{
  font-weight: bold !important;
  color: #949494 !important;
}

  </style>
  <body>
    
<div id="header">
    <nav>
      <ul>
        <li class="fork"><a href="/tingaldama1/">Home</a></li>
        <li class="downloads"><a href="/tingaldama1/contact">Contacts</a></li>
        <li class="downloads"><a href="/tingaldama1/about">About</a></li>
        <li class="downloads"><a href="/tingaldama1/blog">Blogs</a></li>
        <li class="downloads"><a href="/tingaldama1/mlps-resources">MLOps Resources</a></li>
        <li class="downloads"><a href="/tingaldama1/personal-growth">Personal Growth</a></li>
          
      </ul>
    </nav>
  </div><!-- end header -->

    <div class="wrapper">

      <section>
        

        <main>
          <article>
            <h1>In-Depth Guide to Cross Entropy Loss</h1>
            <div class="post-meta">
             <span class="date">Date: February 21, 2024</span>
      <br>
      <span class="categories">
        Categories: cross-entropy-loss | language-model
      </span>
            </div>
            <br>
            <div class="post-content">
              
              <p>Entropy originated in the field of thermodynamics and was later adapted to information theory by Claude Shannon. Its used to measure <strong>uncertainty or unpredictability</strong>in a system. Before this, there was no formal way to <strong>quantify the information</strong> needed to describe a system’s state. You might wonder if this ‘uncertainty’ is the same as the uncertainty we experience in daily life, like being unsure if someone will come to dinner. It’s similar but not the same…</p>

<p>In everyday use, ‘uncertainty’ means having doubts or a lack of confidence.  <strong>In information theory, ‘uncertainty’ is a precise measure of how much information or surprise an outcome holds, based on the probability distribution of a random variable</strong> . Let’s dive into an example.</p>

<p>Lets simulate a fair coin toss for 1000 times:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">random</span>

<span class="c1"># Function to simulate a fair coin toss
</span><span class="k">def</span> <span class="nf">coin_toss</span><span class="p">():</span>
    <span class="k">return</span> <span class="sh">"</span><span class="s">Heads</span><span class="sh">"</span> <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">([</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">])</span> <span class="k">else</span> <span class="sh">"</span><span class="s">Tails</span><span class="sh">"</span>

<span class="c1"># Simulate 1000 coin tosses
</span><span class="n">tosses</span> <span class="o">=</span> <span class="p">[</span><span class="nf">coin_toss</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)]</span>

<span class="c1"># Count the number of heads and tails
</span><span class="n">heads_count</span> <span class="o">=</span> <span class="n">tosses</span><span class="p">.</span><span class="nf">count</span><span class="p">(</span><span class="sh">"</span><span class="s">Heads</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tails_count</span> <span class="o">=</span> <span class="n">tosses</span><span class="p">.</span><span class="nf">count</span><span class="p">(</span><span class="sh">"</span><span class="s">Tails</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Print the results
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Heads: </span><span class="si">{</span><span class="n">heads_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Tails: </span><span class="si">{</span><span class="n">tails_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="o">--------------</span>
<span class="n">Heads</span><span class="p">:</span> <span class="mi">488</span>
<span class="n">Tails</span><span class="p">:</span> <span class="mi">512</span>

</code></pre></div></div>

<p>The probability:</p>

<p>Getting a heads is: P(H) = 488 / 1000 = 0.488</p>

<p>Getting a tail is: P(T) = 512 / 1000 = 0.512</p>

<p>Now lets simulate a unfair coin toss for 100 times:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">random</span>

<span class="c1"># Function to simulate an unfair coin toss
</span><span class="k">def</span> <span class="nf">unfair_coin_toss</span><span class="p">():</span>
    <span class="k">return</span> <span class="sh">"</span><span class="s">Tails</span><span class="sh">"</span> <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="o">/</span><span class="mi">6</span> <span class="k">else</span> <span class="sh">"</span><span class="s">Heads</span><span class="sh">"</span>

<span class="c1"># Simulate 1000 coin tosses
</span><span class="n">tosses</span> <span class="o">=</span> <span class="p">[</span><span class="nf">unfair_coin_toss</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)]</span>

<span class="c1"># Count the number of heads and tails
</span><span class="n">heads_count</span> <span class="o">=</span> <span class="n">tosses</span><span class="p">.</span><span class="nf">count</span><span class="p">(</span><span class="sh">"</span><span class="s">Heads</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tails_count</span> <span class="o">=</span> <span class="n">tosses</span><span class="p">.</span><span class="nf">count</span><span class="p">(</span><span class="sh">"</span><span class="s">Tails</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Print the results
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Heads: </span><span class="si">{</span><span class="n">heads_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Tails: </span><span class="si">{</span><span class="n">tails_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="o">----------</span>
<span class="n">Heads</span><span class="p">:</span> <span class="mi">177</span>
<span class="n">Tails</span><span class="p">:</span> <span class="mi">823</span>
</code></pre></div></div>

<p>The probability:</p>

<p>Getting a heads is: P(H) = 177 / 1000 = 0.177</p>

<p>Getting a tail is: P(T) = 823 / 1000 = 0.823</p>

<p>Calculate Entropy in both scenarios using this Entropy formula:</p>

\[H(X)=−∑iP(xi)logP(xi)\]

<p>Fair coin:</p>

\[H(X)=−∑iP(xi)logP(xi) = -(0.488*log(0.488) + 0.512 * log(0.512)) = 0.9996\]

<p>Unfair coin:</p>

\[H(X)=−∑iP(xi)logP(xi) = -(0.177*log(0.177) + 0.823 * log(0.823)) = 0.6735\]

<p>In the scenario with a fair coin, the entropy is higher, reflecting greater uncertainty compared to the biased coin scenario. This aligns with our intuition: when two events are equally likely, uncertainty is at its maximum.</p>

<hr />

<h3 id="cross-entropy-loss-in-machine-learning"><strong>Cross-Entropy Loss in Machine Learning</strong></h3>

<p>In the context of machine learning, entropy is used to measure the uncertainty in predictions. For a perfect model, the predicted probability distribution matches the actual distribution, resulting in low entropy. Conversely, higher entropy indicates a higher degree of uncertainty in the predictions. This Cross-Entropy loss function is widely used in classification problems because it quantifies how well the predicted probabilities match the true class labels. <strong>Cross-entropy loss builds upon the concept of entropy. It measures the difference between two probability distributions.</strong></p>

<p>Consider a binary classification problem where we predict whether an email is spam (1) or not spam (0). Suppose we have the following true labels and predicted probabilities:</p>

<ul>
  <li>True label: 1 (spam)</li>
  <li>Predicted probability: 0.8 (spam)</li>
</ul>

<p>The cross-entropy loss for this single prediction is:</p>

\[H(P, Q) = -[1 \cdot \log(0.8) + 0 \cdot \log(0.2)] = -\log(0.8) \approx 0.22\]

<p>This value indicates the level of uncertainty in our prediction. If our model were perfect, the predicted probability would be 1, and the loss would be zero, indicating no uncertainty. Binary cross-entropy loss penalizes incorrect predictions, encouraging the model to output probabilities closer to the true labels.</p>

<h3 id="cross-entropy-loss-in-categorical-classification">Cross-Entropy Loss in Categorical Classification</h3>

<p>In multi-class classification problems, cross-entropy loss is extended to categorical cross-entropy loss. Here, the loss function compares the predicted probability distribution over multiple classes with the true distribution. For a classification problem with K classes, the categorical cross-entropy loss is:</p>

<p>For example, in a three-class classification problem (A, B, C) with true label B and predicted probabilities [0.1, 0.7, 0.2], the categorical cross-entropy loss is:</p>

\[L = -[0 \cdot \log(0.1) + 1 \cdot \log(0.7) + 0 \cdot \log(0.2)] = -\log(0.7) \approx 0.36\]

<h3 id="cross-entropy-loss-in-language-model"><strong>Cross-Entropy Loss in language model</strong></h3>

<p>Let’s take a look at an example of how Cross-Entropy loss is used in an encoder-decoder translation model. We’ll start by using a trained translation model to generate the first token in the target language.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">Python</span>
<span class="c1"># encode the src setence and predict the first token in tgt language 
</span><span class="n">src_sentence</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Tell me something about yourself</span><span class="sh">'</span>

<span class="n">encoded_src</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode_as_ids</span><span class="p">(</span><span class="n">src_sentence</span><span class="p">)).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span>

<span class="c1">#the output of the encoder
</span><span class="n">memory</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">src_sentence</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">)</span> 

<span class="c1">#the first token in the tgt sentence (bos token)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">bos_id</span><span class="p">()</span>
<span class="n">tgt_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">([</span><span class="n">x</span><span class="p">]).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1">#feed both memory and x into decoder, the output of decoder are logits with shape(1, 1, vocab_size)
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">tgt_tenosr</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">)</span>
</code></pre></div></div>

<p>The shape of output logits is (batch_size, sequence_length, vocab_size):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([1, 1, 10000])
</code></pre></div></div>

<p>Then apply the softmax function to the logits generated by the model. This <strong>converts the logits into a probability distribution over the entire vocabulary.</strong> <strong>Find the index of the token in the vocabulary that has the highest probability.</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span> 
<span class="kn">import</span> <span class="n">torch</span>

<span class="n">sm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">prob</span> <span class="o">=</span> <span class="nf">sm</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">prob</span><span class="p">))</span>
<span class="o">----</span><span class="nf">tensor</span><span class="p">(</span><span class="mi">900</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prob</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">900</span><span class="p">])</span>
<span class="o">-----</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.5717</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode_ids</span><span class="p">(</span><span class="mi">900</span><span class="p">))</span>
<span class="o">----</span><span class="sh">'</span><span class="s">告诉</span><span class="sh">'</span> 
</code></pre></div></div>

<p>The predicted token is ‘告诉‘, whcih aligns with our the first token of the tgt sentence.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tgt_sen</span> <span class="o">=</span> <span class="sh">'</span><span class="s">告诉我关于你的事情</span><span class="sh">'</span>
</code></pre></div></div>

<p>However, since our model only assigns a probability of 0.5717 to the correct token, which indicates a relatively low confidence, we can calculate the loss using the CrossEntropyLoss equation. The loss is given by -log(0.5717). Let’s compute the loss using PyTorch’s CrossEntropyLoss() function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># now we calculate the loss for the 'correct' prediction with lower confidence
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">([</span><span class="n">tgt_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]]).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">([</span><span class="n">tgt_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]]).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="o">---</span> <span class="nf">tensor</span><span class="p">(</span><span class="mf">0.5591</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s explore two different scenarios: one where our model makes an incorrect prediction and another where it makes a correct prediction with high confidence. For the incorrect prediction, the loss is calculated as -log(4.6965e-07) ≈ 16.2224. For the correct prediction with high confidence, the loss is -log(1.0) ≈ 0</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#change the logits on index 900 to make the predicted token not the token 900
</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">900</span><span class="p">]</span> <span class="o">/=</span> <span class="mi">100</span>
<span class="n">sm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">prob</span> <span class="o">=</span> <span class="nf">sm</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> 
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">prob</span><span class="p">))</span> 
<span class="n">____1114</span> <span class="c1">#notice that the predicted token is 1114 instead of token 900 now 
</span>
<span class="c1">#now we calculate the loss for this "wrong" prediction 
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">([</span><span class="n">tgt_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]]).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="o">------</span><span class="mf">16.2224</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">900</span><span class="p">]</span> <span class="o">*=</span> <span class="mi">10</span>
<span class="n">sm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">prob</span> <span class="o">=</span> <span class="nf">sm</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> 
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">prob</span><span class="p">))</span> 
<span class="o">----</span><span class="mi">900</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prob</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">900</span><span class="p">]</span>
<span class="o">----</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">)</span>

<span class="c1">#now we calculate the loss for correct prediction with absolute confidence
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">([</span><span class="n">tgt_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]]).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="o">----</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span>

</code></pre></div></div>

<p>For the entire source code of this translation model: reference my Github repo: <a href="https://github.com/tingaldama278/English-Chinese-Translation-App">https://github.com/tingaldama278/English-Chinese-Translation-App</a></p>

<h3 id="conclusion">Conclusion</h3>

<p>Cross-entropy loss is a powerful tool in machine learning, leveraging the concept of entropy from Shannon’s Information Theory to measure the uncertainty in predictions. By penalizing incorrect predictions and rewarding correct ones, it guides models to improve their accuracy in both binary and categorical classification tasks. Understanding and effectively implementing cross-entropy loss is crucial for developing robust and reliable predictive models</p>

            </div>
          </article>
        </main>

      </section>
      <div id="title" style="text-align: center;">
    
    <hr>
    <span class="credits left">Project maintained by <a href=""></a></span>
    <br>
    <br>
    <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href=""></a></span>

    <p>&copy; 2024 Tingaldama. All rights reserved.</p>
  </div>
    </div>
  </body>
</html>
