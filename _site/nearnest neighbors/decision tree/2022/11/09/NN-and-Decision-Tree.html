<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Nearest Neighbors &amp; Decision Tree | Tingaldama</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Nearest Neighbors &amp; Decision Tree" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Nearest Neighbors In my last post, i talked about doing classification via probablistic model. But a lot of time, we don’t know how to correctly model P[x y]. In this post, we take a look at a different apporach, which is to use disriminative models: Nearest Neighbor (NN) classification The idea is to assign the label to the same label as its ‘closest neighbor’ for new test example. But the question is: How to measure ‘closeness’ in X? use distances: Euclidean distance, Manhattan distance, cityblock distance compare similarity: cosine similarity domain expertise: minimum number of insertions, deletions, mutations needed The Pro of k-NN A straightforward approach Don’t deal with probability modeling The Cons of k-NN Finding the k closest neighbor takes time. This involves calculating the distance between the test point and each point in the training dataset. If there are nnn training points and each point is d-dimensional, a naive approach requires O(nd) time to compute the distance from the test point to all training points. And sorting them takes O(nlogn). The overall complexity is O(nd+nlogn) per test point, which is very slow for large datasets. The ‘closeness’ in raw measurement space is not good. Noisy data can distorts the computation. Features have different scales. Need to keep all the training data around during test. k-NN is a lazy learning algorithm, meaning it does not build a model during training but rather stores the entire training dataset. During testing, it uses the stored data to make predictions. Decision Tree A decision tree classifier works by recursively partitioning the data into subsets that are more homogeneous in terms of the target variable. Recursive Partitioning: A decision tree splits the data into subsets based on the value of input features. At each node in the tree, the algorithm selects the feature and threshold that results in the best split, meaning the split that most reduces uncertainty or impurity. Uncertainty Reduction: The goal is to partition the data such that the resulting subsets (nodes) are as pure as possible, meaning they contain instances predominantly of a single class. The metrics: Gini Impurity: It measures the probability of a randomly chosen element being incorrectly classified if it was labeled according to the distribution of labels in the dataset. [u(C) = 1 - \sum_{y \in \mathcal{Y}} p_y^2] Entropy: It measures the average amount of information needed to identify the class of an element in the dataset. [u(C) = - \sum_{y \in \mathcal{Y}} p_y \log_2(p_y)] Information Gain: [IG(S, A) = H(S) - \sum_{t \in T} p(t) H(t) = H(S) - H(S A)] Classification error: [u(C) = 1 - \max_{y} p_y] Here we need to maximally reduce the uncertainty to find feature F and threshold T [\arg \max_{F, T} \left[ u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \right]] Purning in Decision Tree Pruning is a technique used in decision tree algorithms to reduce overfitting and improve the model’s generalization to unseen data. Overfitting occurs when a decision tree model becomes too complex and captures noise in the training data instead of the underlying pattern. Pruning simplifies the tree by removing parts that do not provide significant power to classify instances. Two tyeps of pruning: Pre-pruning (Early Stopping) Post-pruning (Pruning after Tree Construction) ID3 (Iterative Dichotomiser 3) ID3 (Iterative Dichotomiser 3) is an algorithm used to create decision trees. ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domain. How it works: Initialization: Start with the entire dataset as the root of the tree. Recursive Splitting: For each node, calculate the entropy of the dataset. For each attribute, calculate the information gain if the dataset is split based on that attribute. Select the attribute that provides the highest information gain and split the dataset based on this attribute. Create child nodes for each subset of the dataset resulting from the split. Repeat the process recursively for each child node, treating the subset of the dataset at each node as the new dataset. Stopping Criteria: The recursion stops when one of the following conditions is met: All instances in the dataset at a node belong to the same class. There are no more attributes to split on. The dataset at a node is empty. ID3 follows a greedy approach, making locally optimal choices at each step by selecting the attribute that provides the highest information gain. This approach is computationally efficient and works well for many practical problems. An example to illustate how it works if we split left and right cell of each iteration: Feature 1 Feature 2 Class 2.5 2.4 0 0.5 0.7 1 2.2 2.9 0 1.9 2.2 0 3.1 3.0 1 Initial Gini Impurity Calculation for the Entire Dataset: [u(C) = 1 - \sum_{y \in \mathcal{Y}} p_y^2 = 1 - (p(y=0)^2 + p(y=1)^2) \approx0.48] Splitting the Dataset on Feature 1 Threshold 2.4: left cell: Feature 1 Feature 2 Class 1.9 2.2 0 0.5 0.7 1 2.2 2.9 0 Class distribution: Class 0: 2 instance Class 1: 1 instance [u(CL) = 1 - \sum_{y \in \mathcal{Y}} p_y^2 = 1 - (p(y=0)^2 + p(y=1)^2) \approx 0.44] right cell: Feature 1 Feature 2 Class 2.5 2.4 0 3.1 3.0 1 [u(CR) = 1 - \sum_{y \in \mathcal{Y}} p_y^2 = 1 - (p(y=0)^2 + p(y=1)^2) \approx 0.5] Wighted Gini impurity for the split: [P_L = 3/5 \space P_R = 2/5] [u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \approx 0.014] Threshold 2.0: left cell: Feature 1 Feature 2 Class 1.9 2.2 0 0.5 0.7 1 right cell: Feature 1 Feature 2 Class 2.5 2.4 0 3.1 3.0 1 2.2 2.9 0 [u(CL) =0.5 \space u(CR) \approx 0.44] [u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \approx 0.016] If we split the dataset with Feature 2: Threshold 1.5: left cell: Feature 1 Feature 2 Class 0.5 0.7 1 right cell: Feature 1 Feature 2 Class 1.9 2.2 0 2.5 2.4 0 3.1 3.0 1 2.2 2.9 0 [u(CL) =0 u(CR) = 0.375] [u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) = 0.18] Threshold 2.5: left cell: Feature 1 Feature 2 Class 0.5 0.7 1 1.9 2.2 0 2.5 2.4 0 right cell: Feature 1 Feature 2 Class 3.1 3.0 1 2.2 2.9 0 [u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \approx 0.014] [\arg \max_{F, T} \left[ u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \right] = 0.18] The optimal split is with Feature 2 and threshold 1.5 , which provides the highest reduction in impurity. The observations of Decision Tree The decision tree construction is via a greedy approach Finding the optimal decision tree is NP-hard! You quickly run out of training data as you go down the tree, so uncertainty estimates become very unstable Tree complexity is highly dependent on data geometry in the feature space Popular instantiations that are used in real-world: ID3, C4.5, CART" />
<meta property="og:description" content="Nearest Neighbors In my last post, i talked about doing classification via probablistic model. But a lot of time, we don’t know how to correctly model P[x y]. In this post, we take a look at a different apporach, which is to use disriminative models: Nearest Neighbor (NN) classification The idea is to assign the label to the same label as its ‘closest neighbor’ for new test example. But the question is: How to measure ‘closeness’ in X? use distances: Euclidean distance, Manhattan distance, cityblock distance compare similarity: cosine similarity domain expertise: minimum number of insertions, deletions, mutations needed The Pro of k-NN A straightforward approach Don’t deal with probability modeling The Cons of k-NN Finding the k closest neighbor takes time. This involves calculating the distance between the test point and each point in the training dataset. If there are nnn training points and each point is d-dimensional, a naive approach requires O(nd) time to compute the distance from the test point to all training points. And sorting them takes O(nlogn). The overall complexity is O(nd+nlogn) per test point, which is very slow for large datasets. The ‘closeness’ in raw measurement space is not good. Noisy data can distorts the computation. Features have different scales. Need to keep all the training data around during test. k-NN is a lazy learning algorithm, meaning it does not build a model during training but rather stores the entire training dataset. During testing, it uses the stored data to make predictions. Decision Tree A decision tree classifier works by recursively partitioning the data into subsets that are more homogeneous in terms of the target variable. Recursive Partitioning: A decision tree splits the data into subsets based on the value of input features. At each node in the tree, the algorithm selects the feature and threshold that results in the best split, meaning the split that most reduces uncertainty or impurity. Uncertainty Reduction: The goal is to partition the data such that the resulting subsets (nodes) are as pure as possible, meaning they contain instances predominantly of a single class. The metrics: Gini Impurity: It measures the probability of a randomly chosen element being incorrectly classified if it was labeled according to the distribution of labels in the dataset. [u(C) = 1 - \sum_{y \in \mathcal{Y}} p_y^2] Entropy: It measures the average amount of information needed to identify the class of an element in the dataset. [u(C) = - \sum_{y \in \mathcal{Y}} p_y \log_2(p_y)] Information Gain: [IG(S, A) = H(S) - \sum_{t \in T} p(t) H(t) = H(S) - H(S A)] Classification error: [u(C) = 1 - \max_{y} p_y] Here we need to maximally reduce the uncertainty to find feature F and threshold T [\arg \max_{F, T} \left[ u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \right]] Purning in Decision Tree Pruning is a technique used in decision tree algorithms to reduce overfitting and improve the model’s generalization to unseen data. Overfitting occurs when a decision tree model becomes too complex and captures noise in the training data instead of the underlying pattern. Pruning simplifies the tree by removing parts that do not provide significant power to classify instances. Two tyeps of pruning: Pre-pruning (Early Stopping) Post-pruning (Pruning after Tree Construction) ID3 (Iterative Dichotomiser 3) ID3 (Iterative Dichotomiser 3) is an algorithm used to create decision trees. ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domain. How it works: Initialization: Start with the entire dataset as the root of the tree. Recursive Splitting: For each node, calculate the entropy of the dataset. For each attribute, calculate the information gain if the dataset is split based on that attribute. Select the attribute that provides the highest information gain and split the dataset based on this attribute. Create child nodes for each subset of the dataset resulting from the split. Repeat the process recursively for each child node, treating the subset of the dataset at each node as the new dataset. Stopping Criteria: The recursion stops when one of the following conditions is met: All instances in the dataset at a node belong to the same class. There are no more attributes to split on. The dataset at a node is empty. ID3 follows a greedy approach, making locally optimal choices at each step by selecting the attribute that provides the highest information gain. This approach is computationally efficient and works well for many practical problems. An example to illustate how it works if we split left and right cell of each iteration: Feature 1 Feature 2 Class 2.5 2.4 0 0.5 0.7 1 2.2 2.9 0 1.9 2.2 0 3.1 3.0 1 Initial Gini Impurity Calculation for the Entire Dataset: [u(C) = 1 - \sum_{y \in \mathcal{Y}} p_y^2 = 1 - (p(y=0)^2 + p(y=1)^2) \approx0.48] Splitting the Dataset on Feature 1 Threshold 2.4: left cell: Feature 1 Feature 2 Class 1.9 2.2 0 0.5 0.7 1 2.2 2.9 0 Class distribution: Class 0: 2 instance Class 1: 1 instance [u(CL) = 1 - \sum_{y \in \mathcal{Y}} p_y^2 = 1 - (p(y=0)^2 + p(y=1)^2) \approx 0.44] right cell: Feature 1 Feature 2 Class 2.5 2.4 0 3.1 3.0 1 [u(CR) = 1 - \sum_{y \in \mathcal{Y}} p_y^2 = 1 - (p(y=0)^2 + p(y=1)^2) \approx 0.5] Wighted Gini impurity for the split: [P_L = 3/5 \space P_R = 2/5] [u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \approx 0.014] Threshold 2.0: left cell: Feature 1 Feature 2 Class 1.9 2.2 0 0.5 0.7 1 right cell: Feature 1 Feature 2 Class 2.5 2.4 0 3.1 3.0 1 2.2 2.9 0 [u(CL) =0.5 \space u(CR) \approx 0.44] [u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \approx 0.016] If we split the dataset with Feature 2: Threshold 1.5: left cell: Feature 1 Feature 2 Class 0.5 0.7 1 right cell: Feature 1 Feature 2 Class 1.9 2.2 0 2.5 2.4 0 3.1 3.0 1 2.2 2.9 0 [u(CL) =0 u(CR) = 0.375] [u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) = 0.18] Threshold 2.5: left cell: Feature 1 Feature 2 Class 0.5 0.7 1 1.9 2.2 0 2.5 2.4 0 right cell: Feature 1 Feature 2 Class 3.1 3.0 1 2.2 2.9 0 [u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \approx 0.014] [\arg \max_{F, T} \left[ u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \right] = 0.18] The optimal split is with Feature 2 and threshold 1.5 , which provides the highest reduction in impurity. The observations of Decision Tree The decision tree construction is via a greedy approach Finding the optimal decision tree is NP-hard! You quickly run out of training data as you go down the tree, so uncertainty estimates become very unstable Tree complexity is highly dependent on data geometry in the feature space Popular instantiations that are used in real-world: ID3, C4.5, CART" />
<link rel="canonical" href="http://localhost:4000/tingaldama/nearnest%20neighbors/decision%20tree/2022/11/09/NN-and-Decision-Tree.html" />
<meta property="og:url" content="http://localhost:4000/tingaldama/nearnest%20neighbors/decision%20tree/2022/11/09/NN-and-Decision-Tree.html" />
<meta property="og:site_name" content="Tingaldama" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-11-09T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Nearest Neighbors &amp; Decision Tree" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-11-09T00:00:00+01:00","datePublished":"2022-11-09T00:00:00+01:00","description":"Nearest Neighbors In my last post, i talked about doing classification via probablistic model. But a lot of time, we don’t know how to correctly model P[x y]. In this post, we take a look at a different apporach, which is to use disriminative models: Nearest Neighbor (NN) classification The idea is to assign the label to the same label as its ‘closest neighbor’ for new test example. But the question is: How to measure ‘closeness’ in X? use distances: Euclidean distance, Manhattan distance, cityblock distance compare similarity: cosine similarity domain expertise: minimum number of insertions, deletions, mutations needed The Pro of k-NN A straightforward approach Don’t deal with probability modeling The Cons of k-NN Finding the k closest neighbor takes time. This involves calculating the distance between the test point and each point in the training dataset. If there are nnn training points and each point is d-dimensional, a naive approach requires O(nd) time to compute the distance from the test point to all training points. And sorting them takes O(nlogn). The overall complexity is O(nd+nlogn) per test point, which is very slow for large datasets. The ‘closeness’ in raw measurement space is not good. Noisy data can distorts the computation. Features have different scales. Need to keep all the training data around during test. k-NN is a lazy learning algorithm, meaning it does not build a model during training but rather stores the entire training dataset. During testing, it uses the stored data to make predictions. Decision Tree A decision tree classifier works by recursively partitioning the data into subsets that are more homogeneous in terms of the target variable. Recursive Partitioning: A decision tree splits the data into subsets based on the value of input features. At each node in the tree, the algorithm selects the feature and threshold that results in the best split, meaning the split that most reduces uncertainty or impurity. Uncertainty Reduction: The goal is to partition the data such that the resulting subsets (nodes) are as pure as possible, meaning they contain instances predominantly of a single class. The metrics: Gini Impurity: It measures the probability of a randomly chosen element being incorrectly classified if it was labeled according to the distribution of labels in the dataset. [u(C) = 1 - \\sum_{y \\in \\mathcal{Y}} p_y^2] Entropy: It measures the average amount of information needed to identify the class of an element in the dataset. [u(C) = - \\sum_{y \\in \\mathcal{Y}} p_y \\log_2(p_y)] Information Gain: [IG(S, A) = H(S) - \\sum_{t \\in T} p(t) H(t) = H(S) - H(S A)] Classification error: [u(C) = 1 - \\max_{y} p_y] Here we need to maximally reduce the uncertainty to find feature F and threshold T [\\arg \\max_{F, T} \\left[ u(C) - \\left( p_L \\cdot u(C_L) + p_R \\cdot u(C_R) \\right) \\right]] Purning in Decision Tree Pruning is a technique used in decision tree algorithms to reduce overfitting and improve the model’s generalization to unseen data. Overfitting occurs when a decision tree model becomes too complex and captures noise in the training data instead of the underlying pattern. Pruning simplifies the tree by removing parts that do not provide significant power to classify instances. Two tyeps of pruning: Pre-pruning (Early Stopping) Post-pruning (Pruning after Tree Construction) ID3 (Iterative Dichotomiser 3) ID3 (Iterative Dichotomiser 3) is an algorithm used to create decision trees. ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domain. How it works: Initialization: Start with the entire dataset as the root of the tree. Recursive Splitting: For each node, calculate the entropy of the dataset. For each attribute, calculate the information gain if the dataset is split based on that attribute. Select the attribute that provides the highest information gain and split the dataset based on this attribute. Create child nodes for each subset of the dataset resulting from the split. Repeat the process recursively for each child node, treating the subset of the dataset at each node as the new dataset. Stopping Criteria: The recursion stops when one of the following conditions is met: All instances in the dataset at a node belong to the same class. There are no more attributes to split on. The dataset at a node is empty. ID3 follows a greedy approach, making locally optimal choices at each step by selecting the attribute that provides the highest information gain. This approach is computationally efficient and works well for many practical problems. An example to illustate how it works if we split left and right cell of each iteration: Feature 1 Feature 2 Class 2.5 2.4 0 0.5 0.7 1 2.2 2.9 0 1.9 2.2 0 3.1 3.0 1 Initial Gini Impurity Calculation for the Entire Dataset: [u(C) = 1 - \\sum_{y \\in \\mathcal{Y}} p_y^2 = 1 - (p(y=0)^2 + p(y=1)^2) \\approx0.48] Splitting the Dataset on Feature 1 Threshold 2.4: left cell: Feature 1 Feature 2 Class 1.9 2.2 0 0.5 0.7 1 2.2 2.9 0 Class distribution: Class 0: 2 instance Class 1: 1 instance [u(CL) = 1 - \\sum_{y \\in \\mathcal{Y}} p_y^2 = 1 - (p(y=0)^2 + p(y=1)^2) \\approx 0.44] right cell: Feature 1 Feature 2 Class 2.5 2.4 0 3.1 3.0 1 [u(CR) = 1 - \\sum_{y \\in \\mathcal{Y}} p_y^2 = 1 - (p(y=0)^2 + p(y=1)^2) \\approx 0.5] Wighted Gini impurity for the split: [P_L = 3/5 \\space P_R = 2/5] [u(C) - \\left( p_L \\cdot u(C_L) + p_R \\cdot u(C_R) \\right) \\approx 0.014] Threshold 2.0: left cell: Feature 1 Feature 2 Class 1.9 2.2 0 0.5 0.7 1 right cell: Feature 1 Feature 2 Class 2.5 2.4 0 3.1 3.0 1 2.2 2.9 0 [u(CL) =0.5 \\space u(CR) \\approx 0.44] [u(C) - \\left( p_L \\cdot u(C_L) + p_R \\cdot u(C_R) \\right) \\approx 0.016] If we split the dataset with Feature 2: Threshold 1.5: left cell: Feature 1 Feature 2 Class 0.5 0.7 1 right cell: Feature 1 Feature 2 Class 1.9 2.2 0 2.5 2.4 0 3.1 3.0 1 2.2 2.9 0 [u(CL) =0 u(CR) = 0.375] [u(C) - \\left( p_L \\cdot u(C_L) + p_R \\cdot u(C_R) \\right) = 0.18] Threshold 2.5: left cell: Feature 1 Feature 2 Class 0.5 0.7 1 1.9 2.2 0 2.5 2.4 0 right cell: Feature 1 Feature 2 Class 3.1 3.0 1 2.2 2.9 0 [u(C) - \\left( p_L \\cdot u(C_L) + p_R \\cdot u(C_R) \\right) \\approx 0.014] [\\arg \\max_{F, T} \\left[ u(C) - \\left( p_L \\cdot u(C_L) + p_R \\cdot u(C_R) \\right) \\right] = 0.18] The optimal split is with Feature 2 and threshold 1.5 , which provides the highest reduction in impurity. The observations of Decision Tree The decision tree construction is via a greedy approach Finding the optimal decision tree is NP-hard! You quickly run out of training data as you go down the tree, so uncertainty estimates become very unstable Tree complexity is highly dependent on data geometry in the feature space Popular instantiations that are used in real-world: ID3, C4.5, CART","headline":"Nearest Neighbors &amp; Decision Tree","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/tingaldama/nearnest%20neighbors/decision%20tree/2022/11/09/NN-and-Decision-Tree.html"},"url":"http://localhost:4000/tingaldama/nearnest%20neighbors/decision%20tree/2022/11/09/NN-and-Decision-Tree.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/tingaldama/assets/css/style.css?v=">
    <script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
    <script src="/tingaldama/assets/js/respond.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="/tingaldama/assets/css/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup theme-color -->
<!-- start theme color meta headers -->
<meta name="theme-color" content="#353535">
<meta name="msapplication-navbutton-color" content="#353535">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- end theme color meta headers -->


<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/tingaldama/favicon.ico" -->

<!-- end custom head snippets -->
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<link rel="shortcut icon" href="/tingaldama//assets/images/favicon.jpg" type="image/x-icon">


  </head>
  <style>
    .categories{
  font-weight: bold;
  color: #949494;
}
 .date{
  font-weight: bold !important;
  color: #949494 !important;
}

  </style>
  <body>
    
<div id="header">
    <nav>
      <ul>
        <li class="fork"><a href="/tingaldama/">Home</a></li>
        <li class="downloads"><a href="/tingaldama/contact">Contacts</a></li>
        <li class="downloads"><a href="/tingaldama/about">About</a></li>
        <li class="downloads"><a href="/tingaldama/blog">Blogs</a></li>
        <li class="downloads"><a href="/tingaldama/mlps-resources">MLOps Resources</a></li>
        <li class="downloads"><a href="/tingaldama/personal-growth">Personal Growth</a></li>
          
      </ul>
    </nav>
  </div><!-- end header -->

    <div class="wrapper">

      <section>
        

        <main>
          <article>
            <h1>Nearest Neighbors & Decision Tree</h1>
            <div class="post-meta">
             <span class="date">Date: November 09, 2022</span>
      <br>
      <span class="categories">
        Categories: Nearnest Neighbors | Decision Tree
      </span>
            </div>
            <br>
            <div class="post-content">
              
              <h2 id="nearest-neighbors">Nearest Neighbors</h2>

<table>
  <tbody>
    <tr>
      <td>In my last post, i talked about doing classification via probablistic model. But a lot of time,  we don’t know how to correctly model P[x</td>
      <td>y].</td>
    </tr>
  </tbody>
</table>

<p>In this post, we take a look at a different apporach, which is to use disriminative models:</p>

<h3 id="nearest-neighbor-nn-classification">Nearest Neighbor (NN) classification</h3>

<p>The idea is to assign the label to the same label as its ‘closest neighbor’ for new test example. But the question is: How to measure ‘closeness’ in X?</p>

<ul>
  <li>use distances: Euclidean distance, Manhattan distance, cityblock distance</li>
  <li>compare similarity: cosine similarity</li>
  <li>domain expertise: minimum number of insertions, deletions, mutations needed</li>
</ul>

<h3 id="the-pro-of-k-nn">The Pro of k-NN</h3>

<ol>
  <li>A straightforward approach</li>
  <li>Don’t deal with probability modeling</li>
</ol>

<h3 id="the-cons-of--k-nn">The Cons of  k-NN</h3>

<ol>
  <li><strong>Finding the k closest neighbor takes time.</strong> This involves calculating the distance between the test point and each point in the training dataset. If there are nn<strong>n</strong> training points and each point is d-dimensional, a naive approach requires O(nd) time to compute the distance from the test point to all training points. And sorting them takes O(nlogn). The overall complexity is <strong>O</strong>(<strong>n</strong>d+<strong>n</strong>lo<strong>g</strong>n) per test point, which is very slow for large datasets.</li>
  <li>The ‘closeness’ in raw measurement space is not good. Noisy data can distorts the computation. Features have different scales.</li>
  <li>Need to keep all the training data around during test. k-NN is a lazy learning algorithm, meaning it does not build a model during training but rather stores the entire training dataset. During testing, it uses the stored data to make predictions.</li>
</ol>

<h2 id="decision-tree">Decision Tree</h2>

<p>A decision tree classifier works by recursively partitioning the data into subsets that are more <strong>homogeneous</strong> in terms of the target variable.</p>

<p><strong>Recursive Partitioning:</strong></p>

<ul>
  <li>A decision tree splits the data into subsets based on the value of input features.</li>
  <li>At each node in the tree, the algorithm selects the feature and threshold that results in the best split, meaning the split that most reduces uncertainty or impurity.</li>
</ul>

<p><strong>Uncertainty Reduction:</strong></p>

<ul>
  <li>The goal is to partition the data such that the resulting subsets (nodes) are as pure as possible, meaning they contain instances predominantly of a single class.</li>
</ul>

<p><strong>The metrics:</strong></p>

<p>Gini Impurity: It measures the probability of a randomly chosen element being incorrectly classified if it was labeled according to the distribution of labels in the dataset.</p>

\[u(C) = 1 - \sum_{y \in \mathcal{Y}} p_y^2\]

<p>Entropy: It measures the average amount of information needed to identify the class of an element in the dataset.</p>

\[u(C) = - \sum_{y \in \mathcal{Y}} p_y \log_2(p_y)\]

<p>Information Gain:</p>

\[IG(S, A) = H(S) - \sum_{t \in T} p(t) H(t) = H(S) - H(S|A)\]

<p>Classification error:</p>

\[u(C) = 1 - \max_{y} p_y\]

<p>Here we need to maximally reduce the uncertainty to find feature F and threshold T</p>

\[\arg \max_{F, T} \left[ u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \right]\]

<h3 id="purning-in-decision-tree">Purning in Decision Tree</h3>

<p>Pruning is a technique used in decision tree algorithms to reduce overfitting and improve the model’s generalization to unseen data. Overfitting occurs when a decision tree model becomes too complex and captures noise in the training data instead of the underlying pattern. Pruning simplifies the tree by removing parts that do not provide significant power to classify instances.</p>

<p>Two tyeps of pruning:</p>

<ul>
  <li><strong>Pre-pruning (Early Stopping)</strong></li>
  <li><strong>Post-pruning (Pruning after Tree Construction)</strong></li>
</ul>

<h3 id="id3-iterative-dichotomiser-3"><strong>ID3 (Iterative Dichotomiser 3)</strong></h3>

<p>ID3 (Iterative Dichotomiser 3) is an algorithm used to create decision trees. ID3 is the precursor to the <a href="https://en.wikipedia.org/wiki/C4.5_algorithm" title="C4.5 algorithm">C4.5 algorithm</a>, and is typically used in the <a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine learning">machine learning</a> and <a href="https://en.wikipedia.org/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a> domain.</p>

<h4 id="how-it-works">How it works:</h4>

<p><strong>Initialization</strong>:</p>

<ul>
  <li>Start with the entire dataset as the root of the tree.</li>
</ul>

<p><strong>Recursive Splitting</strong>:</p>

<ul>
  <li>For each node, calculate the entropy of the dataset.</li>
  <li>For each attribute, calculate the information gain if the dataset is split based on that attribute.</li>
  <li>Select the attribute that provides the highest information gain and split the dataset based on this attribute.</li>
  <li>Create child nodes for each subset of the dataset resulting from the split.</li>
  <li>Repeat the process recursively for each child node, treating the subset of the dataset at each node as the new dataset.</li>
</ul>

<p><strong>Stopping Criteria</strong>:</p>

<ul>
  <li>The recursion stops when one of the following conditions is met:
    <ul>
      <li>All instances in the dataset at a node belong to the same class.</li>
      <li>There are no more attributes to split on.</li>
      <li>The dataset at a node is empty.</li>
    </ul>
  </li>
</ul>

<p>ID3 follows a greedy approach, making locally optimal choices at each step by selecting the attribute that provides the highest information gain. This approach is computationally efficient and works well for many practical problems.</p>

<hr />

<p>An example to illustate how it works if we split left and right cell of each iteration:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Feature 1</th>
      <th>Feature 2</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">2.5</td>
      <td>2.4</td>
      <td>0</td>
    </tr>
    <tr>
      <td style="text-align: center">0.5</td>
      <td>0.7</td>
      <td>1</td>
    </tr>
    <tr>
      <td style="text-align: center">2.2</td>
      <td>2.9</td>
      <td>0</td>
    </tr>
    <tr>
      <td style="text-align: center">1.9</td>
      <td>2.2</td>
      <td>0</td>
    </tr>
    <tr>
      <td style="text-align: center">3.1</td>
      <td>3.0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>Initial Gini Impurity Calculation for the Entire Dataset:</p>

\[u(C) = 1 - \sum_{y \in \mathcal{Y}} p_y^2 = 1 - (p(y=0)^2 + p(y=1)^2) \approx0.48\]

<p>Splitting the Dataset on Feature 1</p>

<p>Threshold 2.4:</p>

<p>left cell:</p>

<table>
  <thead>
    <tr>
      <th>Feature 1</th>
      <th>Feature 2</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1.9</td>
      <td>2.2</td>
      <td>0</td>
    </tr>
    <tr>
      <td>0.5</td>
      <td>0.7</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2.2</td>
      <td>2.9</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>Class distribution:</p>

<ul>
  <li>Class 0: 2 instance</li>
  <li>Class 1: 1 instance</li>
</ul>

\[u(CL) = 1 - \sum_{y \in \mathcal{Y}} p_y^2 = 1 - (p(y=0)^2 + p(y=1)^2) \approx 0.44\]

<p>right cell:</p>

<table>
  <thead>
    <tr>
      <th>Feature 1</th>
      <th>Feature 2</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2.5</td>
      <td>2.4</td>
      <td>0</td>
    </tr>
    <tr>
      <td>3.1</td>
      <td>3.0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

\[u(CR) = 1 - \sum_{y \in \mathcal{Y}} p_y^2 = 1 - (p(y=0)^2 + p(y=1)^2) \approx 0.5\]

<p>Wighted Gini impurity for the split:</p>

\[P_L = 3/5 \space P_R = 2/5\]

\[u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \approx 0.014\]

<p>Threshold 2.0:</p>

<p>left cell:</p>

<table>
  <thead>
    <tr>
      <th>Feature 1</th>
      <th>Feature 2</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1.9</td>
      <td>2.2</td>
      <td>0</td>
    </tr>
    <tr>
      <td>0.5</td>
      <td>0.7</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>right cell:</p>

<table>
  <thead>
    <tr>
      <th>Feature 1</th>
      <th>Feature 2</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2.5</td>
      <td>2.4</td>
      <td>0</td>
    </tr>
    <tr>
      <td>3.1</td>
      <td>3.0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2.2</td>
      <td>2.9</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

\[u(CL)  =0.5 \space  u(CR)  \approx 0.44\]

\[u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \approx 0.016\]

<p><strong>If we split the dataset with Feature 2:</strong></p>

<p>Threshold 1.5:</p>

<p>left cell:</p>

<table>
  <thead>
    <tr>
      <th>Feature 1</th>
      <th>Feature 2</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.5</td>
      <td>0.7</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>right cell:</p>

<table>
  <thead>
    <tr>
      <th>Feature 1</th>
      <th>Feature 2</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1.9</td>
      <td>2.2</td>
      <td>0</td>
    </tr>
    <tr>
      <td>2.5</td>
      <td>2.4</td>
      <td>0</td>
    </tr>
    <tr>
      <td>3.1</td>
      <td>3.0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2.2</td>
      <td>2.9</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

\[u(CL) =0  u(CR) = 0.375\]

\[u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) = 0.18\]

<p>Threshold 2.5:</p>

<p>left cell:</p>

<table>
  <thead>
    <tr>
      <th>Feature 1</th>
      <th>Feature 2</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.5</td>
      <td>0.7</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1.9</td>
      <td>2.2</td>
      <td>0</td>
    </tr>
    <tr>
      <td>2.5</td>
      <td>2.4</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>right cell:</p>

<table>
  <thead>
    <tr>
      <th>Feature 1</th>
      <th>Feature 2</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>3.1</td>
      <td>3.0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2.2</td>
      <td>2.9</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

\[u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \approx 0.014\]

\[\arg \max_{F, T} \left[ u(C) - \left( p_L \cdot u(C_L) + p_R \cdot u(C_R) \right) \right] = 0.18\]

<p>The optimal split is with  <strong>Feature 2 and threshold 1.5</strong> , which provides the highest reduction in impurity.</p>

<hr />

<h3 id="the-observations-of-decision-tree">The observations of Decision Tree</h3>

<ul>
  <li>The decision tree construction is via a greedy approach</li>
  <li>Finding the optimal decision tree is NP-hard!</li>
  <li>You quickly run out of training data as you go down the tree, so uncertainty estimates become very unstable</li>
  <li>Tree complexity is highly dependent on data geometry in the feature space</li>
  <li>Popular instantiations that are used in real-world: ID3, C4.5, CART</li>
</ul>

            </div>
          </article>
        </main>

      </section>
      <div id="title" style="text-align: center;">
    
    <hr>
    <span class="credits left">Project maintained by <a href=""></a></span>
    <br>
    <br>
    <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href=""></a></span>

    <p>&copy; 2024 Tingaldama. All rights reserved.</p>
  </div>
    </div>
  </body>
</html>
