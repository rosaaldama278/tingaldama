<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>A Gentle Introduction to Regression | Tingaldama</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="A Gentle Introduction to Regression" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Parametric vs Nonparametric Regressioin Parametric regression If we assume a particular form of the regressor: Goal: to learn the parameters which yield the minimum error/loss Non-parametric regression If no specific form of regressor is assumed: Goal: to learn the predictor directly from the input data that yields the minimum error/loss Linear Regression Want to find a linear predictor ( f ), i.e., ( w ) (intercept ( w_0 ) absorbed via lifting): [hat{f}(\vec{x}) := \vec{w} \cdot \vec{x}] which minimizes the prediction loss over the population. [min_{\vec{w}} \mathbb{E}_{\vec{x}, y} \left[ L(\hat{f}(\vec{x}), y) \right]] We estimate the parameters by minimizing the corresponding loss on the training data: [\arg \min_{\vec{w}} \frac{1}{n} \sum_{i=1}^n \left[ L(\vec{w} \cdot \vec{x}i, y_i) \right] = \arg \min{\vec{w}} \frac{1}{n} \sum_{i=1}^n \left( \vec{w} \cdot \vec{x}_i - y_i \right)^2] Learning the Parameters Unconstraint problem: [= \arg \min_{\vec{w}} \left| \begin{pmatrix} … &amp; \vec{x}_1 &amp; … … &amp; \vec{x}_i &amp; … … &amp; \vec{x}_n &amp; … \end{pmatrix} \vec{w} - \begin{pmatrix} y_1 y_i y_n \end{pmatrix} \right|_2^2] [= \arg \min_{\vec{w}} | X \vec{w} - \vec{y} |_2^2] Thus best fitting w: [\frac{\partial}{\partial \mathbf{w}} |X\mathbf{w} - \mathbf{y}|^2 = 2X^T (X\mathbf{w} - \mathbf{y})] At stationarity, this results in the equation: [(X^T X)\mathbf{w} = X^T \mathbf{y}] This system is always consistent: [\mathbf{\vec{y}} = \mathbf{\vec{y}}{\text{col}(X)} + \mathbf{\vec{y}}{\text{null}(X^T)}] Thus, [X^T \mathbf{y} = X^T \mathbf{\vec{y}}_{\text{col}(X)}] Since, [{\vec{y}}_{\text{col}(X)} = \sum_i w_i \mathbf{x}_i \quad \text{(for some coefficients (w_i), where (\mathbf{x}_i) are columns of (X))}] Define, [{\vec{w}} := \begin{bmatrix} w_1 \vdots w_d \end{bmatrix}] Then, [(X^T X)\mathbf{\vec{w}} = X^T (X\mathbf{\vec{w}}) = X^T \left(\sum_i w_i \mathbf{x}i \right) = X^T \mathbf{y}{\text{col}(X)} = X^T \mathbf{y}] [\mathbf{\vec{w}}_{\text{ols}} = (X^T X)^{\dagger} X^T \mathbf{y}] Also called the Ordinary Least Squares (OLS). The solution is unique and stable when $(X^T X)$ is invertible Linear Regression in Statisticall Modeling View Let’s assume that data is generated from the following process: A example $x_i$ is draw independently from the data space $X$ \(x_i \sim D_X\) $y_i^{clean}$ is computed as $(w \cdot x_i)$, from a fixed unknown $w$ \(y_i^{clean} := w \cdot x_i\) $y_i$ is corrupted from $y_i^{clean}$ by adding independent Gaussian noise $N(0,\sigma^2)$ \(y_i := y_i^{clean} + \epsilon_i = w \cdot x_i + \epsilon_i \quad\quad \epsilon_i \sim N(0, \sigma^2)\) $(x_i, y_i)$ is revealed as the $i$-th sample \((x_1, y_1), ..., (x_n, y_n) =: S\) How can we determine $w$, from Gaussian noise corrupted observations? [S = (x_1, y_1), …, (x_n, y_n)] Observation: [y_i \sim w \cdot x_i + N(0, \sigma^2) = N(w \cdot x_i, \sigma^2)] parameter: w (Ignored terms indepdent of w) [\log L(w S) = \sum_{i=1}^n \log p(y_i w)] [\propto \sum_{i=1}^n \frac{-(w \cdot x_i - y_i)^2}{2\sigma^2}] optimizing for $w$ yields the same OLS result! Lasso Regression &amp; Ridge Regression Previously we looked at Ordinary Least Square (OLS) [minimize| X \vec{w} - \vec{y} |_2^2] [\mathbf{\vec{w}}_{\text{ols}} = (X^T X)^{\dagger} X^T \mathbf{y}] Which is poorly behaved (due to overfitting) when we have limited data. We can incorporate application dependent piror knowledge. Lasso regression: Objective minimize $ X\vec{w} - \vec{y} ^2 + \lambda|\vec{w}|^2$ reconstruction error ‘regularization’ parameter $\vec{w}_{ridge} = (X^TX + \lambda I)^{-1}X^T\vec{y}$ The ‘regularization’ helps avoid overfitting, and always resulting in a unique solution. Equivalent to the following optimization problem: minimize $ X\vec{w} - \vec{y} ^2$ such that $ \vec{w} ^2 \leq B$ Ridge Regression: Objective minimize $ X\vec{w} - \vec{y} ^2 + \lambda|\vec{w}|_1$ ‘lasso’ penalty $\vec{w}_{lasso} = ?$ no closed form solution Lasso regularization encourages sparse solutions. Equivalent to the following optimization problem: minimize $ X\vec{w} - \vec{y} ^2$ such that $ \vec{w} _1 \leq B$ Logistic Regression Linear regression for classification: Although its name contains “regression,” it is actually used for classification tasks. For a binary classification problem, given input x, how likely is it that it has label 1?Let this be denoted by P, ie, P is the chance that a given x the associated label y = 1, P = P(Y=1 X=x) ranges between 0 and 1, hence cannot be modelled appropriately via linear regression. If we look at the ‘odds’ of getting y=1 (for a given x) $odds(P) := \frac{P}{1-P}$ For an event with P=0.9, odds = 9 But, for an event P=0.1, odds = 0.11 Consider the “log” of the odds (very asymmetric) $log(odds(P)) := logit(P) := log(\frac{P}{1-P})$ $logit(P) = -logit(1-P)$ Symmetric! Can model logit as a linear function!! Model the log-odds or logit with linear function! Given an input x $logit(P(Y=1 X=x)) = logit(P) = log(\frac{P}{1-P}) \stackrel{\text{modeling assumption}}{=} w \cdot x$ $\frac{P}{1-P} = e^{w \cdot x}$ $P(Y=1 X=x) = P = \frac{e^{w \cdot x}}{1+e^{w \cdot x}} = \frac{1}{1+e^{-w \cdot x}}$ Sigmoid! How do we learn the prameters? Given samples $S = (x_1, y_1), …, (x_n, y_n)$ yi is binary) $\mathcal{L}(w S) = \prod_{i=1}^{n} P((x_i, y_i) w) \propto \prod_{i=1}^{n} P(y_i = 1 x_i, w)= \prod_{i=1}^{n} P(y_i = 1 x_i, w)^{y_i} (1 - P(y_i = 1 x_i, w))^{1-y_i}$ (Binomial MLE) $log \mathcal{L}(w S) \propto \sum_{i=1}^{n} y_i log P_{x_i} + (1 - y_i) log(1 - P_{x_i}) = \sum_{i=1}^{n} y_i log \frac{P_{x_i}}{1 - P_{x_i}} + \sum_{i=1}^{n} log(1 - P_{x_i})$ Now, use logistic model! $= \sum_{i=1}^{n} y_i (w \cdot x_i) + \sum_{i=1}^{n} - log(1 + e^{w \cdot x_i})$ No closed form solution Can use iterative methods like gradient ‘ascent’ to find the solution Non-parametric Regression What if we don’t know parametric form of the relationship between the independent and dependent variables? How can we predict value of a new test point x without model assumptions? Kernel Regression $y = f_n(x) := \sum_{i=1}^{n} w_i(x)y_i$ Want weights that emphasize local observations Localization functions: Gaussian Kernel: $K_h(x, x’) = e^{-   x-x’   ^2/h}$ Box Kernel: $1[   x - x’   \leq h]$ \(K_h(x, x&#39;) = e^{-||x-x&#39;||^2/h}\) Gaussian kernel \(= 1[||x - x&#39;|| \leq h]\) Box kernel \(= [1 - (1/h)||x - x&#39;||]_+\) Triangle kernel Then define: w_i(x) := \frac{K_h(x,x_i)}{\sum_{j=1}^n K_h(x,x_j)} $$" />
<meta property="og:description" content="Parametric vs Nonparametric Regressioin Parametric regression If we assume a particular form of the regressor: Goal: to learn the parameters which yield the minimum error/loss Non-parametric regression If no specific form of regressor is assumed: Goal: to learn the predictor directly from the input data that yields the minimum error/loss Linear Regression Want to find a linear predictor ( f ), i.e., ( w ) (intercept ( w_0 ) absorbed via lifting): [hat{f}(\vec{x}) := \vec{w} \cdot \vec{x}] which minimizes the prediction loss over the population. [min_{\vec{w}} \mathbb{E}_{\vec{x}, y} \left[ L(\hat{f}(\vec{x}), y) \right]] We estimate the parameters by minimizing the corresponding loss on the training data: [\arg \min_{\vec{w}} \frac{1}{n} \sum_{i=1}^n \left[ L(\vec{w} \cdot \vec{x}i, y_i) \right] = \arg \min{\vec{w}} \frac{1}{n} \sum_{i=1}^n \left( \vec{w} \cdot \vec{x}_i - y_i \right)^2] Learning the Parameters Unconstraint problem: [= \arg \min_{\vec{w}} \left| \begin{pmatrix} … &amp; \vec{x}_1 &amp; … … &amp; \vec{x}_i &amp; … … &amp; \vec{x}_n &amp; … \end{pmatrix} \vec{w} - \begin{pmatrix} y_1 y_i y_n \end{pmatrix} \right|_2^2] [= \arg \min_{\vec{w}} | X \vec{w} - \vec{y} |_2^2] Thus best fitting w: [\frac{\partial}{\partial \mathbf{w}} |X\mathbf{w} - \mathbf{y}|^2 = 2X^T (X\mathbf{w} - \mathbf{y})] At stationarity, this results in the equation: [(X^T X)\mathbf{w} = X^T \mathbf{y}] This system is always consistent: [\mathbf{\vec{y}} = \mathbf{\vec{y}}{\text{col}(X)} + \mathbf{\vec{y}}{\text{null}(X^T)}] Thus, [X^T \mathbf{y} = X^T \mathbf{\vec{y}}_{\text{col}(X)}] Since, [{\vec{y}}_{\text{col}(X)} = \sum_i w_i \mathbf{x}_i \quad \text{(for some coefficients (w_i), where (\mathbf{x}_i) are columns of (X))}] Define, [{\vec{w}} := \begin{bmatrix} w_1 \vdots w_d \end{bmatrix}] Then, [(X^T X)\mathbf{\vec{w}} = X^T (X\mathbf{\vec{w}}) = X^T \left(\sum_i w_i \mathbf{x}i \right) = X^T \mathbf{y}{\text{col}(X)} = X^T \mathbf{y}] [\mathbf{\vec{w}}_{\text{ols}} = (X^T X)^{\dagger} X^T \mathbf{y}] Also called the Ordinary Least Squares (OLS). The solution is unique and stable when $(X^T X)$ is invertible Linear Regression in Statisticall Modeling View Let’s assume that data is generated from the following process: A example $x_i$ is draw independently from the data space $X$ \(x_i \sim D_X\) $y_i^{clean}$ is computed as $(w \cdot x_i)$, from a fixed unknown $w$ \(y_i^{clean} := w \cdot x_i\) $y_i$ is corrupted from $y_i^{clean}$ by adding independent Gaussian noise $N(0,\sigma^2)$ \(y_i := y_i^{clean} + \epsilon_i = w \cdot x_i + \epsilon_i \quad\quad \epsilon_i \sim N(0, \sigma^2)\) $(x_i, y_i)$ is revealed as the $i$-th sample \((x_1, y_1), ..., (x_n, y_n) =: S\) How can we determine $w$, from Gaussian noise corrupted observations? [S = (x_1, y_1), …, (x_n, y_n)] Observation: [y_i \sim w \cdot x_i + N(0, \sigma^2) = N(w \cdot x_i, \sigma^2)] parameter: w (Ignored terms indepdent of w) [\log L(w S) = \sum_{i=1}^n \log p(y_i w)] [\propto \sum_{i=1}^n \frac{-(w \cdot x_i - y_i)^2}{2\sigma^2}] optimizing for $w$ yields the same OLS result! Lasso Regression &amp; Ridge Regression Previously we looked at Ordinary Least Square (OLS) [minimize| X \vec{w} - \vec{y} |_2^2] [\mathbf{\vec{w}}_{\text{ols}} = (X^T X)^{\dagger} X^T \mathbf{y}] Which is poorly behaved (due to overfitting) when we have limited data. We can incorporate application dependent piror knowledge. Lasso regression: Objective minimize $ X\vec{w} - \vec{y} ^2 + \lambda|\vec{w}|^2$ reconstruction error ‘regularization’ parameter $\vec{w}_{ridge} = (X^TX + \lambda I)^{-1}X^T\vec{y}$ The ‘regularization’ helps avoid overfitting, and always resulting in a unique solution. Equivalent to the following optimization problem: minimize $ X\vec{w} - \vec{y} ^2$ such that $ \vec{w} ^2 \leq B$ Ridge Regression: Objective minimize $ X\vec{w} - \vec{y} ^2 + \lambda|\vec{w}|_1$ ‘lasso’ penalty $\vec{w}_{lasso} = ?$ no closed form solution Lasso regularization encourages sparse solutions. Equivalent to the following optimization problem: minimize $ X\vec{w} - \vec{y} ^2$ such that $ \vec{w} _1 \leq B$ Logistic Regression Linear regression for classification: Although its name contains “regression,” it is actually used for classification tasks. For a binary classification problem, given input x, how likely is it that it has label 1?Let this be denoted by P, ie, P is the chance that a given x the associated label y = 1, P = P(Y=1 X=x) ranges between 0 and 1, hence cannot be modelled appropriately via linear regression. If we look at the ‘odds’ of getting y=1 (for a given x) $odds(P) := \frac{P}{1-P}$ For an event with P=0.9, odds = 9 But, for an event P=0.1, odds = 0.11 Consider the “log” of the odds (very asymmetric) $log(odds(P)) := logit(P) := log(\frac{P}{1-P})$ $logit(P) = -logit(1-P)$ Symmetric! Can model logit as a linear function!! Model the log-odds or logit with linear function! Given an input x $logit(P(Y=1 X=x)) = logit(P) = log(\frac{P}{1-P}) \stackrel{\text{modeling assumption}}{=} w \cdot x$ $\frac{P}{1-P} = e^{w \cdot x}$ $P(Y=1 X=x) = P = \frac{e^{w \cdot x}}{1+e^{w \cdot x}} = \frac{1}{1+e^{-w \cdot x}}$ Sigmoid! How do we learn the prameters? Given samples $S = (x_1, y_1), …, (x_n, y_n)$ yi is binary) $\mathcal{L}(w S) = \prod_{i=1}^{n} P((x_i, y_i) w) \propto \prod_{i=1}^{n} P(y_i = 1 x_i, w)= \prod_{i=1}^{n} P(y_i = 1 x_i, w)^{y_i} (1 - P(y_i = 1 x_i, w))^{1-y_i}$ (Binomial MLE) $log \mathcal{L}(w S) \propto \sum_{i=1}^{n} y_i log P_{x_i} + (1 - y_i) log(1 - P_{x_i}) = \sum_{i=1}^{n} y_i log \frac{P_{x_i}}{1 - P_{x_i}} + \sum_{i=1}^{n} log(1 - P_{x_i})$ Now, use logistic model! $= \sum_{i=1}^{n} y_i (w \cdot x_i) + \sum_{i=1}^{n} - log(1 + e^{w \cdot x_i})$ No closed form solution Can use iterative methods like gradient ‘ascent’ to find the solution Non-parametric Regression What if we don’t know parametric form of the relationship between the independent and dependent variables? How can we predict value of a new test point x without model assumptions? Kernel Regression $y = f_n(x) := \sum_{i=1}^{n} w_i(x)y_i$ Want weights that emphasize local observations Localization functions: Gaussian Kernel: $K_h(x, x’) = e^{-   x-x’   ^2/h}$ Box Kernel: $1[   x - x’   \leq h]$ \(K_h(x, x&#39;) = e^{-||x-x&#39;||^2/h}\) Gaussian kernel \(= 1[||x - x&#39;|| \leq h]\) Box kernel \(= [1 - (1/h)||x - x&#39;||]_+\) Triangle kernel Then define: w_i(x) := \frac{K_h(x,x_i)}{\sum_{j=1}^n K_h(x,x_j)} $$" />
<link rel="canonical" href="http://localhost:4000/tingaldama1/regression%20techniques/parametric%20&%20non-parametric%20methods/statistical%20modeling/2022/12/11/Regression.html" />
<meta property="og:url" content="http://localhost:4000/tingaldama1/regression%20techniques/parametric%20&%20non-parametric%20methods/statistical%20modeling/2022/12/11/Regression.html" />
<meta property="og:site_name" content="Tingaldama" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-12-11T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="A Gentle Introduction to Regression" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-12-11T00:00:00+01:00","datePublished":"2022-12-11T00:00:00+01:00","description":"Parametric vs Nonparametric Regressioin Parametric regression If we assume a particular form of the regressor: Goal: to learn the parameters which yield the minimum error/loss Non-parametric regression If no specific form of regressor is assumed: Goal: to learn the predictor directly from the input data that yields the minimum error/loss Linear Regression Want to find a linear predictor ( f ), i.e., ( w ) (intercept ( w_0 ) absorbed via lifting): [hat{f}(\\vec{x}) := \\vec{w} \\cdot \\vec{x}] which minimizes the prediction loss over the population. [min_{\\vec{w}} \\mathbb{E}_{\\vec{x}, y} \\left[ L(\\hat{f}(\\vec{x}), y) \\right]] We estimate the parameters by minimizing the corresponding loss on the training data: [\\arg \\min_{\\vec{w}} \\frac{1}{n} \\sum_{i=1}^n \\left[ L(\\vec{w} \\cdot \\vec{x}i, y_i) \\right] = \\arg \\min{\\vec{w}} \\frac{1}{n} \\sum_{i=1}^n \\left( \\vec{w} \\cdot \\vec{x}_i - y_i \\right)^2] Learning the Parameters Unconstraint problem: [= \\arg \\min_{\\vec{w}} \\left| \\begin{pmatrix} … &amp; \\vec{x}_1 &amp; … … &amp; \\vec{x}_i &amp; … … &amp; \\vec{x}_n &amp; … \\end{pmatrix} \\vec{w} - \\begin{pmatrix} y_1 y_i y_n \\end{pmatrix} \\right|_2^2] [= \\arg \\min_{\\vec{w}} | X \\vec{w} - \\vec{y} |_2^2] Thus best fitting w: [\\frac{\\partial}{\\partial \\mathbf{w}} |X\\mathbf{w} - \\mathbf{y}|^2 = 2X^T (X\\mathbf{w} - \\mathbf{y})] At stationarity, this results in the equation: [(X^T X)\\mathbf{w} = X^T \\mathbf{y}] This system is always consistent: [\\mathbf{\\vec{y}} = \\mathbf{\\vec{y}}{\\text{col}(X)} + \\mathbf{\\vec{y}}{\\text{null}(X^T)}] Thus, [X^T \\mathbf{y} = X^T \\mathbf{\\vec{y}}_{\\text{col}(X)}] Since, [{\\vec{y}}_{\\text{col}(X)} = \\sum_i w_i \\mathbf{x}_i \\quad \\text{(for some coefficients (w_i), where (\\mathbf{x}_i) are columns of (X))}] Define, [{\\vec{w}} := \\begin{bmatrix} w_1 \\vdots w_d \\end{bmatrix}] Then, [(X^T X)\\mathbf{\\vec{w}} = X^T (X\\mathbf{\\vec{w}}) = X^T \\left(\\sum_i w_i \\mathbf{x}i \\right) = X^T \\mathbf{y}{\\text{col}(X)} = X^T \\mathbf{y}] [\\mathbf{\\vec{w}}_{\\text{ols}} = (X^T X)^{\\dagger} X^T \\mathbf{y}] Also called the Ordinary Least Squares (OLS). The solution is unique and stable when $(X^T X)$ is invertible Linear Regression in Statisticall Modeling View Let’s assume that data is generated from the following process: A example $x_i$ is draw independently from the data space $X$ \\(x_i \\sim D_X\\) $y_i^{clean}$ is computed as $(w \\cdot x_i)$, from a fixed unknown $w$ \\(y_i^{clean} := w \\cdot x_i\\) $y_i$ is corrupted from $y_i^{clean}$ by adding independent Gaussian noise $N(0,\\sigma^2)$ \\(y_i := y_i^{clean} + \\epsilon_i = w \\cdot x_i + \\epsilon_i \\quad\\quad \\epsilon_i \\sim N(0, \\sigma^2)\\) $(x_i, y_i)$ is revealed as the $i$-th sample \\((x_1, y_1), ..., (x_n, y_n) =: S\\) How can we determine $w$, from Gaussian noise corrupted observations? [S = (x_1, y_1), …, (x_n, y_n)] Observation: [y_i \\sim w \\cdot x_i + N(0, \\sigma^2) = N(w \\cdot x_i, \\sigma^2)] parameter: w (Ignored terms indepdent of w) [\\log L(w S) = \\sum_{i=1}^n \\log p(y_i w)] [\\propto \\sum_{i=1}^n \\frac{-(w \\cdot x_i - y_i)^2}{2\\sigma^2}] optimizing for $w$ yields the same OLS result! Lasso Regression &amp; Ridge Regression Previously we looked at Ordinary Least Square (OLS) [minimize| X \\vec{w} - \\vec{y} |_2^2] [\\mathbf{\\vec{w}}_{\\text{ols}} = (X^T X)^{\\dagger} X^T \\mathbf{y}] Which is poorly behaved (due to overfitting) when we have limited data. We can incorporate application dependent piror knowledge. Lasso regression: Objective minimize $ X\\vec{w} - \\vec{y} ^2 + \\lambda|\\vec{w}|^2$ reconstruction error ‘regularization’ parameter $\\vec{w}_{ridge} = (X^TX + \\lambda I)^{-1}X^T\\vec{y}$ The ‘regularization’ helps avoid overfitting, and always resulting in a unique solution. Equivalent to the following optimization problem: minimize $ X\\vec{w} - \\vec{y} ^2$ such that $ \\vec{w} ^2 \\leq B$ Ridge Regression: Objective minimize $ X\\vec{w} - \\vec{y} ^2 + \\lambda|\\vec{w}|_1$ ‘lasso’ penalty $\\vec{w}_{lasso} = ?$ no closed form solution Lasso regularization encourages sparse solutions. Equivalent to the following optimization problem: minimize $ X\\vec{w} - \\vec{y} ^2$ such that $ \\vec{w} _1 \\leq B$ Logistic Regression Linear regression for classification: Although its name contains “regression,” it is actually used for classification tasks. For a binary classification problem, given input x, how likely is it that it has label 1?Let this be denoted by P, ie, P is the chance that a given x the associated label y = 1, P = P(Y=1 X=x) ranges between 0 and 1, hence cannot be modelled appropriately via linear regression. If we look at the ‘odds’ of getting y=1 (for a given x) $odds(P) := \\frac{P}{1-P}$ For an event with P=0.9, odds = 9 But, for an event P=0.1, odds = 0.11 Consider the “log” of the odds (very asymmetric) $log(odds(P)) := logit(P) := log(\\frac{P}{1-P})$ $logit(P) = -logit(1-P)$ Symmetric! Can model logit as a linear function!! Model the log-odds or logit with linear function! Given an input x $logit(P(Y=1 X=x)) = logit(P) = log(\\frac{P}{1-P}) \\stackrel{\\text{modeling assumption}}{=} w \\cdot x$ $\\frac{P}{1-P} = e^{w \\cdot x}$ $P(Y=1 X=x) = P = \\frac{e^{w \\cdot x}}{1+e^{w \\cdot x}} = \\frac{1}{1+e^{-w \\cdot x}}$ Sigmoid! How do we learn the prameters? Given samples $S = (x_1, y_1), …, (x_n, y_n)$ yi is binary) $\\mathcal{L}(w S) = \\prod_{i=1}^{n} P((x_i, y_i) w) \\propto \\prod_{i=1}^{n} P(y_i = 1 x_i, w)= \\prod_{i=1}^{n} P(y_i = 1 x_i, w)^{y_i} (1 - P(y_i = 1 x_i, w))^{1-y_i}$ (Binomial MLE) $log \\mathcal{L}(w S) \\propto \\sum_{i=1}^{n} y_i log P_{x_i} + (1 - y_i) log(1 - P_{x_i}) = \\sum_{i=1}^{n} y_i log \\frac{P_{x_i}}{1 - P_{x_i}} + \\sum_{i=1}^{n} log(1 - P_{x_i})$ Now, use logistic model! $= \\sum_{i=1}^{n} y_i (w \\cdot x_i) + \\sum_{i=1}^{n} - log(1 + e^{w \\cdot x_i})$ No closed form solution Can use iterative methods like gradient ‘ascent’ to find the solution Non-parametric Regression What if we don’t know parametric form of the relationship between the independent and dependent variables? How can we predict value of a new test point x without model assumptions? Kernel Regression $y = f_n(x) := \\sum_{i=1}^{n} w_i(x)y_i$ Want weights that emphasize local observations Localization functions: Gaussian Kernel: $K_h(x, x’) = e^{-   x-x’   ^2/h}$ Box Kernel: $1[   x - x’   \\leq h]$ \\(K_h(x, x&#39;) = e^{-||x-x&#39;||^2/h}\\) Gaussian kernel \\(= 1[||x - x&#39;|| \\leq h]\\) Box kernel \\(= [1 - (1/h)||x - x&#39;||]_+\\) Triangle kernel Then define: w_i(x) := \\frac{K_h(x,x_i)}{\\sum_{j=1}^n K_h(x,x_j)} $$","headline":"A Gentle Introduction to Regression","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/tingaldama1/regression%20techniques/parametric%20&%20non-parametric%20methods/statistical%20modeling/2022/12/11/Regression.html"},"url":"http://localhost:4000/tingaldama1/regression%20techniques/parametric%20&%20non-parametric%20methods/statistical%20modeling/2022/12/11/Regression.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/tingaldama1/assets/css/style.css?v=">
    <script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
    <script src="/tingaldama1/assets/js/respond.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="/tingaldama1/assets/css/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup theme-color -->
<!-- start theme color meta headers -->
<meta name="theme-color" content="#353535">
<meta name="msapplication-navbutton-color" content="#353535">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- end theme color meta headers -->


<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/tingaldama1/favicon.ico" -->

<!-- end custom head snippets -->
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<link rel="shortcut icon" href="/tingaldama1//assets/images/favicon.jpg" type="image/x-icon">


  </head>
  <style>
    .categories{
  font-weight: bold;
  color: #949494;
}
 .date{
  font-weight: bold !important;
  color: #949494 !important;
}

  </style>
  <body>
    
<div id="header">
    <nav>
      <ul>
        <li class="fork"><a href="/tingaldama1/">Home</a></li>
        <li class="downloads"><a href="/tingaldama1/contact">Contacts</a></li>
        <li class="downloads"><a href="/tingaldama1/about">About</a></li>
        <li class="downloads"><a href="/tingaldama1/blog">Blogs</a></li>
        <li class="downloads"><a href="/tingaldama1/mlps-resources">MLOps Resources</a></li>
        <li class="downloads"><a href="/tingaldama1/personal-growth">Personal Growth</a></li>
          
      </ul>
    </nav>
  </div><!-- end header -->

    <div class="wrapper">

      <section>
        

        <main>
          <article>
            <h1>A Gentle Introduction to Regression</h1>
            <div class="post-meta">
             <span class="date">Date: December 11, 2022</span>
      <br>
      <span class="categories">
        Categories: Regression Techniques | Parametric & Non-parametric Methods | Statistical Modeling
      </span>
            </div>
            <br>
            <div class="post-content">
              
              <h2 id="parametric-vs-nonparametric-regressioin">Parametric vs Nonparametric Regressioin</h2>

<p><strong>Parametric regression</strong></p>

<p>If we assume a particular form of the regressor:</p>

<p>Goal: to learn the parameters which yield the minimum error/loss</p>

<p><strong>Non-parametric regression</strong></p>

<p>If no specific form of regressor is assumed:</p>

<p>Goal: to learn the predictor directly from the input data that yields the minimum error/loss</p>

<h3 id="linear-regression">Linear Regression</h3>

<p>Want to find a <strong>linear predictor</strong> ( f ), i.e., ( w ) (intercept ( w_0 ) absorbed via lifting):</p>

\[hat{f}(\vec{x}) := \vec{w} \cdot \vec{x}\]

<p>which minimizes the prediction loss over the population.</p>

\[min_{\vec{w}} \mathbb{E}_{\vec{x}, y} \left[ L(\hat{f}(\vec{x}), y) \right]\]

<p>We estimate the parameters by minimizing the corresponding loss on the training data:</p>

\[\arg \min_{\vec{w}} \frac{1}{n} \sum_{i=1}^n \left[ L(\vec{w} \cdot \vec{x}_i, y_i) \right] = \arg \min_{\vec{w}} \frac{1}{n} \sum_{i=1}^n \left( \vec{w} \cdot \vec{x}_i - y_i \right)^2\]

<h4 id="learning-the-parameters">Learning the Parameters</h4>

<p>Unconstraint problem:</p>

\[= \arg \min_{\vec{w}} \left\|
\begin{pmatrix}
... &amp; \vec{x}_1 &amp; ... \\
... &amp; \vec{x}_i &amp; ... \\
... &amp; \vec{x}_n &amp; ...
\end{pmatrix}
\vec{w} -
\begin{pmatrix}
y_1 \\
y_i \\
y_n
\end{pmatrix}
\right\|_2^2\]

\[= \arg \min_{\vec{w}} \| X \vec{w} - \vec{y} \|_2^2\]

<p>Thus best fitting w:</p>

\[\frac{\partial}{\partial \mathbf{w}} \|X\mathbf{w} - \mathbf{y}\|^2 = 2X^T (X\mathbf{w} - \mathbf{y})\]

<p>At stationarity, this results in the equation:</p>

\[(X^T X)\mathbf{w} = X^T \mathbf{y}\]

<p>This system is always consistent:</p>

\[\mathbf{\vec{y}} = \mathbf{\vec{y}}_{\text{col}(X)} + \mathbf{\vec{y}}_{\text{null}(X^T)}\]

<p>Thus,</p>

\[X^T \mathbf{y} = X^T \mathbf{\vec{y}}_{\text{col}(X)}\]

<p>Since,</p>

\[{\vec{y}}_{\text{col}(X)} = \sum_i w_i \mathbf{x}_i \quad \text{(for some coefficients \(w_i\), where \(\mathbf{x}_i\) are columns of \(X\))}\]

<p>Define,</p>

\[{\vec{w}} := \begin{bmatrix}
w_1 \\
\vdots \\
w_d
\end{bmatrix}\]

<p>Then,</p>

\[(X^T X)\mathbf{\vec{w}} = X^T (X\mathbf{\vec{w}}) = X^T \left(\sum_i w_i \mathbf{x}_i \right) = X^T \mathbf{y}_{\text{col}(X)} = X^T \mathbf{y}\]

\[\mathbf{\vec{w}}_{\text{ols}} = (X^T X)^{\dagger} X^T \mathbf{y}\]

<p>Also called the Ordinary Least Squares (OLS). The solution is unique and stable when $(X^T X)$ is invertible</p>

<h3 id="linear-regression-in-statisticall-modeling-view">Linear Regression in Statisticall Modeling View</h3>

<p>Let’s assume that data is generated from the following process:</p>

<ul>
  <li>A example $x_i$ is draw independently from the data space $X$
\(x_i \sim D_X\)</li>
  <li>$y_i^{clean}$ is computed as $(w \cdot x_i)$, from a fixed unknown $w$
\(y_i^{clean} := w \cdot x_i\)</li>
  <li>$y_i$ is corrupted from $y_i^{clean}$ by adding independent Gaussian noise $N(0,\sigma^2)$
\(y_i := y_i^{clean} + \epsilon_i = w \cdot x_i + \epsilon_i \quad\quad \epsilon_i \sim N(0, \sigma^2)\)</li>
  <li>$(x_i, y_i)$ is revealed as the $i$-th sample
\((x_1, y_1), ..., (x_n, y_n) =: S\)</li>
</ul>

<p>How can we determine $w$, from Gaussian noise corrupted observations?</p>

\[S = (x_1, y_1), ..., (x_n, y_n)\]

<p>Observation:</p>

\[y_i \sim w \cdot x_i + N(0, \sigma^2) = N(w \cdot x_i, \sigma^2)\]

<p>parameter: w (Ignored terms indepdent of w)</p>

\[\log L(w|S) = \sum_{i=1}^n \log p(y_i|w)\]

\[\propto \sum_{i=1}^n \frac{-(w \cdot x_i - y_i)^2}{2\sigma^2}\]

<p>optimizing for $w$ yields the same OLS result!</p>

<h4 id="lasso-regression--ridge-regression">Lasso Regression &amp; Ridge Regression</h4>

<p>Previously we looked at Ordinary Least Square (OLS)</p>

\[minimize\| X \vec{w} - \vec{y} \|_2^2\]

\[\mathbf{\vec{w}}_{\text{ols}} = (X^T X)^{\dagger} X^T \mathbf{y}\]

<p>Which is  poorly behaved (due to overfitting) when we have limited data. We can incorporate application dependent piror knowledge.</p>

<p><strong>Lasso regression:</strong></p>

<p>Objective</p>

<table>
  <tbody>
    <tr>
      <td>minimize $</td>
      <td>X\vec{w} - \vec{y}</td>
      <td>^2 + \lambda|\vec{w}|^2$</td>
    </tr>
  </tbody>
</table>

<p>reconstruction error ‘regularization’ parameter</p>

<p>$\vec{w}_{ridge} = (X^TX + \lambda I)^{-1}X^T\vec{y}$</p>

<p>The ‘regularization’ helps avoid overfitting, and always resulting in a unique solution. Equivalent to the following optimization problem:</p>

<table>
  <tbody>
    <tr>
      <td>minimize $</td>
      <td>X\vec{w} - \vec{y}</td>
      <td>^2$</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>such that $</td>
      <td>\vec{w}</td>
      <td>^2 \leq B$</td>
    </tr>
  </tbody>
</table>

<p><strong>Ridge Regression:</strong></p>

<p>Objective</p>

<table>
  <tbody>
    <tr>
      <td>minimize $</td>
      <td>X\vec{w} - \vec{y}</td>
      <td>^2 + \lambda|\vec{w}|_1$</td>
    </tr>
  </tbody>
</table>

<p>‘lasso’ penalty</p>

<p>$\vec{w}_{lasso} = ?$ no closed form solution</p>

<p>Lasso regularization encourages sparse solutions. Equivalent to the following optimization problem:</p>

<table>
  <tbody>
    <tr>
      <td>minimize $</td>
      <td>X\vec{w} - \vec{y}</td>
      <td>^2$</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>such that $</td>
      <td>\vec{w}</td>
      <td>_1 \leq B$</td>
    </tr>
  </tbody>
</table>

<h3 id="logistic-regression">Logistic Regression</h3>

<p>Linear regression for classification:</p>

<table>
  <tbody>
    <tr>
      <td>Although its name contains “regression,” it is actually used for classification tasks. For a binary classification problem, given input x, how likely is it that it has label 1?Let this be denoted by P, ie, P is the chance that a given x the associated label y = 1, P = P(Y=1</td>
      <td>X=x) ranges between 0 and 1, hence cannot be modelled appropriately via linear regression. If we look at the ‘odds’ of getting y=1 (for a given x) $odds(P) := \frac{P}{1-P}$</td>
    </tr>
  </tbody>
</table>

<p>For an event with P=0.9, odds = 9
But, for an event P=0.1, odds = 0.11</p>

<p>Consider the “log” of the odds (very asymmetric)</p>

<p>$log(odds(P)) := logit(P) := log(\frac{P}{1-P})$</p>

<p>$logit(P) = -logit(1-P)$ Symmetric! Can model logit as a linear function!!</p>

<p>Model the log-odds or logit with linear function!
Given an input x</p>

<table>
  <tbody>
    <tr>
      <td>$logit(P(Y=1</td>
      <td>X=x)) = logit(P) = log(\frac{P}{1-P}) \stackrel{\text{modeling assumption}}{=} w \cdot x$</td>
    </tr>
  </tbody>
</table>

<p>$\frac{P}{1-P} = e^{w \cdot x}$</p>

<table>
  <tbody>
    <tr>
      <td>$P(Y=1</td>
      <td>X=x) = P = \frac{e^{w \cdot x}}{1+e^{w \cdot x}} = \frac{1}{1+e^{-w \cdot x}}$    Sigmoid!</td>
    </tr>
  </tbody>
</table>

<p>How do we learn the prameters?</p>

<p>Given samples $S = (x_1, y_1), …, (x_n, y_n)$ yi is binary)</p>

<table>
  <tbody>
    <tr>
      <td>$\mathcal{L}(w</td>
      <td>S) = \prod_{i=1}^{n} P((x_i, y_i)</td>
      <td>w) \propto \prod_{i=1}^{n} P(y_i = 1</td>
      <td>x_i, w)= \prod_{i=1}^{n} P(y_i = 1</td>
      <td>x_i, w)^{y_i} (1 - P(y_i = 1</td>
      <td>x_i, w))^{1-y_i}$ (Binomial MLE)</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>$log \mathcal{L}(w</td>
      <td>S) \propto \sum_{i=1}^{n} y_i log P_{x_i} + (1 - y_i) log(1 - P_{x_i}) = \sum_{i=1}^{n} y_i log \frac{P_{x_i}}{1 - P_{x_i}} + \sum_{i=1}^{n} log(1 - P_{x_i})$</td>
    </tr>
  </tbody>
</table>

<p>Now, use logistic model!</p>

<p>$= \sum_{i=1}^{n} y_i (w \cdot x_i) + \sum_{i=1}^{n} - log(1 + e^{w \cdot x_i})$</p>

<p>No closed form solution
Can use iterative methods like gradient ‘ascent’ to find the solution</p>

<h3 id="non-parametric-regression">Non-parametric Regression</h3>

<p>What if we don’t know parametric form of the relationship between the independent and dependent variables? How can we predict value of a</p>

<p>new test point  x without model assumptions?</p>

<h4 id="kernel-regression">Kernel Regression</h4>

<p>$y = f_n(x) := \sum_{i=1}^{n} w_i(x)y_i$</p>

<p>Want weights that emphasize local observations</p>

<p>Localization functions:</p>

<table>
  <tbody>
    <tr>
      <td>Gaussian Kernel: $K_h(x, x’) = e^{-</td>
      <td> </td>
      <td>x-x’</td>
      <td> </td>
      <td>^2/h}$</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Box Kernel: $1[</td>
      <td> </td>
      <td>x - x’</td>
      <td> </td>
      <td>\leq h]$</td>
    </tr>
  </tbody>
</table>

<p>\(K_h(x, x') = e^{-||x-x'||^2/h}\) Gaussian kernel
\(= 1[||x - x'|| \leq h]\) Box kernel
\(= [1 - (1/h)||x - x'||]_+\) Triangle kernel</p>

<p>Then define:
w_i(x) := \frac{K_h(x,x_i)}{\sum_{j=1}^n K_h(x,x_j)}
$$</p>

            </div>
          </article>
        </main>

      </section>
      <div id="title" style="text-align: center;">
    
    <hr>
    <span class="credits left">Project maintained by <a href=""></a></span>
    <br>
    <br>
    <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href=""></a></span>

    <p>&copy; 2024 Tingaldama. All rights reserved.</p>
  </div>
    </div>
  </body>
</html>
