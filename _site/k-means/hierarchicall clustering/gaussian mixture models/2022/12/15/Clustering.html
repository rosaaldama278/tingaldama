<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Unsupervised Learning and Clustering | Tingaldama</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Unsupervised Learning and Clustering" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Unsupervised Learning Data : x1, x2, … in X Assumption : there is an underlying structure in X Learning task : discover the structure given n examples from the data Goal : come up with the summary of the data using the discovered structure Clustering k-means Given : data $\vec{x_1}, \vec{x_2}, \ldots, \vec{x_n} \in \mathbb{R}^d$, and intended number of groupings $k$ Idea : find a set of representatives $\vec{c_1}, \vec{c_2}, \ldots, \vec{c_k}$ such that data is close to some representative Optimization : $\displaystyle \min_{c_1, \ldots, c_k} \left[\sum_{i=1}^n \min_{j=1, \ldots, k} \lVert\vec{x_i} - \vec{c_j}\rVert^2\right]$ The k-means problem has been proven to be NP-hard even for small dimensions and specific cases. This means there is no known polynomial-time algorithm that can solve the problem exactly for all instances. Approximate K-means Algorithm Given : data $\vec{x_1}, \vec{x_2}, \ldots, \vec{x_n} \in \mathbb{R}^d$, and intended number of groupings $k$ Alternating optimization algorithm : Initialize cluster centers $\vec{c_1}, \vec{c_2}, \ldots, \vec{c_k}$ (say randomly) Repeat till no more changes occur Assign data to its closest center (this creates a partition) (assume centers are fixed) Find the optimal centers $\vec{c_1}, \vec{c_2}, \ldots, \vec{c_k}$ (assuming the data partition is fixed) The approximation can be arbitrarily bad, compared to the best cluster assignment. Performance quality is heavily dependent on the initialization. Hierarchical Clustering There are two different approaches: top down, basically we partition data into two groups, then recurse on each part, then stop when cannot partition data anymore; bottom up, we start by each data samples as its own cluster, repeatdly merge ‘closest’ pair of clusters and stop when only one cluster is left. Clustering via Probabilistic Mixture Modeling Probabilistic mixture models assume that the data is generated from a mixture of several probability distributions. Each distribution represents a different cluster. The overall model is a weighted sum of these distributions, where the weights correspond to the probability of each distribution (cluster) generating a given data point. Given: $\vec{x}_1, \vec{x}_2, \ldots \vec{x}_n \in \mathbb{R}^d$ and number of intended number of clusters $k$. Assume a joint probability distribution $(X, C)$ over the joint space $\mathbb{R}^d \times [k]$ [C \sim \begin{pmatrix} \pi_1 \vdots \pi_k \end{pmatrix}] Discrete distribution over the clusters $P[C=i] = \pi_i$ $X C = i \sim$ Some multivariate distribution, e.g. $N(\vec{\mu}_i, \Sigma_i)$ Parameters: $\theta = (\pi_1, \vec{\mu}_1, \Sigma_1, \ldots, \pi_k, \vec{\mu}_k, \Sigma_k)$ d Modeling assumption data $(x_1,c_1),…, (x_n,c_n)$ i.i.d. from $\mathbb{R}^d \times [k]$ BUT only get to see partial information: $x_1, x_2, …, x_n$ $(c_1, …, c_n \text{ hidden!})$ Gaussian Mixture Modeling (GMM) GMMs are a specific type of probabilistic mixture model where each cluster is modeled by a Gaussian (normal) distribution. Given: $\vec{x}_1, \vec{x}_2, \ldots \vec{x}_n \in \mathbb{R}^d$ and $k$. Assume a joint probability distribution $(X, C)$ over the joint space $\mathbb{R}^d \times [k]$ [C \sim \begin{pmatrix} \pi_1 \vdots \pi_k \end{pmatrix}, \quad X|C = i \sim N(\vec{\mu}_i, \Sigma_i) \text{ Gaussian Mixture Model}] $\theta = (\pi_1, \vec{\mu}_1, \Sigma_1, \ldots, \pi_k, \vec{\mu}_k, \Sigma_k)$ [P[\vec{x} \theta] = \sum_{j=1}^k \pi_j \frac{1}{\sqrt{(2\pi)^d \det(\Sigma_j)}} e^{ -\frac{1}{2} (\vec{x} - \vec{\mu}_j)^T \Sigma_j^{-1} (\vec{x} - \vec{\mu}_j) }] πj: The mixing coefficient for the j-th Gaussian component **How do we learn the parameters $\theta$ then? ** Maximum Likelihood Estimation MLE for Mixture modeling (like GMMs) is NOT a convex optimization problem. Even though a global MLE maximizer is not appropriate, several local maximizers are desirable! Expectatino Maximization (EM) Similar in spirit to the alternating update for k-means algorithm Idea: Initialize the parameters arbitrarily Given the current setting of parameters find the best (soft) assignment of data samples to the clusters (Expectation-step) Update all the parameters with respect to the current (soft) assignment that maximizes the likelihood (Maximization-step) Repeat until no more progress is made. Dimensionality Reduction Find a low-dimensional representation that retains important information, and suppresses irrelevant/noise information (Dimensionality reduction)" />
<meta property="og:description" content="Unsupervised Learning Data : x1, x2, … in X Assumption : there is an underlying structure in X Learning task : discover the structure given n examples from the data Goal : come up with the summary of the data using the discovered structure Clustering k-means Given : data $\vec{x_1}, \vec{x_2}, \ldots, \vec{x_n} \in \mathbb{R}^d$, and intended number of groupings $k$ Idea : find a set of representatives $\vec{c_1}, \vec{c_2}, \ldots, \vec{c_k}$ such that data is close to some representative Optimization : $\displaystyle \min_{c_1, \ldots, c_k} \left[\sum_{i=1}^n \min_{j=1, \ldots, k} \lVert\vec{x_i} - \vec{c_j}\rVert^2\right]$ The k-means problem has been proven to be NP-hard even for small dimensions and specific cases. This means there is no known polynomial-time algorithm that can solve the problem exactly for all instances. Approximate K-means Algorithm Given : data $\vec{x_1}, \vec{x_2}, \ldots, \vec{x_n} \in \mathbb{R}^d$, and intended number of groupings $k$ Alternating optimization algorithm : Initialize cluster centers $\vec{c_1}, \vec{c_2}, \ldots, \vec{c_k}$ (say randomly) Repeat till no more changes occur Assign data to its closest center (this creates a partition) (assume centers are fixed) Find the optimal centers $\vec{c_1}, \vec{c_2}, \ldots, \vec{c_k}$ (assuming the data partition is fixed) The approximation can be arbitrarily bad, compared to the best cluster assignment. Performance quality is heavily dependent on the initialization. Hierarchical Clustering There are two different approaches: top down, basically we partition data into two groups, then recurse on each part, then stop when cannot partition data anymore; bottom up, we start by each data samples as its own cluster, repeatdly merge ‘closest’ pair of clusters and stop when only one cluster is left. Clustering via Probabilistic Mixture Modeling Probabilistic mixture models assume that the data is generated from a mixture of several probability distributions. Each distribution represents a different cluster. The overall model is a weighted sum of these distributions, where the weights correspond to the probability of each distribution (cluster) generating a given data point. Given: $\vec{x}_1, \vec{x}_2, \ldots \vec{x}_n \in \mathbb{R}^d$ and number of intended number of clusters $k$. Assume a joint probability distribution $(X, C)$ over the joint space $\mathbb{R}^d \times [k]$ [C \sim \begin{pmatrix} \pi_1 \vdots \pi_k \end{pmatrix}] Discrete distribution over the clusters $P[C=i] = \pi_i$ $X C = i \sim$ Some multivariate distribution, e.g. $N(\vec{\mu}_i, \Sigma_i)$ Parameters: $\theta = (\pi_1, \vec{\mu}_1, \Sigma_1, \ldots, \pi_k, \vec{\mu}_k, \Sigma_k)$ d Modeling assumption data $(x_1,c_1),…, (x_n,c_n)$ i.i.d. from $\mathbb{R}^d \times [k]$ BUT only get to see partial information: $x_1, x_2, …, x_n$ $(c_1, …, c_n \text{ hidden!})$ Gaussian Mixture Modeling (GMM) GMMs are a specific type of probabilistic mixture model where each cluster is modeled by a Gaussian (normal) distribution. Given: $\vec{x}_1, \vec{x}_2, \ldots \vec{x}_n \in \mathbb{R}^d$ and $k$. Assume a joint probability distribution $(X, C)$ over the joint space $\mathbb{R}^d \times [k]$ [C \sim \begin{pmatrix} \pi_1 \vdots \pi_k \end{pmatrix}, \quad X|C = i \sim N(\vec{\mu}_i, \Sigma_i) \text{ Gaussian Mixture Model}] $\theta = (\pi_1, \vec{\mu}_1, \Sigma_1, \ldots, \pi_k, \vec{\mu}_k, \Sigma_k)$ [P[\vec{x} \theta] = \sum_{j=1}^k \pi_j \frac{1}{\sqrt{(2\pi)^d \det(\Sigma_j)}} e^{ -\frac{1}{2} (\vec{x} - \vec{\mu}_j)^T \Sigma_j^{-1} (\vec{x} - \vec{\mu}_j) }] πj: The mixing coefficient for the j-th Gaussian component **How do we learn the parameters $\theta$ then? ** Maximum Likelihood Estimation MLE for Mixture modeling (like GMMs) is NOT a convex optimization problem. Even though a global MLE maximizer is not appropriate, several local maximizers are desirable! Expectatino Maximization (EM) Similar in spirit to the alternating update for k-means algorithm Idea: Initialize the parameters arbitrarily Given the current setting of parameters find the best (soft) assignment of data samples to the clusters (Expectation-step) Update all the parameters with respect to the current (soft) assignment that maximizes the likelihood (Maximization-step) Repeat until no more progress is made. Dimensionality Reduction Find a low-dimensional representation that retains important information, and suppresses irrelevant/noise information (Dimensionality reduction)" />
<link rel="canonical" href="http://localhost:4000/tingaldama/k-means/hierarchicall%20clustering/gaussian%20mixture%20models/2022/12/15/Clustering.html" />
<meta property="og:url" content="http://localhost:4000/tingaldama/k-means/hierarchicall%20clustering/gaussian%20mixture%20models/2022/12/15/Clustering.html" />
<meta property="og:site_name" content="Tingaldama" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-12-15T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Unsupervised Learning and Clustering" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-12-15T00:00:00+01:00","datePublished":"2022-12-15T00:00:00+01:00","description":"Unsupervised Learning Data : x1, x2, … in X Assumption : there is an underlying structure in X Learning task : discover the structure given n examples from the data Goal : come up with the summary of the data using the discovered structure Clustering k-means Given : data $\\vec{x_1}, \\vec{x_2}, \\ldots, \\vec{x_n} \\in \\mathbb{R}^d$, and intended number of groupings $k$ Idea : find a set of representatives $\\vec{c_1}, \\vec{c_2}, \\ldots, \\vec{c_k}$ such that data is close to some representative Optimization : $\\displaystyle \\min_{c_1, \\ldots, c_k} \\left[\\sum_{i=1}^n \\min_{j=1, \\ldots, k} \\lVert\\vec{x_i} - \\vec{c_j}\\rVert^2\\right]$ The k-means problem has been proven to be NP-hard even for small dimensions and specific cases. This means there is no known polynomial-time algorithm that can solve the problem exactly for all instances. Approximate K-means Algorithm Given : data $\\vec{x_1}, \\vec{x_2}, \\ldots, \\vec{x_n} \\in \\mathbb{R}^d$, and intended number of groupings $k$ Alternating optimization algorithm : Initialize cluster centers $\\vec{c_1}, \\vec{c_2}, \\ldots, \\vec{c_k}$ (say randomly) Repeat till no more changes occur Assign data to its closest center (this creates a partition) (assume centers are fixed) Find the optimal centers $\\vec{c_1}, \\vec{c_2}, \\ldots, \\vec{c_k}$ (assuming the data partition is fixed) The approximation can be arbitrarily bad, compared to the best cluster assignment. Performance quality is heavily dependent on the initialization. Hierarchical Clustering There are two different approaches: top down, basically we partition data into two groups, then recurse on each part, then stop when cannot partition data anymore; bottom up, we start by each data samples as its own cluster, repeatdly merge ‘closest’ pair of clusters and stop when only one cluster is left. Clustering via Probabilistic Mixture Modeling Probabilistic mixture models assume that the data is generated from a mixture of several probability distributions. Each distribution represents a different cluster. The overall model is a weighted sum of these distributions, where the weights correspond to the probability of each distribution (cluster) generating a given data point. Given: $\\vec{x}_1, \\vec{x}_2, \\ldots \\vec{x}_n \\in \\mathbb{R}^d$ and number of intended number of clusters $k$. Assume a joint probability distribution $(X, C)$ over the joint space $\\mathbb{R}^d \\times [k]$ [C \\sim \\begin{pmatrix} \\pi_1 \\vdots \\pi_k \\end{pmatrix}] Discrete distribution over the clusters $P[C=i] = \\pi_i$ $X C = i \\sim$ Some multivariate distribution, e.g. $N(\\vec{\\mu}_i, \\Sigma_i)$ Parameters: $\\theta = (\\pi_1, \\vec{\\mu}_1, \\Sigma_1, \\ldots, \\pi_k, \\vec{\\mu}_k, \\Sigma_k)$ d Modeling assumption data $(x_1,c_1),…, (x_n,c_n)$ i.i.d. from $\\mathbb{R}^d \\times [k]$ BUT only get to see partial information: $x_1, x_2, …, x_n$ $(c_1, …, c_n \\text{ hidden!})$ Gaussian Mixture Modeling (GMM) GMMs are a specific type of probabilistic mixture model where each cluster is modeled by a Gaussian (normal) distribution. Given: $\\vec{x}_1, \\vec{x}_2, \\ldots \\vec{x}_n \\in \\mathbb{R}^d$ and $k$. Assume a joint probability distribution $(X, C)$ over the joint space $\\mathbb{R}^d \\times [k]$ [C \\sim \\begin{pmatrix} \\pi_1 \\vdots \\pi_k \\end{pmatrix}, \\quad X|C = i \\sim N(\\vec{\\mu}_i, \\Sigma_i) \\text{ Gaussian Mixture Model}] $\\theta = (\\pi_1, \\vec{\\mu}_1, \\Sigma_1, \\ldots, \\pi_k, \\vec{\\mu}_k, \\Sigma_k)$ [P[\\vec{x} \\theta] = \\sum_{j=1}^k \\pi_j \\frac{1}{\\sqrt{(2\\pi)^d \\det(\\Sigma_j)}} e^{ -\\frac{1}{2} (\\vec{x} - \\vec{\\mu}_j)^T \\Sigma_j^{-1} (\\vec{x} - \\vec{\\mu}_j) }] πj: The mixing coefficient for the j-th Gaussian component **How do we learn the parameters $\\theta$ then? ** Maximum Likelihood Estimation MLE for Mixture modeling (like GMMs) is NOT a convex optimization problem. Even though a global MLE maximizer is not appropriate, several local maximizers are desirable! Expectatino Maximization (EM) Similar in spirit to the alternating update for k-means algorithm Idea: Initialize the parameters arbitrarily Given the current setting of parameters find the best (soft) assignment of data samples to the clusters (Expectation-step) Update all the parameters with respect to the current (soft) assignment that maximizes the likelihood (Maximization-step) Repeat until no more progress is made. Dimensionality Reduction Find a low-dimensional representation that retains important information, and suppresses irrelevant/noise information (Dimensionality reduction)","headline":"Unsupervised Learning and Clustering","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/tingaldama/k-means/hierarchicall%20clustering/gaussian%20mixture%20models/2022/12/15/Clustering.html"},"url":"http://localhost:4000/tingaldama/k-means/hierarchicall%20clustering/gaussian%20mixture%20models/2022/12/15/Clustering.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/tingaldama/assets/css/style.css?v=">
    <script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
    <script src="/tingaldama/assets/js/respond.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="/tingaldama/assets/css/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup theme-color -->
<!-- start theme color meta headers -->
<meta name="theme-color" content="#353535">
<meta name="msapplication-navbutton-color" content="#353535">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- end theme color meta headers -->


<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/tingaldama/favicon.ico" -->

<!-- end custom head snippets -->
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<link rel="shortcut icon" href="/tingaldama//assets/images/favicon.jpg" type="image/x-icon">


  </head>
  <style>
    .categories{
  font-weight: bold;
  color: #949494;
}
 .date{
  font-weight: bold !important;
  color: #949494 !important;
}

  </style>
  <body>
    
<div id="header">
    <nav>
      <ul>
        <li class="fork"><a href="/tingaldama/">Home</a></li>
        <li class="downloads"><a href="/tingaldama/contact">Contacts</a></li>
        <li class="downloads"><a href="/tingaldama/about">About</a></li>
        <li class="downloads"><a href="/tingaldama/blog">Blogs</a></li>
        <li class="downloads"><a href="/tingaldama/mlps-resources">MLOps Resources</a></li>
        <li class="downloads"><a href="/tingaldama/personal-growth">Personal Growth</a></li>
          
      </ul>
    </nav>
  </div><!-- end header -->

    <div class="wrapper">

      <section>
        

        <main>
          <article>
            <h1>Unsupervised Learning and Clustering</h1>
            <div class="post-meta">
             <span class="date">Date: December 15, 2022</span>
      <br>
      <span class="categories">
        Categories: k-means | Hierarchicall Clustering | Gaussian Mixture Models
      </span>
            </div>
            <br>
            <div class="post-content">
              
              <h2 id="unsupervised-learning">Unsupervised Learning</h2>

<p><strong>Data</strong> : x1, x2, … in X</p>

<p><strong>Assumption</strong> : there is an underlying structure in X</p>

<p><strong>Learning task</strong> : discover the structure given n examples from the data</p>

<p><strong>Goal</strong> : come up with the summary of the data using the discovered structure</p>

<h3 id="clustering"><strong>Clustering</strong></h3>

<h4 id="k-means">k-means</h4>

<p><strong>Given</strong> : data $\vec{x_1}, \vec{x_2}, \ldots, \vec{x_n} \in \mathbb{R}^d$, and intended number of groupings $k$</p>

<p><strong>Idea</strong> :
find a set of representatives $\vec{c_1}, \vec{c_2}, \ldots, \vec{c_k}$ such that data is close to some representative</p>

<p><strong>Optimization</strong> :
$\displaystyle \min_{c_1, \ldots, c_k} \left[\sum_{i=1}^n \min_{j=1, \ldots, k} \lVert\vec{x_i} - \vec{c_j}\rVert^2\right]$</p>

<p>The k-means problem has been proven to be NP-hard even for small dimensions and specific cases. This means there is no known polynomial-time algorithm that can solve the problem exactly for all instances.</p>

<h4 id="approximate-k-means-algorithm">Approximate K-means Algorithm</h4>

<p><strong>Given</strong> : data $\vec{x_1}, \vec{x_2}, \ldots, \vec{x_n} \in \mathbb{R}^d$, and intended number of groupings $k$</p>

<p><strong>Alternating optimization algorithm</strong> :</p>

<ul>
  <li>Initialize cluster centers $\vec{c_1}, \vec{c_2}, \ldots, \vec{c_k}$ (say randomly)</li>
  <li>Repeat till no more changes occur
    <ul>
      <li>Assign data to its closest center (this creates a partition) (assume centers are fixed)</li>
      <li>Find the optimal centers $\vec{c_1}, \vec{c_2}, \ldots, \vec{c_k}$ (assuming the data partition is fixed)</li>
    </ul>
  </li>
</ul>

<p>The approximation can be arbitrarily bad, compared to the best cluster assignment. Performance quality is heavily dependent on the initialization.</p>

<h3 id="hierarchical-clustering">Hierarchical Clustering</h3>

<p>There are two different approaches: top down, basically we partition data into two groups, then recurse on each part, then stop when cannot partition data anymore; bottom up, we start by each data samples as its own cluster, repeatdly merge ‘closest’ pair of clusters and stop when only one cluster is left.</p>

<h4 id="clustering-via-probabilistic-mixture-modeling">Clustering via Probabilistic Mixture Modeling</h4>

<p>Probabilistic mixture models assume that the data is generated from a mixture of several probability distributions. Each distribution represents a different cluster. The overall model is a weighted sum of these distributions, where the weights correspond to the probability of each distribution (cluster) generating a given data point.</p>

<p>Given: $\vec{x}_1, \vec{x}_2, \ldots \vec{x}_n \in \mathbb{R}^d$ and number of intended number of clusters $k$.
Assume a joint probability distribution $(X, C)$ over the joint space $\mathbb{R}^d \times [k]$</p>

\[C \sim \begin{pmatrix}
\pi_1 \
\vdots \
\pi_k
\end{pmatrix}\]

<p>Discrete distribution over the clusters $P[C=i] = \pi_i$</p>

<table>
  <tbody>
    <tr>
      <td>$X</td>
      <td>C = i \sim$ Some multivariate distribution, e.g. $N(\vec{\mu}_i, \Sigma_i)$</td>
    </tr>
  </tbody>
</table>

<p>Parameters: $\theta = (\pi_1, \vec{\mu}_1, \Sigma_1, \ldots, \pi_k, \vec{\mu}_k, \Sigma_k)$ d</p>

<p>Modeling assumption data $(x_1,c_1),…, (x_n,c_n)$ i.i.d. from $\mathbb{R}^d \times [k]$
BUT only get to see partial information: $x_1, x_2, …, x_n$ $(c_1, …, c_n \text{ hidden!})$</p>

<h4 id="gaussian-mixture-modeling-gmm">Gaussian Mixture Modeling (GMM)</h4>

<p>GMMs are a specific type of probabilistic mixture model where each cluster is modeled by a Gaussian (normal) distribution.</p>

<p>Given: $\vec{x}_1, \vec{x}_2, \ldots \vec{x}_n \in \mathbb{R}^d$ and $k$.
Assume a joint probability distribution $(X, C)$ over the joint space $\mathbb{R}^d \times [k]$</p>

\[C \sim \begin{pmatrix}
\pi_1 \
\vdots \
\pi_k
\end{pmatrix}, \quad X|C = i \sim N(\vec{\mu}_i, \Sigma_i) \text{ Gaussian Mixture Model}\]

<p>$\theta = (\pi_1, \vec{\mu}_1, \Sigma_1, \ldots, \pi_k, \vec{\mu}_k, \Sigma_k)$</p>

\[P[\vec{x} | \theta] = \sum_{j=1}^k \pi_j \frac{1}{\sqrt{(2\pi)^d \det(\Sigma_j)}} e^{ -\frac{1}{2} (\vec{x} - \vec{\mu}_j)^T \Sigma_j^{-1} (\vec{x} - \vec{\mu}_j) }\]

<p><strong>π</strong>j: The mixing coefficient for the j-th Gaussian component</p>

<p>**How do we learn the parameters $\theta$ then? **</p>

<h4 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h4>

<p>MLE for Mixture modeling (like GMMs) is NOT a convex optimization problem. Even though a global MLE maximizer is not appropriate, several</p>

<p>local maximizers are desirable!</p>

<h4 id="expectatino-maximization-em">Expectatino Maximization (EM)</h4>

<p>Similar in spirit to the alternating update for k-means algorithm</p>

<p>Idea:</p>

<ul>
  <li>Initialize the parameters arbitrarily</li>
  <li>Given the current setting of parameters find the best (soft) assignment of</li>
  <li>data samples to the clusters (Expectation-step)</li>
  <li>Update all the parameters with respect to the current (soft) assignment that maximizes the likelihood (Maximization-step)</li>
  <li>Repeat until no more progress is made.</li>
</ul>

<h3 id="dimensionality-reduction">Dimensionality Reduction</h3>

<p>Find a low-dimensional representation that retains important information, and suppresses irrelevant/noise information (<code class="language-plaintext highlighter-rouge">Dimensionality reduction</code>)</p>

            </div>
          </article>
        </main>

      </section>
      <div id="title" style="text-align: center;">
    
    <hr>
    <span class="credits left">Project maintained by <a href=""></a></span>
    <br>
    <br>
    <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href=""></a></span>

    <p>&copy; 2024 Tingaldama. All rights reserved.</p>
  </div>
    </div>
  </body>
</html>
